[
  {
    "objectID": "posts/23-11-2025_covariance-shrinkage/index.html",
    "href": "posts/23-11-2025_covariance-shrinkage/index.html",
    "title": "Covariance Shrinkage: From Linear Shrinkage to Random Matrix Theory",
    "section": "",
    "text": "The estimation of the covariance matrix \\(\\Sigma \\in \\mathbb{R}^{N \\times N}\\) is a fundamental problem in multivariate statistics, essential for portfolio optimization, risk management, and dimensionality reduction. The standard estimator, the sample covariance matrix \\(S\\), is unbiased and converges to \\(\\Sigma\\) as the sample size \\(T \\to \\infty\\) while \\(N\\) remains fixed. However, in modern applications (e.g., genomics, finance), we often face the “large \\(N\\), small \\(T\\)” regime, or more generally, the asymptotic regime where both \\(N, T \\to \\infty\\) such that \\(N/T \\to q \\in (0, \\infty)\\).\nIn this regime, \\(S\\) becomes ill-conditioned or singular (if \\(N &gt; T\\)). Even when invertible, its eigenvalues are systematically distorted: small eigenvalues are underestimated, and large eigenvalues are overestimated. This phenomenon is rigorously described by Random Matrix Theory (RMT).\nThis post explores three major approaches to mitigate these issues:\n\nLinear Shrinkage: Pulling the sample covariance towards a structured target (Ledoit & Wolf 2004).\nSpectral Analysis: Understanding the limiting distribution of eigenvalues via the Marchenko-Pastur law.\nNonlinear Shrinkage: Optimally correcting the eigenvalues individually using the “Direct Kernel” method (Ledoit & Wolf 2020).\n\nWe will implement these concepts in Python and validate them through simulation."
  },
  {
    "objectID": "posts/23-11-2025_covariance-shrinkage/index.html#introduction",
    "href": "posts/23-11-2025_covariance-shrinkage/index.html#introduction",
    "title": "Covariance Shrinkage: From Linear Shrinkage to Random Matrix Theory",
    "section": "",
    "text": "The estimation of the covariance matrix \\(\\Sigma \\in \\mathbb{R}^{N \\times N}\\) is a fundamental problem in multivariate statistics, essential for portfolio optimization, risk management, and dimensionality reduction. The standard estimator, the sample covariance matrix \\(S\\), is unbiased and converges to \\(\\Sigma\\) as the sample size \\(T \\to \\infty\\) while \\(N\\) remains fixed. However, in modern applications (e.g., genomics, finance), we often face the “large \\(N\\), small \\(T\\)” regime, or more generally, the asymptotic regime where both \\(N, T \\to \\infty\\) such that \\(N/T \\to q \\in (0, \\infty)\\).\nIn this regime, \\(S\\) becomes ill-conditioned or singular (if \\(N &gt; T\\)). Even when invertible, its eigenvalues are systematically distorted: small eigenvalues are underestimated, and large eigenvalues are overestimated. This phenomenon is rigorously described by Random Matrix Theory (RMT).\nThis post explores three major approaches to mitigate these issues:\n\nLinear Shrinkage: Pulling the sample covariance towards a structured target (Ledoit & Wolf 2004).\nSpectral Analysis: Understanding the limiting distribution of eigenvalues via the Marchenko-Pastur law.\nNonlinear Shrinkage: Optimally correcting the eigenvalues individually using the “Direct Kernel” method (Ledoit & Wolf 2020).\n\nWe will implement these concepts in Python and validate them through simulation."
  },
  {
    "objectID": "posts/23-11-2025_covariance-shrinkage/index.html#setup-and-plotting-theme",
    "href": "posts/23-11-2025_covariance-shrinkage/index.html#setup-and-plotting-theme",
    "title": "Covariance Shrinkage: From Linear Shrinkage to Random Matrix Theory",
    "section": "2 Setup and Plotting Theme",
    "text": "2 Setup and Plotting Theme\nFirst, we define our plotting theme using plotnine to ensure academic-quality visualizations.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport plotnine as pn\nfrom scipy import linalg, stats\nfrom plotnine import *\nfrom IPython.display import display\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Define Academic Theme\nTHEME_ACADEMIC = pn.theme(\n    text=pn.element_text(family=\"monospace\"),\n    plot_title=pn.element_text(weight=\"bold\", size=12, ha=\"center\"),\n    legend_text=pn.element_text(size=10),\n    legend_title=pn.element_text(size=10, hjust = 0.5),\n    panel_background=pn.element_rect(fill=\"white\"),\n    panel_border=pn.element_rect(color=\"grey\", size=0.5),\n    axis_ticks=pn.element_line(color=\"grey\"),\n    panel_grid_major=pn.element_line(color=\"grey\", size=0.1, alpha=0.3),\n    panel_grid_minor=pn.element_line(color=\"grey\", size=0.1, alpha=0.3),\n    legend_background=pn.element_rect(fill=\"white\", color=None),\n    legend_key=pn.element_rect(fill=\"white\", color=None),\n    legend_key_size=18,\n    plot_margin=0.01,\n    figure_size=(5, 3.5),\n    axis_title=element_text(size=12),\n    axis_text=element_text(size=10),\n    panel_spacing=0.05\n)\n\nPALETTE = [\n    \"#8F44FFFF\", \"#00A1D5FF\", \"#B24745FF\", \"#79AF97FF\", \"#6A6599FF\",\n    \"#80796BFF\", \"#FFC107FF\", \"#00C49AFF\", \"#FF7043FF\", \"#003366FF\",\n    \"#66BB6AFF\", \"#BA68C8FF\", \"#8B0000FF\", \"#556B2FFF\", \"#FFD700FF\",\n    \"#40E0D0FF\", \"#E6E6FAFF\", \"#800000FF\", \"#A0522DFF\"\n]"
  },
  {
    "objectID": "posts/23-11-2025_covariance-shrinkage/index.html#linear-shrinkage-estimation",
    "href": "posts/23-11-2025_covariance-shrinkage/index.html#linear-shrinkage-estimation",
    "title": "Covariance Shrinkage: From Linear Shrinkage to Random Matrix Theory",
    "section": "3 Linear Shrinkage Estimation",
    "text": "3 Linear Shrinkage Estimation\n\n3.1 The Bias-Variance Tradeoff\nLet \\(X\\) be a \\(T \\times N\\) matrix of i.i.d. observations with mean zero and covariance \\(\\Sigma\\). The sample covariance matrix is \\(S = \\frac{1}{T} X^\\top X\\). While \\(\\mathbb{E}[S] = \\Sigma\\), the variance of the entries of \\(S\\) can be large when \\(T\\) is not sufficiently larger than \\(N\\).\nLinear shrinkage proposes an estimator \\(\\hat{\\Sigma}_{shrink}\\) that is a convex combination of the sample covariance \\(S\\) and a highly structured target estimator \\(F\\) (e.g., the identity matrix scaled by average variance, or a single-factor model covariance).\n\\[\n\\hat{\\Sigma}_{shrink} = \\delta F + (1 - \\delta) S, \\quad \\delta \\in [0, 1]\n\\]\nHere, \\(\\delta\\) is the shrinkage intensity.\n\n\\(S\\) has low bias (zero) but high variance.\n\\(F\\) has high bias but low variance (it has few parameters).\n\\(\\hat{\\Sigma}_{shrink}\\) balances bias and variance to minimize the Mean Squared Error (MSE).\n\n\n\n3.2 Optimal Shrinkage Intensity\nLedoit and Wolf (2004) derived the optimal \\(\\delta^*\\) that minimizes the expected Frobenius norm of the error:\n\\[\n\\min_{\\delta} \\mathbb{E} \\left[ \\| \\hat{\\Sigma}_{shrink} - \\Sigma \\|_F^2 \\right]\n\\]\nThe analytical solution for the optimal intensity is given by:\n\\[\n\\delta^* = \\frac{\\mathbb{E}[\\| S - \\Sigma \\|_F^2]}{\\mathbb{E}[\\| S - F \\|_F^2]}\n\\]\nIntuitively, this ratio represents the variance of the sample covariance divided by the total mean squared error of the sample covariance relative to the target.\nCrucially, while this formula depends on the unknown true covariance \\(\\Sigma\\), Ledoit and Wolf (2004) derived consistent estimators for the numerator and denominator using only the sample data \\(X\\). By estimating the asymptotic variance of the entries of \\(S\\), we can compute a practical \\(\\hat{\\delta}^*\\) without ever knowing \\(\\Sigma\\).\nLet’s implement a simple linear shrinkage towards the identity matrix (scaled by the average trace).\n\n\n3.3 Practical Implementation\nIn practice, we do not need to implement the estimation of \\(\\delta^*\\) from scratch. The scikit-learn library provides a robust implementation via sklearn.covariance.LedoitWolf. This class automatically handles the estimation of the required asymptotic variances to compute the optimal shrinkage intensity.\nHowever, for educational purposes, we implement a simplified version below to demonstrate the mechanics.\n\n\nCode\ndef ledoit_wolf_shrinkage(X):\n    \"\"\"\n    Computes the Ledoit-Wolf shrinkage estimator towards the identity matrix.\n    \n    Args:\n        X: (T, N) array of observations.\n        \n    Returns:\n        Sigma_shrink: (N, N) shrunk covariance matrix.\n        delta: Calculated shrinkage intensity.\n    \"\"\"\n    T, N = X.shape\n    S = np.cov(X, rowvar=False)\n    \n    # Target F: Identity scaled by average variance\n    mu = np.trace(S) / N\n    F = mu * np.eye(N)\n    \n    # Calculate optimal delta (simplified implementation of LW2004)\n    # We use a simplified proxy for demonstration or use sklearn's implementation logic\n    # For rigorous implementation, one computes pi, rho, gamma terms.\n    \n    # Using a simplified heuristic for demonstration:\n    # delta approx min(1, (1/T * sum(var)) / sum((S - F)^2))\n    # We will use a manual calculation of the LW formula for trace target\n    \n    X_centered = X - X.mean(axis=0)\n    \n    # Calculate pi_hat (sum of asymptotic variances of entries of S)\n    # Y is T x N x N tensor of outer products\n    # This is memory intensive, optimized approach:\n    \n    # Compute sum of squared differences from S for each observation's outer product\n    # \\pi_{ij} = AsyVar(\\sqrt{T} s_{ij})\n    \n    # Efficient calculation of delta usually requires O(N^2) or O(N^3)\n    # Here we use a standard approximation for Gaussian X:\n    # Optimal delta for Gaussian X towards Identity is roughly:\n    # delta = (N + 1) / (T - 1) * ( (sum lambda^2 - tr(S)^2/N) / (tr(S^2) - tr(S)^2/N) ) ... \n    # Actually, let's stick to the conceptual definition for the simulation \n    # and calculate delta empirically or use a fixed range for demonstration, \n    # OR implement the robust estimator.\n    \n    # Let's use a simpler estimator for b2:\n    # b2 = 1/T * (E[tr(S^2)] - tr(Sigma^2)) ...\n    \n    # Implementation of Ledoit-Wolf 2004:\n    n, p = T, N\n    sample = pd.DataFrame(X)\n    cov = sample.cov().values\n    mean = np.trace(cov) / p\n    target = mean * np.eye(p)\n    \n    # Calculate d2\n    d2 = np.sum((cov - target)**2)\n    \n    # Calculate b2\n    # This part is usually the bottleneck. \n    # We use the fact that for centered data X:\n    # b2 = 1/T^2 \\sum_{i=1}^T || x_i x_i' - S ||^2\n    \n    # We can compute this efficiently\n    X_c = X - X.mean(axis=0)\n    \n    # We need sum of squared variances of individual entries\n    # var(s_ij) = 1/T^2 * sum( (x_ki x_kj - s_ij)^2 )\n    \n    # Let's compute b2 term by term for diagonal and off-diagonal\n    # This is O(N^2 T)\n    \n    # Optimization:\n    # b2 = 1/T^2 * sum_k ( sum_ij (x_ki x_kj - s_ij)^2 )\n    #    = 1/T^2 * sum_k ( || x_k x_k' - S ||_F^2 )\n    \n    # We can just loop if N is not too large, or use broadcasting\n    # For the blog post simulation (N=100), a loop is fine.\n    \n    b2 = 0\n    for i in range(T):\n        x_i = X_c[i, :].reshape(-1, 1)\n        diff = x_i @ x_i.T - cov\n        b2 += np.sum(diff**2)\n    \n    b2 = b2 / (T**2)\n    b2 = min(b2, d2)\n    \n    delta = b2 / d2\n    shrunk_cov = delta * target + (1 - delta) * cov\n    \n    return shrunk_cov, delta\n\n\nTo visualize what linear shrinkage actually does, let’s look at a heatmap of the correlation matrices for a small example.\n\n\nCode\n# Generate a small example\nN_small = 10\nT_small = 20\n# Create a structured covariance (block diagonal)\nSigma_small = np.zeros((N_small, N_small))\nfor i in range(N_small):\n    for j in range(N_small):\n        if abs(i-j) &lt; 3:\n            Sigma_small[i, j] = 0.8**abs(i-j)\n            \nX_small = np.random.multivariate_normal(np.zeros(N_small), Sigma_small, T_small)\nS_small = np.cov(X_small, rowvar=False)\nS_lw_small, delta_small = ledoit_wolf_shrinkage(X_small)\n\n# Prepare data for heatmap\ndef melt_matrix(M, name):\n    df = pd.DataFrame(M)\n    df['Row'] = range(M.shape[0])\n    df = df.melt(id_vars='Row', var_name='Col', value_name='Value')\n    df['Type'] = name\n    return df\n\ndf_heat = pd.concat([\n    melt_matrix(S_small, \"Sample\"),\n    melt_matrix(S_lw_small, f\"Shrunk (δ={delta_small:.2f})\")\n])\n\nplot_heat = (\n    ggplot(df_heat, aes(x='Col', y='Row', fill='Value'))\n    + geom_tile()\n    + facet_wrap('~Type')\n    + scale_fill_cmap(name='viridis')\n    + scale_y_reverse()\n    + labs(title=\"Linear Shrinkage Effect\")\n    + THEME_ACADEMIC\n    + theme(figure_size=(5, 2.5))\n)\n\ndisplay(plot_heat)\n\n\n/tmp/ipykernel_826249/463712102.py:11: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n\n\n\n\n\nHeatmap comparison: The sample covariance (left) exhibits noisy off-diagonal elements. The linearly shrunk covariance (right) dampens this noise, preserving the underlying block-diagonal structure."
  },
  {
    "objectID": "posts/23-11-2025_covariance-shrinkage/index.html#random-matrix-theory-the-marchenko-pastur-law",
    "href": "posts/23-11-2025_covariance-shrinkage/index.html#random-matrix-theory-the-marchenko-pastur-law",
    "title": "Covariance Shrinkage: From Linear Shrinkage to Random Matrix Theory",
    "section": "4 Random Matrix Theory: The Marchenko-Pastur Law",
    "text": "4 Random Matrix Theory: The Marchenko-Pastur Law\nWhen \\(N\\) and \\(T\\) are large, the eigenvalues of the sample covariance matrix \\(S\\) of a random matrix \\(X\\) (with i.i.d. entries with mean 0 and variance \\(\\sigma^2\\)) are not concentrated around \\(\\sigma^2\\). Instead, they spread out according to a deterministic probability density function known as the Marchenko-Pastur (MP) law.\nLet \\(q = T/N\\). If \\(q \\ge 1\\), the eigenvalues \\(\\lambda\\) of \\(S\\) are distributed on the interval \\([\\lambda_-, \\lambda_+]\\) with density:\n\\[\nf(\\lambda) = \\frac{T}{N} \\frac{\\sqrt{(\\lambda_+ - \\lambda)(\\lambda - \\lambda_-)}}{2\\pi \\sigma^2 \\lambda} \\mathbb{1}_{[\\lambda_-, \\lambda_+]}\n\\]\nwhere the spectral bounds are: \\[\n\\lambda_{\\pm} = \\sigma^2 \\left( 1 \\pm \\sqrt{\\frac{N}{T}} \\right)^2\n\\]\nIf \\(N &gt; T\\) (\\(q &lt; 1\\)), there is an additional point mass at zero of weight \\(1 - T/N\\).\nThis distribution explains why sample covariance matrices appear to have “structure” (large leading eigenvalues) even when the data is pure noise.\n\n4.1 Intuition: Why do eigenvalues spread out?\nThe sample covariance matrix \\(S\\) is constructed to “fit” the data \\(X\\). Even if \\(X\\) is pure noise, there will always be some random directions in the high-dimensional space along which the variance of the projected data is larger than average, and others where it is smaller. The eigendecomposition identifies these directions of maximum variance.\nWhen \\(N\\) is large relative to \\(T\\), the optimizer (eigendecomposition) has many degrees of freedom to find these spurious correlations. Consequently, the largest sample eigenvalues systematically overestimate the true variance (they capture noise as signal), and the smallest eigenvalues underestimate it. This results in the “smearing” of the spectral density observed in the Marchenko-Pastur law.\n\n\n4.2 Implementation of MP Density\n\n\nCode\ndef marchenko_pastur_pdf(var, q, pts=1000):\n    \"\"\"\n    Generates the Marchenko-Pastur PDF.\n    \n    Args:\n        var: Variance of the underlying i.i.d. process (sigma^2).\n        q: Ratio T/N.\n        pts: Number of points for the curve.\n        \n    Returns:\n        pd.DataFrame with 'x' and 'y' columns.\n    \"\"\"\n    # q = T/N in the text, but standard MP notation often uses Q = T/N or q = N/T.\n    # Let's stick to the formula: lambda_pm = sigma^2 * (1 +/- sqrt(N/T))^2\n    # Let ratio = N/T\n    ratio = 1/q \n    \n    lambda_min = var * (1 - np.sqrt(ratio))**2\n    lambda_max = var * (1 + np.sqrt(ratio))**2\n    \n    x = np.linspace(lambda_min, lambda_max, pts)\n    \n    # f(lambda) = (1 / (2*pi*var*ratio*x)) * sqrt((max-x)(x-min))\n    # Note: The pre-factor depends on definition. \n    # Standard form for density of eigenvalues of S = 1/T X'X where X is T x N\n    # If we look at density of eigenvalues, the integral should be 1.\n    \n    def mp_density(l):\n        return (1 / (2 * np.pi * var * ratio * l)) * np.sqrt((lambda_max - l) * (l - lambda_min))\n    \n    y = mp_density(x)\n    # Handle numerical issues at boundaries\n    y = np.nan_to_num(y)\n    \n    return pd.DataFrame({'x': x, 'y': y})"
  },
  {
    "objectID": "posts/23-11-2025_covariance-shrinkage/index.html#nonlinear-shrinkage-estimation",
    "href": "posts/23-11-2025_covariance-shrinkage/index.html#nonlinear-shrinkage-estimation",
    "title": "Covariance Shrinkage: From Linear Shrinkage to Random Matrix Theory",
    "section": "5 Nonlinear Shrinkage Estimation",
    "text": "5 Nonlinear Shrinkage Estimation\nLinear shrinkage is limited because it applies the same shrinkage intensity to all eigenvalues (pulling them towards the mean). However, RMT tells us that small sample eigenvalues are biased downwards and large ones upwards. A more optimal approach would be to apply a non-linear transformation \\(\\phi(\\lambda_i)\\) to each sample eigenvalue \\(\\lambda_i\\).\n\n5.1 The QuEST Function and Direct Kernel Method\nLedoit and Wolf (2012, 2020) developed a method to estimate the optimal non-linear shrinkage formula. The key quantity is the Stieltjes transform \\(m(z)\\) of the limiting spectral distribution of the sample covariance matrix.\nFor a complex number \\(z \\in \\mathbb{C}^+\\), the Stieltjes transform is defined as: \\[\nm(z) = \\int \\frac{1}{\\lambda - z} dF(\\lambda)\n\\]\nThe optimal non-linear shrinkage formula for the eigenvalues is given by: \\[\nd_i^* = \\frac{\\lambda_i}{|1 - q^{-1} - q^{-1} \\lambda_i \\breve{m}(\\lambda_i)|^2}\n\\] where \\(\\breve{m}(\\lambda)\\) is the limit of the Stieltjes transform as \\(z\\) approaches the real axis from above: \\(\\breve{m}(\\lambda) = \\lim_{\\eta \\to 0^+} m(\\lambda + i\\eta)\\).\nIn the Direct Kernel method (2020), we estimate \\(\\breve{m}(\\lambda)\\) directly from the sample eigenvalues using a kernel density estimator, avoiding numerical inversion of the QuEST function.\n\n\n5.2 The Direct Kernel Workflow\nThe practical implementation of Nonlinear Shrinkage involves the following steps:\n\nEigendecomposition: Compute the eigenvalues \\(\\lambda_i\\) and eigenvectors \\(u_i\\) of the sample covariance matrix \\(S\\).\nSpectral Density Estimation: Use a Kernel Density Estimator (KDE) to estimate the limiting spectral density \\(f(\\lambda)\\) from the sample eigenvalues.\nHilbert Transform: Compute the Hilbert transform of the estimated density to obtain the real part of the Stieltjes transform \\(\\breve{m}(\\lambda)\\).\nOptimal Correction: Apply the QuEST formula to map each sample eigenvalue \\(\\lambda_i\\) to its optimal population counterpart \\(d_i^*\\).\nReconstruction: Form the new estimator \\(\\hat{\\Sigma}_{nl} = \\sum_{i=1}^N d_i^* u_i u_i^\\top\\).\n\nThis method is computationally efficient and does not require numerical optimization. In Python, the PyRMT library offers a production-ready implementation.\n\n\n5.3 Implementation\nWe implement a simplified version of the Direct Kernel estimator.\n\n\nCode\ndef nonlinear_shrinkage(X):\n    \"\"\"\n    Computes the Nonlinear Shrinkage estimator using the Direct Kernel method.\n    Reference: Ledoit, O. and Wolf, M. (2020). \"The Power of (Non-)Linear Shrinking\"\n    \n    Args:\n        X: (T, N) array of observations.\n        \n    Returns:\n        Sigma_nonlinear: (N, N) nonlinearly shrunk covariance matrix.\n    \"\"\"\n    T, N = X.shape\n    S = np.cov(X, rowvar=False)\n    evals, evecs = np.linalg.eigh(S)\n    \n    # Sort eigenvalues\n    idx = evals.argsort()\n    evals = evals[idx]\n    evecs = evecs[:, idx]\n    \n    # 1. Estimate the Stieltjes transform m(z)\n    # We use a kernel density estimator for the spectral density f(lambda)\n    # Bandwidth h ~ N^(-1/3) is a common heuristic for this problem\n    h = T**(-0.35) \n    \n    # Discretize the spectrum\n    # We evaluate m(lambda_i) for each sample eigenvalue\n    lambda_i = evals\n    \n    # m(x) = 1/N sum_j 1/(lambda_j - z)\n    # We need m(lambda_i + i*h) approx\n    # The Direct Kernel estimator uses a specific kernel K\n    \n    # Simplified implementation:\n    # m_hat(x) = 1/N \\sum_{j=1}^N \\frac{1}{\\lambda_j - x - i h}\n    # This is equivalent to using a Cauchy kernel\n    \n    z = lambda_i + 1j * h\n    \n    # Vectorized calculation of Stieltjes transform\n    # m[i] = mean( 1 / (evals - z[i]) )\n    # But we need to be careful with dimensions.\n    # We want m evaluated at each z_i corresponding to lambda_i\n    \n    # m_hat = np.zeros(N, dtype=complex)\n    # for k in range(N):\n    #     m_hat[k] = np.mean(1 / (evals - z[k]))\n        \n    # Fully vectorized:\n    # z is (N,), evals is (N,)\n    # We need outer subtraction\n    diff = evals.reshape(1, -1) - z.reshape(-1, 1) # (N, N)\n    m_hat = np.mean(1 / diff, axis=1)\n    \n    # 2. Apply the nonlinear shrinkage formula\n    # d_tilde = lambda / |1 - (T/N)^(-1) - (T/N)^(-1) * lambda * m_hat|^2\n    # Note: Ledoit-Wolf use concentration ratio c = N/T.\n    c = N / T\n    \n    # The formula in LW2020 (Eq 2.5) for the \"quantized\" estimator:\n    # d_i = lambda_i / | 1 - c - c * lambda_i * m_hat(lambda_i) |^2\n    \n    denom = np.abs(1 - c - c * lambda_i * m_hat)**2\n    d_star = lambda_i / denom\n    \n    # 3. Reconstruct the matrix\n    # Sigma_hat = U @ diag(d_star) @ U.T\n    Sigma_nonlinear = evecs @ np.diag(d_star) @ evecs.T\n    \n    return Sigma_nonlinear, d_star\n\n\nThe core of nonlinear shrinkage is the mapping from sample eigenvalues \\(\\lambda_i\\) to population eigenvalues \\(d_i^*\\). Let’s visualize this mapping function.\n\n\nCode\n# Generate data for visualization\nN_viz = 500\nT_viz = 1000 # q = 2\nX_viz = np.random.normal(0, 1, (T_viz, N_viz))\n_, d_star_viz = nonlinear_shrinkage(X_viz)\nS_viz = np.cov(X_viz, rowvar=False)\nevals_viz = np.linalg.eigvalsh(S_viz)\n\ndf_shrinkage = pd.DataFrame({\n    'Sample Eigenvalue': evals_viz,\n    'Shrunk Eigenvalue': d_star_viz\n})\n\n# Theoretical 45-degree line (No Shrinkage)\nline_df = pd.DataFrame({'x': [0, max(evals_viz)], 'y': [0, max(evals_viz)]})\n\nplot_func = (\n    ggplot(df_shrinkage, aes(x='Sample Eigenvalue', y='Shrunk Eigenvalue'))\n    + geom_point(color=PALETTE[0], alpha=0.3, size=0.5)\n    + geom_line(aes(x='x', y='y'), data=line_df, linetype='dashed', color='black', size=1)\n    + labs(\n        title=\"Nonlinear Shrinkage Function\",\n        x=\"Sample Eigenvalue\",\n        y=\"Shrunk Eigenvalue\"\n    )\n    + THEME_ACADEMIC\n)\n\ndisplay(plot_func)\n\n\n\n\n\nMapping from sample eigenvalues to optimal population eigenvalues (N=500, T=1000).\n\n\n\n\nThis plot reveals the “S-shape” characteristic of optimal shrinkage: 1. Small eigenvalues (left) are pulled up (above the dashed line). 2. Large eigenvalues (right) are pulled down (below the dashed line). 3. This counteracts the “repulsion” effect predicted by Random Matrix Theory."
  },
  {
    "objectID": "posts/23-11-2025_covariance-shrinkage/index.html#simulation-study",
    "href": "posts/23-11-2025_covariance-shrinkage/index.html#simulation-study",
    "title": "Covariance Shrinkage: From Linear Shrinkage to Random Matrix Theory",
    "section": "6 Simulation Study",
    "text": "6 Simulation Study\nWe will now simulate a scenario to demonstrate:\n\nThe fit of the sample covariance eigenvalues to the MP distribution (for pure noise).\nThe recovery of a true covariance structure using both Linear and Nonlinear shrinkage.\nThe specific adjustment of eigenvalues performed by the nonlinear method.\n\n\n6.1 Scenario 1: Pure Noise (Null Hypothesis)\nWe generate a random matrix \\(X \\in \\mathbb{R}^{T \\times N}\\) with \\(N=200, T=1000\\) (so \\(q=5\\)) from \\(\\mathcal{N}(0, I)\\). The true eigenvalues are all 1.\n\n\nCode\n# Define MP PDF function here to ensure it is available for plotting\ndef marchenko_pastur_pdf(var, q, pts=1000):\n    ratio = 1/q \n    lambda_min = var * (1 - np.sqrt(ratio))**2\n    lambda_max = var * (1 + np.sqrt(ratio))**2\n    x = np.linspace(lambda_min, lambda_max, pts)\n    def mp_density(l):\n        return (1 / (2 * np.pi * var * ratio * l)) * np.sqrt((lambda_max - l) * (l - lambda_min))\n    y = mp_density(x)\n    y = np.nan_to_num(y)\n    return pd.DataFrame({'x': x, 'y': y})\n\nN = 200\nT = 1000\nsigma_sq = 1.0\n\n# Generate random data\nX_noise = np.random.normal(0, np.sqrt(sigma_sq), (T, N))\n\n# Sample Covariance\nS_noise = np.cov(X_noise, rowvar=False)\nevals_noise = np.linalg.eigvalsh(S_noise)\n\n# Theoretical MP PDF\nmp_dist = marchenko_pastur_pdf(sigma_sq, q=T/N)\n\n# Plotting\ndf_evals = pd.DataFrame({'eigenvalue': evals_noise})\n\nplot_mp = (\n    ggplot()\n    + geom_histogram(\n        aes(x='eigenvalue', y='stat(density)'), \n        data=df_evals, \n        bins=30, \n        fill=PALETTE[1], \n        alpha=0.7,\n        color=\"white\"\n    )\n    + geom_line(\n        aes(x='x', y='y'), \n        data=mp_dist, \n        color=PALETTE[2], \n        size=1.5\n    )\n    + labs(\n        title=\"Eigenvalues vs Marchenko-Pastur\",\n        x=\"Eigenvalue\",\n        y=\"Density\"\n    )\n    + THEME_ACADEMIC\n)\n\ndisplay(plot_mp)\n\n\n\n\n\nHistogram of sample eigenvalues (N=200, T=1000) vs theoretical Marchenko-Pastur density.\n\n\n\n\nThe histogram of sample eigenvalues perfectly matches the theoretical Marchenko-Pastur curve. Note that while the true eigenvalues are all exactly 1.0, the sample eigenvalues range from \\(\\approx 0.6\\) to \\(\\approx 1.5\\). This “smearing” of the spectrum is the noise dressing effect.\n\n\n6.2 Scenario 2: Signal + Noise and Shrinkage Performance\nNow consider a case where the true covariance matrix \\(\\Sigma\\) has some structure (Signal) plus noise. We will compare the Sample Covariance, Ledoit-Wolf Linear Shrinkage, and Nonlinear Shrinkage in terms of their ability to recover the true matrix.\nWe define the Normalized Frobenius Loss as: \\[\n\\text{Loss}(\\hat{\\Sigma}, \\Sigma) = \\frac{\\| \\hat{\\Sigma} - \\Sigma \\|_F}{\\| \\Sigma \\|_F}\n\\]\nWe will vary the sample size \\(T\\) while keeping \\(N\\) fixed to see how the estimators converge.\n\n\nCode\ndef generate_structured_covariance(N, rho=0.7):\n    \"\"\"Generates a Toeplitz covariance matrix (AR(1) process).\"\"\"\n    Sigma = np.zeros((N, N))\n    for i in range(N):\n        for j in range(N):\n            Sigma[i, j] = rho**(abs(i - j))\n    return Sigma\n\nN = 100\nT_values = [50, 100, 200, 500, 1000]\nrho = 0.5\nSigma_true = generate_structured_covariance(N, rho)\n\nresults = []\n\nfor t in T_values:\n    # Run multiple trials to smooth results\n    for trial in range(20):\n        # Generate Data X ~ N(0, Sigma)\n        # X = Z * chol(Sigma)\n        Z = np.random.normal(0, 1, (t, N))\n        L = np.linalg.cholesky(Sigma_true)\n        X = Z @ L.T\n        \n        # 1. Sample Covariance\n        S = np.cov(X, rowvar=False)\n        loss_sample = np.linalg.norm(S - Sigma_true, 'fro') / np.linalg.norm(Sigma_true, 'fro')\n        \n        # 2. Linear Shrinkage (Ledoit-Wolf)\n        S_lw, delta = ledoit_wolf_shrinkage(X)\n        loss_lw = np.linalg.norm(S_lw - Sigma_true, 'fro') / np.linalg.norm(Sigma_true, 'fro')\n        \n        # 3. Nonlinear Shrinkage\n        S_nl, _ = nonlinear_shrinkage(X)\n        loss_nl = np.linalg.norm(S_nl - Sigma_true, 'fro') / np.linalg.norm(Sigma_true, 'fro')\n        \n        results.append({'T': t, 'Method': 'Sample Covariance', 'Loss': loss_sample})\n        results.append({'T': t, 'Method': 'Linear Shrinkage', 'Loss': loss_lw})\n        results.append({'T': t, 'Method': 'Nonlinear Shrinkage', 'Loss': loss_nl})\n\ndf_results = pd.DataFrame(results)\n\n# Aggregate for plotting\ndf_summary = df_results.groupby(['T', 'Method'])['Loss'].mean().reset_index()\n\nplot_loss = (\n    ggplot(df_summary, aes(x='T', y='Loss', color='Method', group='Method'))\n    + geom_line(size=1.5)\n    + geom_point(size=3, fill=\"white\", stroke=1.5)\n    + scale_color_manual(values=[PALETTE[0], PALETTE[2], PALETTE[7]])\n    + scale_y_log10()\n    + labs(\n        title=\"Estimator Convergence\",\n        x=\"Sample Size (T)\",\n        y=\"Loss (Log Scale)\"\n    )\n    + THEME_ACADEMIC\n)\n\ndisplay(plot_loss)\n\n\n\n\n\nNormalized Frobenius loss of covariance estimators as sample size T increases (N=100 fixed).\n\n\n\n\n\n\n6.3 Scenario 3: Eigenvalue Adjustment\nTo visualize how Nonlinear Shrinkage works, let’s look at the eigenvalues directly. We use a “Spiked Covariance” model where a few eigenvalues are large (signal) and the rest are 1 (noise).\n\n\nCode\n# Spiked Covariance Model\nN = 200\nT = 400 # q = 2\nn_spikes = 10\nspike_val = 10.0\n\nSigma_spiked = np.eye(N)\nSigma_spiked[:n_spikes, :n_spikes] = spike_val * np.eye(n_spikes)\n\n# Generate Data\nZ = np.random.normal(0, 1, (T, N))\nL = np.linalg.cholesky(Sigma_spiked)\nX_spiked = Z @ L.T\n\n# Compute Estimators\nS_spiked = np.cov(X_spiked, rowvar=False)\nS_nl_spiked, evals_nl = nonlinear_shrinkage(X_spiked)\n\n# Extract Eigenvalues\nevals_sample = np.linalg.eigvalsh(S_spiked)\nevals_true = np.linalg.eigvalsh(Sigma_spiked)\n# Sort descending\nevals_sample = np.sort(evals_sample)[::-1]\nevals_true = np.sort(evals_true)[::-1]\nevals_nl = np.sort(evals_nl)[::-1]\n\n# Create DataFrame for plotting\ndf_scree = pd.DataFrame({\n    'Rank': np.tile(np.arange(1, N+1), 3),\n    'Eigenvalue': np.concatenate([evals_true, evals_sample, evals_nl]),\n    'Type': ['True Population'] * N + ['Sample (Noisy)'] * N + ['Nonlinear Shrinkage'] * N\n})\n\nplot_scree = (\n    ggplot(df_scree, aes(x='Rank', y='Eigenvalue', color='Type'))\n    + geom_line(size=1.2)\n    + scale_y_log10()\n    + scale_color_manual(values=[PALETTE[7], PALETTE[0], PALETTE[2]])\n    + labs(\n        title=\"Scree Plot\",\n        x=\"Rank\",\n        y=\"Eigenvalue (Log)\"\n    )\n    + THEME_ACADEMIC\n)\n\ndisplay(plot_scree)\n\n\n\n\n\nScree plot comparing true population eigenvalues (spiked model), sample eigenvalues, and nonlinearly shrunk eigenvalues.\n\n\n\n\n\n\n6.4 Interpretation\n\nConvergence Plot: Nonlinear shrinkage consistently outperforms the sample covariance. In many regimes, especially where \\(N/T\\) is large, it can also outperform linear shrinkage because it adapts to the specific shape of the spectrum rather than just shrinking everything towards a single point.\nScree Plot: The sample eigenvalues (purple) overestimate the signal spikes and underestimate the noise floor (the “smearing” effect). The nonlinear shrinkage (teal) successfully pulls the large eigenvalues down and the small eigenvalues up, much closer to the true population spectrum (red)."
  },
  {
    "objectID": "posts/23-11-2025_covariance-shrinkage/index.html#conclusion",
    "href": "posts/23-11-2025_covariance-shrinkage/index.html#conclusion",
    "title": "Covariance Shrinkage: From Linear Shrinkage to Random Matrix Theory",
    "section": "7 Conclusion",
    "text": "7 Conclusion\nIn high-dimensional statistics, the sample covariance matrix is untrustworthy due to the “curse of dimensionality,” manifesting as a deterministic spreading of eigenvalues described by the Marchenko-Pastur law.\n\nLinear shrinkage (Ledoit-Wolf) offers a robust improvement by pulling the matrix towards a stable target, effectively reducing variance at the cost of some bias.\nNonlinear shrinkage takes this a step further by using Random Matrix Theory to optimally correct individual eigenvalues. It effectively “inverts” the Marchenko-Pastur equation to recover the population spectrum.\n\nFor practitioners, implementing nonlinear shrinkage (or using libraries like PyRMT) is a powerful way to clean covariance matrices before feeding them into sensitive algorithms like Mean-Variance Optimization.\n\n7.1 Practitioner’s Guide: When to use what?\n\nSmall \\(N/T\\) (e.g., &lt; 0.1): The sample covariance matrix is generally sufficient and well-conditioned.\nModerate \\(N/T\\): Linear shrinkage (Ledoit-Wolf) is a robust, low-variance estimator that is easy to implement and interpret. It is the standard baseline in finance.\nLarge \\(N/T\\) (but &lt; 1): Nonlinear shrinkage provides significant gains over linear shrinkage by correcting the specific spectral distortion predicted by RMT. It is ideal for high-dimensional portfolios.\n\\(N &gt; T\\) (Singular Regime): The sample covariance is singular. Shrinkage is strictly necessary.\n\nIf \\(T\\) is reasonably large (e.g., \\(T &gt; 100\\)), Nonlinear Shrinkage is preferred as it can still recover the spectrum effectively.\nIf \\(T\\) is very small (Extreme Scarcity), Linear Shrinkage is often safer. The estimation of the spectral density required for nonlinear shrinkage becomes noisy, whereas linear shrinkage relies on a stable, structured target (like the identity or a factor model) to regularize the solution heavily."
  },
  {
    "objectID": "posts/23-11-2025_covariance-shrinkage/index.html#references",
    "href": "posts/23-11-2025_covariance-shrinkage/index.html#references",
    "title": "Covariance Shrinkage: From Linear Shrinkage to Random Matrix Theory",
    "section": "8 References",
    "text": "8 References\n\nLedoit, O., & Wolf, M. (2004). A well-conditioned estimator for large-dimensional covariance matrices. Journal of Multivariate Analysis.\nLedoit, O., & Wolf, M. (2020). The Power of (Non-)Linear Shrinking: A Review and Guide to Covariance Matrix Estimation. Journal of Financial Econometrics.\nMarchenko, V. A., & Pastur, L. A. (1967). Distribution of eigenvalues for some sets of random matrices. Matematicheskii Sbornik.\nPotters, M., & Bouchaud, J. P. (2020). A First Course in Random Matrix Theory: For Physicists, Engineers and Data Scientists. Cambridge University Press."
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html",
    "href": "posts/07-02-2025_portfolio-var/index.html",
    "title": "Comparing Univariate and Multivariate Models for Value at Risk Forecasting",
    "section": "",
    "text": "Value at Risk (VaR) is a crucial metric in modern financial risk management"
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#sec-introduction",
    "href": "posts/07-02-2025_portfolio-var/index.html#sec-introduction",
    "title": "Comparing Univariate and Multivariate Models for Value at Risk Forecasting",
    "section": "1 Introduction",
    "text": "1 Introduction\nThe past four decades have been shaped by extreme events in financial markets, such as the Black Monday crash, the dot-com bubble, and the 2008 global financial crisis. These supposedly rare events highlighted that reducing systemic risk is crucial for financial stability (Embrechts et al. 2014). This led to the introduction and tightening of the Basel Accords, which use risk measures to determine appropriate risk capital requirements for financial institutions.\nValue at Risk (VaR) remains the most popular measure for downside market risk, despite a shift towards the severity-based expected shortfall (ES) (Embrechts et al. 2014). For a long equity portfolio, the \\(p\\)% VaR for period \\(t\\) forecasted at time \\(t-1\\) is defined as the negative \\(p\\)-quantile of the conditional portfolio return distribution:\n\\[\n\\text{VaR}_t^p=-Q_p(r_{\\text{PF},t}|\\mathcal{F}_{t-1})=-\\inf_x\\{x\\in\\mathbb{R}:\\mathbb{P}(r_{\\text{PF},t}\\leq x|\\mathcal{F}_{t-1})\\geq p\\},\\quad p\\in(0,1).\n\\tag{1}\\]\nHere, \\(Q_p(\\cdot)\\) denotes the quantile function and \\(\\mathcal{F}_{t-1}\\) represents all information available at time \\(t-1\\). The parameter \\(p\\) indicates that with target probability \\(p\\), the portfolio losses will exceed the VaR (Marc S. Paolella, Kuester, and Mittnik 2006).\nDue to the practical relevance of VaR, it is essential to determine estimation methods that neither severely underestimate nor overestimate future losses. Many models use the generalized autoregressive conditional heteroskedasticity (GARCH) framework (Bollerslev 1986) or extensions to account for volatility clustering and the “leverage effect” in financial time series.\nA fundamental question in VaR modeling is whether more complex multivariate models outperform simpler univariate alternatives. Santos, Nogales, and Ruiz (2013) found that multivariate models significantly outperform univariate counterparts for large portfolios, while Kole et al. (2017) found that multivariate models have greater predictive ability, though differences are often not significant. They also found that data frequency is more important than model choice.\nThis study compares factor copula-DCC-NGARCH models introduced by Fortin, Simonato, and Dionne (2022) with established models like the diagonal MixN(k)-GARCH (Haas, Mittnik, and Paolella 2004) and the COMFORT model class (Marc S. Paolella and Polak 2015). We show that multivariate models display desirable VaR properties in terms of correct unconditional coverage and independence of violations, though we don’t find sufficient evidence to claim that multivariate approaches outperform univariate procedures in terms of forecast ability, or vice versa."
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#sec-methodology",
    "href": "posts/07-02-2025_portfolio-var/index.html#sec-methodology",
    "title": "Comparing Univariate and Multivariate Models for Value at Risk Forecasting",
    "section": "2 Methodology",
    "text": "2 Methodology\n\n2.1 Univariate Models\nFor univariate models, we assume the following portfolio return dynamics:\n\\[\nr_{\\text{PF},t} = \\mu + \\epsilon_t\n\\]\nwhere\n\\[\n\\epsilon_t = \\sigma_t z_t, \\quad z_t \\stackrel{iid}{\\sim} F(0,1)\n\\]\nHere, \\(F(0,1)\\) is a standardized distribution, \\(\\mu\\) is the unconditional location, and \\(\\sigma_t\\) is the scale parameter. The conditional variance \\(\\sigma_t^2 = \\mathbb{V}[r_{\\text{PF},t}|\\mathcal{F}_{t-1}]\\) is modeled as non-constant.\n\n2.1.1 GARCH and Extensions\nThe standard GARCH(1,1) model (Bollerslev 1986) is formulated as:\n\\[\n\\sigma_t^2 = \\omega + \\alpha\\epsilon_{t-1}^2 + \\beta\\sigma_{t-1}^2\n\\tag{2}\\]\nwhere \\(\\omega &gt; 0, \\alpha \\geq 0\\), and \\(\\beta \\geq 0\\). For covariance stationarity, the parameters must satisfy \\(\\alpha + \\beta &lt; 1\\).\nA special case is the exponentially weighted moving average (EWMA):\n\\[\n\\sigma^2_t = \\lambda\\sigma^2_{t-1} + (1-\\lambda)\\epsilon_t^2, \\quad \\lambda \\in (0,1)\n\\tag{3}\\]\nThis formulation puts more weight on recent observations, but is not covariance stationary since \\(\\lambda + (1-\\lambda) = 1\\).\nTo account for the “leverage effect” (negative news increasing volatility more than positive news of equal magnitude), we include the GJR-GARCH(1,1) model (Glosten, Jagannathan, and Runkle 1993):\n\\[\n\\sigma_t^2 = \\omega + (\\alpha + \\gamma I_{t-1})\\epsilon_{t-1}^2 + \\beta\\sigma_{t-1}^2\n\\tag{4}\\]\nwhere \\(I_{t-1} = \\mathbb{I}_{\\{\\epsilon_{t-1} &lt; 0\\}}\\) is an indicator function.\nAnother asymmetric model is the NGARCH(1,1) (R. F. Engle and Ng 1993):\n\\[\n\\sigma_t^2 = \\omega + \\alpha\\sigma_{t-1}^2(\\epsilon_{t-1} - \\theta)^2 + \\beta\\sigma_{t-1}^2\n\\]\nFor \\(\\theta &gt; 0\\), negative innovations have a larger impact on conditional variance than positive errors of the same magnitude.\n\n\n2.1.2 Mixed Normal GARCH\nWe also include the k-component mixed normal GARCH(1,1) (MixN(k)-GARCH) (Haas, Mittnik, and Paolella 2004), where the conditional distribution of the error term \\(\\epsilon_t\\) is assumed to be mixed normal with zero mean:\n\\[\n\\epsilon_t|\\mathcal{F}_{t-1} \\sim \\text{Mix}_k\\text{N}(p_1,...,p_k, \\mu_1,...,\\mu_k, \\sigma_{1,t}^2,...,\\sigma_{k,t}^2), \\quad \\sum_{i=1}^k p_i\\mu_i = 0\n\\]\nThe associated conditional variances follow GARCH processes:\n\\[\n\\sigma_{i,t}^2 = \\omega_i + \\alpha_i\\epsilon_{i,t-1}^2 + \\beta_i\\sigma_{i,t-1}^2, \\quad i=1,...,k\n\\]\n\n\n\n2.2 Multivariate Models\n\n2.2.1 Factor Copula-DCC-GARCH Model\nThe factor copula model proposed by Fortin, Simonato, and Dionne (2022) uses equity factors to capture the main risks of stock returns. It utilizes the Carhart four-factor model (Carhart 1997), which adds a momentum factor to the Fama-French three-factor model (Fama and French 1993):\n\\[\nr_{k,t} - r_{f,t} = \\alpha_{k,t} + \\beta_{k, \\text{RMRF}}\\text{RMRF}_t + \\beta_{k,\\text{SMB}}\\text{SMB}_t + \\beta_{k,\\text{HML}}\\text{HML}_t + \\beta_{k, \\text{MOM}}\\text{MOM}_t + \\varepsilon_{k,t}\n\\tag{5}\\]\nor in vector form:\n\\[\nr_{k,t} - r_{f,t} = \\alpha_{k,t} + \\mathbf{\\beta}_k'\\mathbf{r}_{\\text{F},t} + \\varepsilon_{k,t}\n\\tag{6}\\]\nThis reduces dimensionality by modeling only four factors instead of all portfolio constituents.\nFor the factor dynamics, we use the Dynamic Conditional Correlation (DCC) structure (R. Engle 2002), which decomposes the conditional covariance matrix into standard deviations and correlations:\n\\[\n\\mathbf{Y}_{t}|\\mathcal{F}_{t-1} \\sim \\mathcal{N}_n(\\mathbf{\\mu}, \\mathbf{\\Sigma_t}), \\quad \\mathbf{\\Sigma_t} = \\mathbf{D_t}\\mathbf{\\Gamma_t}\\mathbf{D_t}\n\\tag{7}\\]\nwhere \\(\\mathbf{D_t} = \\text{diag}(\\sigma_{1,t},\\sigma_{2,t},...,\\sigma_{n,t})\\) contains the conditional standard deviations.\nTo account for non-normality, we use copulas to model the joint conditional distribution of factor returns. Copulas allow modeling marginals independently of the multivariate distribution. By Sklar’s theorem:\n\\[\n\\mathbf{F_t}(\\mathbf{z_t}) = \\mathbf{C_t}(F_{1,t}(z_{1,t}),...,F_{n,t}(z_{n,t}))\n\\tag{8}\\]\nwhere \\(\\mathbf{F_t}(\\mathbf{z_t})\\) is the joint conditional distribution, \\(F_{i,t}(\\cdot)\\) are the conditional marginals, and \\(\\mathbf{C_t}:[0,1]^n \\rightarrow [0,1]\\) is the conditional copula.\n\n\n2.2.2 COMFORT Model\nThe Common Market Factor Non-Gaussian Returns (COMFORT) model (Marc S. Paolella and Polak 2015) uses a multivariate generalized hyperbolic (MGHyp) distribution with a CCC or DCC structure for the covariance matrix. This model can be expressed as a continuous normal mixture:\n\\[\n\\mathbf{Y_t} = \\mathbf{\\mu} + \\mathbf{\\gamma} G_t + \\mathbf{\\varepsilon_t}, \\quad \\mathbf{\\varepsilon_t} = \\mathbf{\\Sigma_t}^{1/2}\\sqrt{G_t}\\mathbf{Z_t}\n\\tag{9}\\]\nwhere \\(\\mathbf{Z_t} \\stackrel{iid}{\\sim} \\mathcal{N}_n(\\mathbf{0},\\mathbf{I_n})\\) and the mixing random variables \\(G_t|\\mathcal{F}_{t-1} \\sim \\text{GIG}(\\lambda_t,\\chi_t,\\psi_t)\\) follow a generalized inverse Gaussian distribution.\nAn important property of the MGHyp distribution is that it is closed under linear operations. Therefore, portfolio returns \\(r_{\\text{PF},t} = \\mathbf{w}'\\mathbf{Y_t}\\) are univariate GHyp distributed:\n\\[\nr_{\\text{PF},t}|\\mathcal{F}_{t-1} \\sim \\text{GHyp}(\\mathbf{w}'\\mathbf{\\mu},\\mathbf{w}'\\mathbf{\\gamma},\\mathbf{w}'\\mathbf{\\Sigma_t}\\mathbf{w},\\lambda_t,\\chi_t, \\psi_t)\n\\tag{10}\\]\n\n\n\n2.3 Data\nWe use an equally weighted portfolio of ten large-cap stocks (identical to those used by Fortin, Simonato, and Dionne (2022)): Boeing, Caterpillar, Chevron, Coca-Cola, Exxon, GE, IBM, Merck, P&G, and UTC. However, we analyze 2,767 daily returns from January 2, 2001, to December 30, 2011, rather than weekly returns.\nThe return data shows that most factors and stocks have means close to zero, with the median larger than the mean in most cases. The mean absolute deviation (MAD) is considerably smaller than the standard deviation, indicating the presence of outliers. Most returns are left-skewed with leptokurtic behavior, and all return distributions reject the assumption of normality based on Jarque-Bera statistics.\n\n\n\n\n\n\n\n\nFigure 1: ACF Plots of the Fama-French-Carhart Factors\n\n\n\n\n\nFigure Figure 1 shows autocorrelation function (ACF) plots for the factors. It is evident that the factors exhibit stronger autocorrelation in absolute returns than in the returns themselves, justifying the use of volatility models.\n\n\n\n\n\n\n\n\nFigure 2: Chi-Square Q-Q Plots of the Stock and Factor Returns\n\n\n\n\n\nFigure Figure 2 shows Q-Q plots of the robust squared Mahalanobis distances against \\(\\chi^2\\) distributions. The non-linear relationship indicates large multivariate outliers and multivariate non-normality.\n\n\n\n\n\n\n\n\nFigure 3: Portfolio Returns\n\n\n\n\n\nFigure Figure 3 demonstrates blatant volatility clustering (Panel A) and that the portfolio returns are not normally distributed (Panel B).\n\n\n2.4 Value at Risk Forecasts\nFor all models, we assume a constant conditional mean over time. Forecasting uses a rolling window approach with the previous 1,000 observations to predict the one-step-ahead VaR.\nFor univariate GARCH models (except MixN(k)-GARCH), we use the analytical formula:\n\\[\n\\widehat{\\text{VaR}_t^p} = -\\mu_{\\text{PF}} - \\sigma_{\\text{PF}, t} Q_p(z_t|\\mathcal{F}_{t-1})\n\\tag{11}\\]\nwhere \\(\\sigma_{\\text{PF}, t}\\) is the conditional standard deviation and \\(Q_p(z_t)\\) is the p-quantile of the standardized returns.\nFor the factor copula-DCC-(N)GARCH models, we simulate factor returns, apply the Carhart model to generate single stock returns, and calculate the portfolio return. The VaR estimate is then the negative p-quantile of the simulated portfolio returns.\nFor the COMFORT model, we use the p-quantile function of the corresponding conditional univariate GHyp distribution:\n\\[\n\\widehat{\\text{VaR}_t^p} = -Q_p(r_{\\text{PF},t}|\\mathcal{F}_{t-1})\n\\tag{12}\\]\n\n\n2.5 Backtesting\nBacktesting checks whether the forecasts exhibit desirable properties. Following Christoffersen (1998), we use three likelihood-ratio tests.\nLet \\(I_t\\) be the indicator variable for a \\(\\text{VaR}_t^p\\) forecast:\n\\[\nI_t = \\mathbb{I}_{\\{r_{\\text{PF},t} &lt; -\\text{VaR}_t^p\\}} =\n\\begin{cases}\n1 & \\text{if } r_{\\text{PF},t} &lt; -\\text{VaR}_t^p \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nA sequence of VaR forecasts is efficient with respect to \\(\\mathcal{F}_{t-1}\\) if:\n\\[\n\\mathbb{E}[I_t|\\mathcal{F}_{t-1}] = \\mathbb{E}[I_t|I_{t-1},I_{t-2},...,I_1] = p, \\quad t=1,2,...,T\n\\tag{13}\\]\nThis is equivalent to testing that \\(\\{I_t\\}_{t=1}^T \\overset{\\text{iid}}{\\sim} \\text{Bernoulli}(p)\\).\n\n2.5.1 Unconditional Coverage Test\nThis tests whether the expected value of \\(I_t\\) equals \\(p\\):\n\\[\nH_0: \\mathbb{E}[I_t] = p \\quad \\text{versus} \\quad H_A: \\mathbb{E}[I_t] \\neq p\n\\]\nThe likelihood-ratio test statistic is:\n\\[\nLR_{uc} = -2\\log\\left(\\frac{L(p;I_1,I_2,...,I_T)}{L(\\hat{p};I_1,I_2,...,I_T)}\\right) \\overset{\\text{asy}}{\\sim} \\chi_1^2\n\\tag{14}\\]\nwhere \\(\\hat{p} = \\frac{n_1}{n_0+n_1}\\) is the maximum likelihood estimate of \\(p\\).\n\n\n2.5.2 Independence Test\nThis tests whether the indicator sequence is independently distributed, against a first-order Markov chain alternative:\n\\[\nLR_{ind} = -2\\log\\left(\\frac{L(\\hat{\\Pi}_2;I_2,...,I_T|I_1)}{L(\\hat{\\Pi}_1;I_2,...,I_T|I_1)}\\right) \\overset{\\text{asy}}{\\sim} \\chi_1^2\n\\tag{15}\\]\nwhere \\(\\hat{\\Pi}_1\\) and \\(\\hat{\\Pi}_2\\) are the estimated transition probability matrices under the alternative and null hypotheses.\n\n\n2.5.3 Conditional Coverage Test\nThis combines the unconditional coverage and independence tests:\n\\[\nLR_{cc} = -2\\log\\left(\\frac{L(p;I_2,...,I_T|I_1)}{L(\\hat{\\Pi}_1;I_2,...,I_T|I_1)}\\right) \\overset{\\text{asy}}{\\sim} \\chi_2^2\n\\tag{16}\\]\nIt can also be calculated as:\n\\[\nLR_{cc} = LR_{uc} + LR_{ind}\n\\]\n\n\n\n2.6 Comparison of Predictive Ability\nTo rank VaR estimates, we use the “tick” loss function:\n\\[\nL_V(\\theta_t, r_{\\text{PF},t}) = (r_{\\text{PF},t} + \\theta_t)(p - \\mathbb{I}_{\\{r_{\\text{PF},t} &lt; -\\theta_t\\}})\n\\]\nThis loss function has the property that:\n\\[\nQ_p(r_{\\text{PF},t}|\\mathcal{F}_{t-1}) = -\\text{VaR}_t^p = \\arg\\min_{\\theta_t} \\mathbb{E}[L_V(\\theta_t, r_{\\text{PF},t})]\n\\tag{17}\\]\nFor statistical inference, we use the conditional predictive ability (CPA) test (Giacomini and White 2006). The null hypothesis of equal conditional predictive ability is:\n\\[\nH_0: \\mathbb{E}[\\Delta L_{i,j,t}|\\mathcal{F}_{t-1}] = 0 \\quad \\text{almost surely } \\forall t\n\\]\nwhere \\(\\Delta L_{i,j,t} = L_{V_{i,t}} - L_{V_{j,t}}\\) is the loss differential between models i and j.\nThe test statistic is:\n\\[\nGW_{i,j} = T\\bar{Z}'\\hat{\\Omega}\\bar{Z} \\overset{d}{\\rightarrow} \\chi^2_q \\quad \\text{as } T \\rightarrow \\infty\n\\]\nwhere \\(Z_t = h_{t-1}\\Delta L_{i,j,t}\\), \\(\\bar{Z} = \\frac{1}{T}\\sum_{t=2}^T Z_t\\), and \\(\\hat{\\Omega} = \\frac{1}{T}\\sum_{t=2}^T Z_t Z_t'\\)."
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#sec-results",
    "href": "posts/07-02-2025_portfolio-var/index.html#sec-results",
    "title": "Comparing Univariate and Multivariate Models for Value at Risk Forecasting",
    "section": "3 Results",
    "text": "3 Results\n\n3.1 Value at Risk Backtests\n\n\n\n\n\n\n\n\nFigure 4: Chi-Square Q-Q Plot of the Carhart OLS Residuals\n\n\n\n\n\nFigure Figure 4 shows the Q-Q plot of OLS residuals, clearly indicating their non-normality.\nTable 1 shows that while all univariate models passed the likelihood ratio test of independence, only a few showed adequate conditional or unconditional coverage. In particular, only the skewed-t GJR-GARCH passed the test of conditional coverage for both VaR levels. In contrast, most multivariate models passed all likelihood ratio tests, with the exception of the multivariate normal (MN)-DCC-GARCH.\nIt’s notable that using skewed-t NGARCH marginals in the factor copula framework leads to a very low percentage of violations compared to other models. Surprisingly, even using a normal copula with normal GARCH marginals for factor returns produces sound VaR estimates, possibly because much of the multivariate non-normality of stock returns might be captured in the bootstrapped OLS residuals.\nModels assuming normality of returns (GARCH, EWMA, and MN-DCC-GARCH) consistently show too many violations, with none displaying adequate conditional or unconditional coverage. For these models, the percentage of violations is higher than the corresponding VaR level. For the COMFORT and factor copula models, however, we observe fewer violations than expected.\nThe most adequate coverage is achieved by the factor copula-DCC models with GARCH marginals for the 1% VaR level and by the COMFORT models for the 5% level. The factor skewed-t copula with GARCH marginals belongs to the three models with the most appropriate coverage for both VaR percentiles.\nNearly all exceedances for the COMFORT models occurred during or after the 2008 financial crisis, despite passing the independence test. At the 1% VaR level, Gaussian models without leverage effects have approximately three times as many exceedances as expected. At the 5% VaR level, this discrepancy is smaller.\n\n\n3.2 Conditional Predictive Ability Tests\nIn terms of average tick loss, univariate models perform well despite their suboptimal backtesting results. The skewed-t GJR model had the lowest and the MN-DCC-GARCH the highest average loss for both VaR levels. Multivariate models achieve better ranks at the 1% level than at the 5% level, with factor copula-based models showing lower average loss than COMFORT models at both levels.\nWithin the factor copula-DCC models, GARCH marginals achieved lower mean losses than skewed-t NGARCH marginals, reinforcing the hypothesis that bootstrapped OLS residuals account for much of the non-normality in stock returns.\nThe CPA test results show that the MN-DCC-GARCH is significantly outperformed by every other model. Most rejections occur in univariate vs. univariate or multivariate vs. multivariate comparisons. For multivariate models, factor copula-DCC models using skewed-t NGARCH marginals have significantly higher predictive ability than their counterparts with normal GARCH marginals. The normal copula is superior for skewed-t NGARCH marginals, but for normal GARCH marginals, the t and skewed-t copula versions significantly outperform the normal copula.\nAt the 5% VaR level, in addition to MN-DCC-GARCH, the MixN(3)-GARCH is also significantly outperformed by all other models. The Student t GJR-GARCH, skewed-t GJR-GARCH, and MixN(2)-GARCH all display significantly higher predictive ability than the COMFORT models.\nInterestingly, the skewed-t GJR-GARCH, which significantly outperformed every other univariate model, did not achieve significantly better VaR forecasts than the factor copula-DCC-(N)GARCH models."
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#sec-conclusion",
    "href": "posts/07-02-2025_portfolio-var/index.html#sec-conclusion",
    "title": "Comparing Univariate and Multivariate Models for Value at Risk Forecasting",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nOur study assessed various univariate and multivariate models for VaR forecasting. Most univariate models produced inadequate VaR estimates with too many violations, while most multivariate models displayed adequate coverage with independently occurring VaR exceedances.\nThe CPA tests revealed that the MN-DCC-GARCH model is significantly outperformed by all other models, which is expected given the evident multivariate non-normality of stock returns. However, we found no general, significant outperformance of multivariate models over univariate ones, or vice versa, at either VaR level.\nOne important finding is that using daily returns (higher frequency data) makes the factor copula-DCC-NGARCH models feasible for VaR forecasting, consistent with Kole et al. (2017) who found that data frequency is more important than model choice for VaR forecasts.\nWe also found that replacing skewed-t NGARCH marginals with normal GARCH marginals for factor returns increased predictive accuracy and yielded better unconditional coverage. This may be because OLS residuals from the Carhart model capture most of the multivariate non-normality in stock returns.\nFor future research, it would be interesting to examine how the factor copula-DCC-GARCH model performs with larger portfolios, which would highlight the advantage of its dimensionality reduction. The only computationally expensive parts—fitting the DCC-GARCH structure and the copula—depend only on the number of factors, not the portfolio size."
  },
  {
    "objectID": "posts/15-02-2026_ml4hc-heart-disease/index.html#introduction",
    "href": "posts/15-02-2026_ml4hc-heart-disease/index.html#introduction",
    "title": "Predicting Heart Disease: When Simple Models Rival Deep Learning",
    "section": "1 Introduction",
    "text": "1 Introduction\nCardiovascular diseases remain by far the leading cause of death worldwide, and a lot of clinical decision-making still depends on a physician’s intuition and experience. The question we set out to answer in a course project for ETH’s Machine Learning for Healthcare class was straightforward: given a handful of routine clinical measurements, can we reliably predict whether a patient has heart disease? And more importantly, can we do so in a way that a cardiologist could actually scrutinize and trust?\nWe train three models that sit at very different points on the interpretability spectrum: a Lasso logistic regression (fully transparent), a multi-layer perceptron (a black box that needs post-hoc explanation), and a neural additive model (a recent architecture designed to combine neural network flexibility with additive model interpretability)."
  },
  {
    "objectID": "posts/15-02-2026_ml4hc-heart-disease/index.html#the-data",
    "href": "posts/15-02-2026_ml4hc-heart-disease/index.html#the-data",
    "title": "Predicting Heart Disease: When Simple Models Rival Deep Learning",
    "section": "2 The Data",
    "text": "2 The Data\nWe utilize the Heart Failure Prediction Dataset from Kaggle — a popular benchmark that combines several older heart disease studies into one clean table. Each of the 734 training samples is a patient described by 11 clinical features and a binary label: heart disease or not. The features cover the usual suspects you would find in a cardiology workup — age, resting blood pressure, cholesterol, maximum heart rate, ST segment depression during exercise — alongside categorical variables like sex, chest pain type, fasting blood sugar, resting ECG result, exercise-induced angina, and ST slope direction. For a detailed description of the features, please see the Kaggle dataset documentation.\nEven a quick pair plot of the numeric features hints at what the models will later confirm:\n\nPatients with heart disease tend to be older, reach lower maximum heart rates during exercise, and show higher Oldpeak values (a measure of ST segment depression on an electrocardiogram). None of these relationships are dramatic on their own, but in combination they start painting a picture.\nLooking at the categorical side, several features are heavily imbalanced (most study participants are male, most chest pain is asymptomatic), but the target variable itself is reasonably balanced — so no need for oversampling tricks."
  },
  {
    "objectID": "posts/15-02-2026_ml4hc-heart-disease/index.html#three-models-one-explainability-technique",
    "href": "posts/15-02-2026_ml4hc-heart-disease/index.html#three-models-one-explainability-technique",
    "title": "Predicting Heart Disease: When Simple Models Rival Deep Learning",
    "section": "3 Three Models, One Explainability Technique",
    "text": "3 Three Models, One Explainability Technique\n\n3.1 Lasso Logistic Regression\nStandard logistic regression models the probability of the positive class (heart disease) as a linear function of the features passed through a sigmoid:\n\\[\nP(y = 1 \\mid \\mathbf{x}) = \\sigma(\\mathbf{w}^\\top \\mathbf{x} + b) = \\frac{1}{1 + e^{-(\\mathbf{w}^\\top \\mathbf{x} + b)}}\n\\]\nwhere \\(\\mathbf{w} \\in \\mathbb{R}^p\\) is the weight vector, \\(b\\) is the bias, and \\(\\sigma\\) is the logistic sigmoid function. The model is trained by minimizing the negative log-likelihood (binary cross-entropy). The Lasso variant adds an \\(\\ell_1\\) penalty on the weights, giving the objective:\n\\[\n\\mathcal{L}(\\mathbf{w}, b) = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log \\hat{p}_i + (1 - y_i) \\log(1 - \\hat{p}_i) \\right] + \\lambda \\|\\mathbf{w}\\|_1\n\\]\nwhere \\(\\lambda &gt; 0\\) controls the regularization strength. The key property of the \\(\\ell_1\\) penalty is that it encourages sparsity i.e. the possibility to drive coefficients all the way to exactly zero whilst keeping the objective convex. Realize that the \\(\\ell_1\\) penalty is only applied to the weights \\(\\mathbf{w}\\), not the bias \\(b\\), to maintain translation invariance. Finally, the problem \\[\n\\arg\\min_{\\mathbf{w}, b} \\mathcal{L}(\\mathbf{w}, b)\n\\] can be solved efficiently using coordinate descent or proximal gradient methods.\nThe induced sparsity makes the Lasso a natural starting point for any clinical prediction task where interpretability is a hard requirement: each non-zero coefficient directly tells you the direction and magnitude of that feature’s influence on the log-odds of disease. Specifically, after selecting \\(\\lambda\\) through cross-validation and standardizing all numeric and dummy encoded features to ensure equal penalization, we obtained the coefficient plot below:\n\n\n\nThe two most prominent predictors are immediately visible. A flat ST slope has the largest positive coefficient in the model, while an upsloping ST slope has the largest negative coefficient, suggesting a protective association. This is consistent with cardiology literature: a flat or downsloping ST segment during exercise is a well-documented marker of myocardial ischemia, while an upsloping response is considered normal (Hodnesdal et al. 2013). Asymptomatic chest pain — which despite its name refers to a specific clinical presentation where patients experience atypical or no chest symptoms — has the third largest coefficient magnitude. This is a known red flag in cardiology, as patients who present without classic chest pain symptoms are often at higher risk because their disease goes undetected longer.\n\n\n3.2 SHAP Values\nMoving forward, we need a way to make the predictions of neural networks more explainable in a way that provides little additional overhead. Shapley additive explanations (SHAP) values (Lundberg and Lee (2017)) are the most well-known tool for such post-hoc explainability insights. SHAP is rooted in cooperative game theory with the idea to treat a prediction as a “game” where the features are “players” and the prediction is the “payout”, and then ask: how much did each player contribute? The Shapley value of feature \\(j\\) for a specific prediction is defined as:\n\\[\n\\phi_j = \\sum_{S \\subseteq \\{1, \\dots, p\\} \\setminus \\{j\\}} \\frac{|S|!\\;(p - |S| - 1)!}{p!} \\left[ f(S \\cup \\{j\\}) - f(S) \\right]\n\\]\nwhere \\(S\\) is a subset of features, \\(f(S)\\) is the model’s expected output when only the features in \\(S\\) are “present” (and the remaining features are marginalized out), and \\(p\\) is the total number of features. The sum iterates over all possible subsets \\(S\\) that exclude feature \\(j\\), and computes the marginal contribution of adding \\(j\\) to each subset, weighted by the number of permutations in which that subset would arise. This yields an explainability method that is theoretically grounded, model-agnostic, and provides local explanations for individual predictions. In practice, exact computation of SHAP values is intractable for large feature sets, so various approximation methods (like Kernel SHAP or Tree SHAP) are used depending on the model type.\n\n\n3.3 Multi-Layer Perceptron\nA multi-layer perceptron (MLP) is a feed-forward neural network consisting of an input layer, one or more hidden layers, and an output layer. Given an input \\(\\mathbf{x} \\in \\mathbb{R}^p\\), the forward pass through an MLP with \\(L\\) hidden layers computes:\n\\[\n\\mathbf{h}^{(0)} = \\mathbf{x}, \\qquad \\mathbf{h}^{(\\ell)} = \\phi\\!\\left(\\mathbf{W}^{(\\ell)} \\mathbf{h}^{(\\ell-1)} + \\mathbf{b}^{(\\ell)}\\right) \\quad \\text{for } \\ell = 1, \\dots, L\n\\]\nwhere \\(\\mathbf{W}^{(\\ell)}\\) and \\(\\mathbf{b}^{(\\ell)}\\) are the weight matrix and bias vector of layer \\(\\ell\\), and \\(\\phi\\) is a non-linear activation function (typically ReLU: \\(\\phi(z) = \\max(0, z)\\)). For binary classification, the final layer produces a scalar output passed through a sigmoid: \\(\\hat{p} = \\sigma(\\mathbf{w}^{(L+1)\\top} \\mathbf{h}^{(L)} + b^{(L+1)})\\).\nThe power of MLPs lies in their expressiveness — with sufficient width and depth, they can approximate arbitrarily complex functions (the universal approximation theorem). The downside is that the learned representations are distributed across many neurons, layers, and non-linear transformations, making it essentially impossible to attribute a prediction to any single input feature by inspecting the weights alone. To crack open this black box, we turned to SHAP values.\nThe waterfall plots below show SHAP explanations for two healthy patients and two patients with heart disease:\n\n\n\n Two healthy patients: ST Slope Upsloping is the dominant protective factor\n\n\n Two diseased patients: Old Peak and Cholesterol drive risk upward\n\n\n\nFor the healthy patients, having an upsloping ST slope was the strongest factor pushing the prediction toward “no disease”. For the diseased patients, high Oldpeak values and low cholesterol readings (likely the zero-imputed missing values) were the biggest risk drivers. The cholesterol finding might seem paradoxical at first — shouldn’t high cholesterol be bad? — but recall that the zero values were overwhelmingly associated with heart disease. The model is picking up on the missing-data signal, not on low cholesterol being protective.\nThe overall SHAP beeswarm plot across all test patients gives a broader view:\n\nEach dot is one patient’s SHAP value for a given feature, colored by whether the feature value was high (red) or low (blue). The pattern for ST Slope Upsloping is particularly clean: high feature values (red, meaning the patient does have an upsloping slope) cluster exclusively on the negative SHAP side, while low values (blue, meaning no upsloping slope) cluster on the positive side. This is exactly the same story the Lasso told us, just from a completely different model.\n\n\n3.4 Neural Additive Models\nNeural Additive Models (NAMs), introduced by Agarwal et al. (2020), are an elegant attempt to get the best of both worlds. The model belongs to the family of generalized additive models (GAMs), which restrict the prediction to a sum of univariate functions:\n\\[\ng\\!\\left(\\mathbb{E}[y \\mid \\mathbf{x}]\\right) = \\beta + f_1(x_1) + f_2(x_2) + \\dots + f_k(x_k)\n\\]\nwhere \\(g\\) is a link function (the logit for binary classification), \\(\\beta\\) is a bias term, and each \\(f_j\\) is a shape function that depends on a single feature. Classical GAMs typically use splines for the \\(f_j\\)’s. The key innovation of NAMs is to parameterize each shape function as its own small neural network:\n\\[\nP(y = 1 \\mid \\mathbf{x}) = \\sigma\\!\\left(\\beta + \\sum_{j=1}^{k} \\text{NN}_j(x_j)\\right)\n\\]\nwhere each \\(\\text{NN}_j\\) is a separate multi-layer perceptron that takes only \\(x_j\\) as input. Because each sub-network contains non-linear activation functions, the shape functions can be arbitrarily complex — unlike the linear relationships that logistic regression is limited to. But because the overall model is additive (no interactions between features), each learned shape function can be visualized independently.\n\n\n\nNAM architecture. Source: Agarwal et al. (2020), Figure 1.\n\n\nThis additive structure is the key insight. Because each feature is processed in isolation, there are no interactions between features inside the model. This means we can plot \\(\\text{NN}_j(x_j)\\) as a function of \\(x_j\\) for every feature and see exactly how the model uses it — something that is impossible with a standard MLP where all features interact through shared hidden layers.\nIn practice, we found NAMs to be sensitive to hyperparameter choices. Initial attempts with hand-tuned parameters produced unstable shape functions that changed substantially across random seeds. Following the original paper, we used Bayesian optimization (via Optuna) to search the hyperparameter space, and trained an ensemble of 100 NAMs with different random initializations to get stable estimates.\nThe resulting shape functions are shown below. The thick blue line is the ensemble mean, the thin lines are individual ensemble members, and the red bars indicate data density:\n\nThese plots are where NAMs really shine. For the continuous features, we can read off the non-linear relationships directly. The Cholesterol shape function shows a sharp spike at zero (confirming the missing-data signal), followed by a relatively flat region for normal values and a slight uptick at very high values. Oldpeak shows a roughly monotonic increase — greater ST depression means higher risk. For the binary features, the shape functions collapse to simple step functions since there are only two possible inputs.\nWe also computed SHAP values for the NAM to compare with the MLP:\n\nAn interesting difference emerges when comparing this to the MLP’s SHAP plot. Because each feature is modeled by its own sub-network in the NAM, the SHAP values for a given feature depend only on that feature’s value — not on the values of other features. This is why the SHAP dots for binary features appear as two clean vertical lines (one per category), while in the MLP’s SHAP plot the same features show much more spread. The NAM’s additive structure constrains the explanations to be simpler and more consistent."
  },
  {
    "objectID": "posts/15-02-2026_ml4hc-heart-disease/index.html#how-do-they-compare",
    "href": "posts/15-02-2026_ml4hc-heart-disease/index.html#how-do-they-compare",
    "title": "Predicting Heart Disease: When Simple Models Rival Deep Learning",
    "section": "4 How Do They Compare?",
    "text": "4 How Do They Compare?\n\n4.1 Performance\nAfter training a linear model, a black-box neural network, and an interpretable neural architecture on the same data, the natural question is: does the added complexity buy us anything? The short answer is no.\n\nThe Lasso leads on balanced accuracy (0.826) and precision (0.851). The NAM narrowly edges ahead on F1 (0.867), recall (0.891), and ROC AUC (0.894). The MLP lands somewhere in between. With only 734 patients and margins this thin, these differences are well within the range of statistical noise — the three models are, for all practical purposes, tied.\n\n\n4.2 Feature Importance Agreement\nThe performance numbers are a draw, but the more compelling finding lies in what the models agree on. Despite having fundamentally different internal mechanics, all three consistently identify the same features as the strongest predictors:\n\nST Slope is the single most important feature in all three models. An upsloping ST segment is protective; a flat one is a risk factor. This is well-established in cardiology: a flat or downsloping ST response to exercise suggests insufficient blood flow to the heart muscle.\nOldpeak (ST depression during exercise) is the second or third most important feature across all models. Higher values indicate greater risk, consistent with the clinical interpretation of exercise-induced ischemia.\nChest Pain Type: Asymptomatic is a strong positive predictor of heart disease in all three models. This reflects the clinical reality that patients with atypical presentations are often diagnosed later and with more advanced disease.\n\nThis level of agreement across fundamentally different model families is reassuring. It suggests that these features are genuinely predictive of heart disease in this population, not artifacts of any particular modeling choice."
  },
  {
    "objectID": "posts/15-02-2026_ml4hc-heart-disease/index.html#reflections",
    "href": "posts/15-02-2026_ml4hc-heart-disease/index.html#reflections",
    "title": "Predicting Heart Disease: When Simple Models Rival Deep Learning",
    "section": "5 Reflections",
    "text": "5 Reflections\nIf we had to deploy one of these models in a clinical setting, the Lasso would be our pick — not because it outperforms the others, but because a cardiologist can inspect a table of coefficients and immediately see what is driving a prediction. The MLP requires SHAP to be interpretable after the fact; the NAM requires understanding shape functions. When patient outcomes are on the line, the model that a clinician can scrutinize and override with the least friction is the one worth deploying.\nThat said, the NAM proved its value as an exploratory tool. Its shape functions surfaced non-linear relationships that the Lasso is structurally unable to capture — the Cholesterol spike at zero being the clearest example. A linear model requires you to manually engineer a binary indicator for that pattern; the NAM discovers it on its own. For hypothesis generation and deeper data understanding, this is a meaningful advantage.\nOn the practical side, NAMs were considerably more difficult to train than either the Lasso or the MLP. The shape functions were sensitive to hyperparameter choices and random initialization, and producing reliable estimates required both Bayesian hyperparameter optimization and ensembling over 100 models. In a clinical deployment where reproducibility is non-negotiable, this added complexity is a legitimate concern.\nThe broader takeaway from this project is that the most productive question in healthcare ML is often not “which model achieves the highest AUC?” but rather “do we understand what the model has learned, and does it align with clinical knowledge?” When three architectures spanning the full interpretability spectrum independently converge on the same risk factors — and those factors match established cardiology — that convergence carries more weight than any single metric on a leaderboard."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPredicting Heart Disease: When Simple Models Rival Deep Learning\n\n\n\n\n\n\nPython\n\n\nData Science\n\n\nMachine Learning\n\n\nDeep Learning\n\n\nHealthcare\n\n\n\nWe train a Lasso logistic regression, a multi-layer perceptron, and a neural additive model to predict heart disease from routine clinical data. All three achieve high and comparable accuracy. Using SHAP values, learned shape functions, and coefficient analysis to interpret each model, we find that they independently identify the same clinical features as most important, consistent with established cardiology literature.\n\n\n\n\n\nFeb 15, 2026\n\n\nJan Schlegel\n\n\n\n\n\n\n\n\n\n\n\n\nData-Driven Tactics: Analyzing Spain’s Playing Style in Euro 2024 Qualifiers\n\n\n\n\n\n\nPython\n\n\nData Science\n\n\nData Analysis\n\n\nVisualization\n\n\nSports Analytics\n\n\n\nWe examine Spain’s characteristic ‘Tiki-Taka’ playing style through multiple analytical lenses: offside patterns, passing networks, set-piece execution, formation dynamics, and defensive line positioning. Our analysis reveals how Spain maintains possession through short passing sequences involving defenders, utilizes full pitch width when in possession, and employs a high defensive line that reflects their controlling philosophy. We also identify tactical vulnerabilities, particularly Spain’s susceptibility to offsides and counter-attacks, providing actionable insights for opposition teams.\n\n\n\n\n\nFeb 14, 2026\n\n\nJan Schlegel\n\n\n\n\n\n\n\n\n\n\n\n\nCovariance Shrinkage: From Linear Shrinkage to Random Matrix Theory\n\n\n\n\n\n\nQuantitative Finance\n\n\nPython\n\n\nHigh-Dimensional Statistics\n\n\n\nIn high-dimensional regimes where the number of variables \\(N\\) is comparable to the sample size \\(T\\), the sample covariance matrix is known to be an ill-conditioned and noisy estimator of the population covariance. This post provides a rigorous mathematical treatment of covariance shrinkage, exploring the bias-variance tradeoff inherent in linear shrinkage estimators (Ledoit-Wolf). We further ground these methods in Random Matrix Theory, specifically the Marchenko-Pastur law, to characterize the asymptotic behavior of eigenvalues. Finally, we introduce and implement Nonlinear Shrinkage (Ledoit-Wolf 2020), which applies a non-linear transformation to the sample eigenvalues based on the estimation of the Stieltjes transform. Simulation studies demonstrate the efficacy of these methods in recovering true spectral properties.\n\n\n\n\n\nNov 23, 2025\n\n\nJan Schlegel\n\n\n\n\n\n\n\n\n\n\n\n\nComparing Univariate and Multivariate Models for Value at Risk Forecasting\n\n\n\n\n\n\nSimulation\n\n\nR\n\n\n\nThis post explores the effectiveness of univariate and multivariate GARCH-based models in forecasting Value at Risk (VaR) for a long equity portfolio. While multivariate models generally perform better in backtests, univariate models often fall short. However, neither model type consistently outperforms the other in predictive accuracy, highlighting the trade-offs between simplicity and complexity in risk forecasting.\n\n\n\n\n\nFeb 7, 2025\n\n\nJan Schlegel\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jan Schlegel",
    "section": "",
    "text": "Hi there! I’m Jan, a statistician and product manager based in Zurich. I recently completed my Master’s in Statistics at ETH Zurich, specializing in probabilistic AI, high-dimensional statistics, and causality. My work focuses on understanding and improving how machine learning systems make decisions—from interpreting generative models to building robust methods for high-stakes applications like climate forecasting and reinsurance.\nWhen I’m not working on difficult (for me) problems, you’ll find me running, in the gym, swimming, building PCs, or getting lost (intentionally) in GeoGuessr."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Jan Schlegel",
    "section": "Education",
    "text": "Education\n\n\n🎓 Master of Science (M.Sc. ETH) in Statistics\n\n🏛️ ETH Zurich | 📍 Zurich, Switzerland | 📅 Sep 2023 - Sep 2025\n\n\nGPA: 5.99 / 6.00\n\n\nFocus: (Probabilistic) Machine Learning, Statistical Learning Theory, Computational Statistics, Causality\nThesis: Mechanistic Interpretability in Text-to-Image Diffusion Models (Grade: 6.00/6.00)\nSemester Paper: Extrapolation and Distributional Robustness for Climate Downscaling\n\n\n\n🎓 Bachelor of Arts in Business and Economics\n\n🏛️ University of Zurich | 📍 Zurich, Switzerland | 📅 Sep 2019 - Feb 2023\n\n\nGPA: 5.87 / 6.00 (Class Rank 1)\n\n\nMajor: Banking and Finance; Minor: Statistics\nThesis: Portfolio Value at Risk Forecasting with Copula-GARCH Models (Grade: 6.00/6.00)\nCompleted 215 ECTS (vs. standard 180) with a strong focus on quantitative finance, statistics, and computing"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Jan Schlegel",
    "section": "Experience",
    "text": "Experience\n\n\n💼 Product Manager\n\n🏛️ Swiss Re | 📍 Zurich, Switzerland | 📅 Feb 2026 - Present\n\n\nDrive robust portfolio optimization initiatives across multiple product lines\nTransitioned to full-time role following successful quantitative internship\n\n\n\n💼 Quantitative Intern\n\n🏛️ Swiss Re | 📍 Zurich, Switzerland | 📅 Oct 2025 - Jan 2026\n\n\nDeveloped robust high-dimensional covariance estimation methods applicable to multiple reinsurance contexts\nDesigned robust portfolio optimization techniques tailored to reinsurance, providing actionable steering insights\n\n\n\n💼 Biostatistics Research Assistant\n\n🏛️ University of Zurich (EBPI) | 📍 Zurich, Switzerland | 📅 Feb 2022 - Aug 2023\n\n\nContributed to data and code management and reproducible statistical workflows for a COVID-19 cohort study published in Nature Communications\nApplied statistical modelling techniques to complex longitudinal datasets and presented the results to principal investigators and the research team\n\n\n\n💼 Accountant (Military Service)\n\n🏛️ Swiss Armed Forces | 📍 Kloten, Switzerland | 📅 Jul 2021 - Nov 2021\n\n\nSuccessfully balanced full-time university studies with mandatory military service obligations\nManaged the complete accounting for a 150-person military unit\n\n\n\n💼 Teaching Assistant Mathematics\n\n🏛️ University of Zurich | 📍 Zurich, Switzerland | 📅 Sep 2020 - Jul 2021\n\n\nLed twice-weekly tutorials covering Analysis and Linear Algebra for groups of up to 30 students"
  },
  {
    "objectID": "posts/14-02-2026_soccer-analytics/index.html#introduction",
    "href": "posts/14-02-2026_soccer-analytics/index.html#introduction",
    "title": "Data-Driven Tactics: Analyzing Spain’s Playing Style in Euro 2024 Qualifiers",
    "section": "1 Introduction",
    "text": "1 Introduction\nModern football analytics has transformed how we understand team tactics and playing styles. By leveraging detailed match event and tracking data, we can quantify patterns that were previously only observable through subjective analysis. This post examines Spain’s playing dynamics during the Euro 2024 qualifiers, combining Wyscout event data with Skillcorner tracking data to provide a multi-faceted tactical analysis.\nSpain’s national team has long been synonymous with possession-based football and their distinctive ‘Tiki-Taka’ style—characterized by short passing sequences, high technical skill, and positional fluidity. However, this approach comes with inherent tactical trade-offs. Through systematic data analysis, we can identify not only the strengths of this philosophy but also potential weaknesses that opposition teams might exploit.\nThe full code for this analysis is available on  GitHub. Note that the underlying data is confidential and therefore not included in the repository.\n\n1.1 Offside Patterns\nBased on our viewing experience we expected that Spain would have a relatively high number of offsides compared to other teams. A first glance at the data later confirmed that Spain was indeed among the teams with the most offsides in the Euro 2024 qualifiers. This is also displayed in the table below which is based on all Wyscout data of all matches after removing all of the non-European teams. Interestingly, Albania is also among the top ten teams with the highest number of offsides per game.\nOne player particularly caught our eye: Álvaro Morata who was caught offside eight times across his six game appearances. However, as nominal data can be misleading in football, we decided to additionally have a look at the normalized offside calls per player. In the below plot we can see that albeit Álvaro Morata being flagged offside more frequently than his teammates he is not the player with the highest offside rate due to his high playing time (461 played minutes). Instead, the first place goes to Ansu Fati who, despite being caught offside only once, had the highest offside rate due to only playing for 45 minutes.\n\nMoreover, most of the offsides by the Spanish team occured after a forward pass attempt (most frequently by Dani Carvajal) into the final third of the pitch. The animations below show four examples of Spanish offside positions captured from different qualifying matches.\n\n\n\n\n\n\nCross from Dani Carvajal to Álvaro Morata (Spain vs Scotland)\n\n\n\n\n\nThrough ball from Dani Carvajal to Gavi (Spain vs Georgia)\n\n\n\n\n\n\n\nThrough ball from Pedro Porro to Álvaro Morata (Spain vs Scotland)\n\n\n\n\n\nThrough ball from Dani Carvajal to Álvaro Morata (Spain vs Scotland)\n\n\n\n\n\n1.2 Passing Patterns and Accuracy\nAdditionally, we investigated the relationship between passing accuracy and number of passes when normalizing the number of passes by the minutes played in the left plot below. It is apparent that center backs tend to have the highest number of passes and the highest accuracy whilst forwards usually have the lowest passing completion rate with few passes per minute. This is in line with the typical Spanish “Tiki-Taka” playstyle where risky passes are often avoided and instead the ball is circulated around the pitch frequently involving defenders in the build up. Moreover, when stratifying the accuracy by whether or not the player was under pressure at the time of the pass, we observe that the accuracy of the passes under pressure tends to be lower than the passes without pressure. This is displayed in the right plot below where the forwards again are in the bottom left corner and the center back in the top right corner. Our explanation for this is that the forwards often will be pressured by multiple defenders yielding a much lower accuracy when under pressure compared to under no pressure. Interestingly, the pass completion percentage of the Goalkeeper Unai Simón is much lower when under pressure probably because he just clears the ball away whenever under pressure. Finally, be aware that contains only relatively few passes under pressure and thus we had to restrict ourselves to players with a minimum of 135 minutes (equivalent to 1.5 full games) played to not overcrowd the plot with players that have made no or only few passes under pressure.\n\n\n\n\n\n\n\n\n\n\nThis “false nine” role is further emphasized in the passing sonars below with the central forward only involved in few passes. Judging from the color fill of the sonars it appears that apart from the goalkeeper and the central midfielders, the players generally tend to avoid long passes. Again, we can see that forwards often pass backwards to keep possession, while the defenders often pass the ball back and forth to each other, and the central midfielders distribute it and play out wide. These patterns demonstrate that the Spanish team remains faithful to their beloved “Tiki-Taka” style of play."
  },
  {
    "objectID": "posts/14-02-2026_soccer-analytics/index.html#set-piece-analysis",
    "href": "posts/14-02-2026_soccer-analytics/index.html#set-piece-analysis",
    "title": "Data-Driven Tactics: Analyzing Spain’s Playing Style in Euro 2024 Qualifiers",
    "section": "2 Set Piece Analysis",
    "text": "2 Set Piece Analysis\nIn the following subsections, we will examine different set pieces, how they are executed by the Spanish team and how they change the game dynamics and outcomes.\n\n2.1 Throw-ins\nThe plots below show the throw-in sonars (left) and trajectories (right) for the Spanish team across all eight qualifying matches and are based on Wyscout data. The throw-in sonars show the angle of the throw-ins, with the length of the sonar indicating the frequency of throw-ins in that direction and the color specifying the length of the throw-in in meters. The plot on the right shows the trajectory from the beginning to the end location of the throw-ins colored by whether the throw-in was accurate or not. In the plot on the right, Spain attacks from the bottom to the top of the plot.\n\n\n\n\n\n\n\n\n\n\nIt is apparent from the throw-in trajectories on the right that most of the throw-ins for the Spanish team take place in the middle and attacking thirds of the pitch. Moreover, there does not appear to be a systematic relationship between throw-in accuracy and the different thirds. In general, there are only few inaccurate throw-ins despite the average throw-in distance being almost 20 meters. This can be explained by the throw-in sonars from which it is evident that most of the throw-ins and directed backwards, especially in the attacking third. Thus, the Spanish team does not rely on throw-ins to create scoring opportunities in the attacking third but instead utilize them to maintain possession and stabilize their play.\n\n\n2.2 Corners\nThe plot below shows the corner kick trajectories for the Spanish team colored by accuracy, where a corner is considered accurate if a Spanish player successfully receives the ball. Overall, Spain’s corner accuracy is striking, especially for deliveries going directly into the box. We can also see that many corners are played short and backwards, likely to retain possession and create a better crossing angle.\nInterestingly, there is a notable asymmetry between the two sides: corners from the left side are overwhelmingly accurate, even when delivered into the middle of the box. In contrast, corners from the right side that target the area close to the goal are frequently inaccurate, suggesting that the in-swinging deliveries from the left are more effective than the out-swinging ones from the right for Spain’s set-piece setup."
  },
  {
    "objectID": "posts/14-02-2026_soccer-analytics/index.html#formation-and-positioning-analysis",
    "href": "posts/14-02-2026_soccer-analytics/index.html#formation-and-positioning-analysis",
    "title": "Data-Driven Tactics: Analyzing Spain’s Playing Style in Euro 2024 Qualifiers",
    "section": "3 Formation and Positioning Analysis",
    "text": "3 Formation and Positioning Analysis\n\n3.1 Formation Convex Hulls\nThe plots below show the formation at two random time points during the first half and two random time points in the second half for all games for which we have Skillcorner data. As in Shaw and Glickman (2019), the shaded regions indicate the convex hull of the players with the blue arrow pointing to the center of mass of this convex hull. This allows us to compare the relative positioning of the Spanish players across randomly selected time points stratified by possession type. The distinction between in possession and out of possession is made only when the team has been in or out of possession for at least 10 seconds, ensuring that only stable formations are included in the analysis. Finally, Spain attacks from the bottom to the top in all of the plots.\n\n\n\nIn Possession\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOut of Possession\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is evident, that whenever Spain is in possession, the team tries utilizing the full width of the pitch. This creates space and ensures that there are always passing options available for Spains infamous short passing game. Formation-wise, the 4-3-3 formation of Spain tends to persist whenever Spain is in possession. The midfielders often form a triangle to provide multiple passing options. This structure is crucial for maintaining possession and slowly advancing the ball up the pitch. In contrast, when Spain is out of possession, the team tends to adopt a more narrow and compact formation. The formation still resembles a 4-3-3, but the midfielder drop back which leads to the players being closer together. This limits the space available for the opposition allows Spain to press more effectively in the midfield. Additionally, this compact formation often forces the opposition to play on the sides of the pitch instead of through the densely-populated middle.\n\n\n3.2 Defensive Line Positioning\nThe heatmaps below display Spain’s defensive line positioning both in possession and out of possession across all matches for which we have access to Skillcorner data. .Note that contrary to above formation plots, these heatmaps include all data points for completeness i.e. there is no minimum amount of time in/ out of possession required. Again, Spain attacks from the bottom to the top in all of the plots.\n\n\n\nIn Possession\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOut of Possession\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe heatmaps show that the defensive line tends to be slightly more offensive when in possession but the difference between in and out of possession is not striking. In general, the defensive line is always relatively high up the pitch, with the most frequent positions being right in front of the middle line. This is in line with Spain’s controlling play style when in possession and high pressing when out of possession. Additionally, the defensive line tends to be higher when playing against weaker teams.\nMoreover, both wing backs tend to be positioned further forward than the center backs, as is typical in the 4-3-3 formation that Spain loves to run. This allows the wing backs to provide width and support when attacking whilst the more defensively positioned center-backs provide stability and try to always be available for a back pass when the team is under pressure or wants stabilize the game."
  },
  {
    "objectID": "posts/14-02-2026_soccer-analytics/index.html#tactical-recommendations-for-opposition-teams",
    "href": "posts/14-02-2026_soccer-analytics/index.html#tactical-recommendations-for-opposition-teams",
    "title": "Data-Driven Tactics: Analyzing Spain’s Playing Style in Euro 2024 Qualifiers",
    "section": "4 Tactical Recommendations for Opposition Teams",
    "text": "4 Tactical Recommendations for Opposition Teams\nBased on our comprehensive analysis of Spain’s playing style, we can identify several tactical vulnerabilities that opposition teams might exploit:\n\n4.1 Exploiting the Offside Trap\nAs was detailed in the offside analysis section, Spanish players frequently get caught offside. Hence, Albania should maintain a well-organized defensive line that moves up as a unit to catch Spanish players offside. Particularly, Albania could try to exploit the fact that Carvajal’s through ball attempts to Álvaro Morata often result in offside positions by tracking Morata and stepping up as a unit when Carvajal is about to play the ball, effectively creating an offside trap. The resulting free-kick could then be used to quickly launch a counter-attack exploiting Spain’s high positioning.\n\n\n4.2 Pressing the Forwards\nThe passing analysis showed us that the forwards are particularly susceptible to making inaccurate passes when under pressure. Moreover, we observed that in Spain’s game development style, the strikers often drop back to receive the ball and then distribute it back to the midfield or to the flanks. This suggests pressuring the forwards whenever possible, without compromising our own defensive line too much, to provoke inaccurate passes. Albania could implement this by having one of the center backs remain as attentive as possible while maintaining close one-to-one marking of the central forward.\n\n\n4.3 Exploiting the High Defensive Line\nAs seen in our defensive line analysis, Spain employs a high defensive line as a consequence of their possession-based playstyle, which renders them vulnerable to quick counter-attacks. Hence, Albania should occasionally try to exploit this high defensive line by attempting long balls behind the Spanish defense, even if this might cost Albania possession more often than not."
  }
]