[
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html",
    "href": "posts/07-02-2025_portfolio-var/index.html",
    "title": "Comparing Univariate and Multivariate Models for Value at Risk Forecasting",
    "section": "",
    "text": "Value at Risk (VaR) is a crucial metric in modern financial risk management"
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#introduction",
    "href": "posts/07-02-2025_portfolio-var/index.html#introduction",
    "title": "Comparing Univariate and Multivariate Models for Value at Risk Forecasting",
    "section": "1 Introduction",
    "text": "1 Introduction\nThe past four decades have been shaped by extreme events in financial markets, such as the Black Monday crash, the dot-com bubble, and the 2008 global financial crisis. These supposedly rare events highlighted that reducing systemic risk is crucial for financial stability (Embrechts et al. 2014). This led to the introduction and tightening of the Basel Accords, which use risk measures to determine appropriate risk capital requirements for financial institutions.\nValue at Risk (VaR) remains the most popular measure for downside market risk, despite a shift towards the severity-based expected shortfall (ES) (Embrechts et al. 2014). For a long equity portfolio, the \\(p\\)% VaR for period \\(t\\) forecasted at time \\(t-1\\) is defined as the negative \\(p\\)-quantile of the conditional portfolio return distribution:\n\\[\n\\text{VaR}_t^p=-Q_p(r_{\\text{PF},t}|\\mathcal{F}_{t-1})=-\\inf_x\\{x\\in\\mathbb{R}:\\mathbb{P}(r_{\\text{PF},t}\\leq x|\\mathcal{F}_{t-1})\\geq p\\},\\quad p\\in(0,1).\n\\tag{1}\\]\nHere, \\(Q_p(\\cdot)\\) denotes the quantile function and \\(\\mathcal{F}_{t-1}\\) represents all information available at time \\(t-1\\). The parameter \\(p\\) indicates that with target probability \\(p\\), the portfolio losses will exceed the VaR (Marc S. Paolella, Kuester, and Mittnik 2006).\nDue to the practical relevance of VaR, it is essential to determine estimation methods that neither severely underestimate nor overestimate future losses. Many models use the generalized autoregressive conditional heteroskedasticity (GARCH) framework (Bollerslev 1986) or extensions to account for volatility clustering and the “leverage effect” in financial time series.\nA fundamental question in VaR modeling is whether more complex multivariate models outperform simpler univariate alternatives. Santos, Nogales, and Ruiz (2013) found that multivariate models significantly outperform univariate counterparts for large portfolios, while Kole et al. (2017) found that multivariate models have greater predictive ability, though differences are often not significant. They also found that data frequency is more important than model choice.\nThis study compares factor copula-DCC-NGARCH models introduced by Fortin, Simonato, and Dionne (2022) with established models like the diagonal MixN(k)-GARCH (Haas, Mittnik, and Paolella 2004) and the COMFORT model class (Marc S. Paolella and Polak 2015). We show that multivariate models display desirable VaR properties in terms of correct unconditional coverage and independence of violations, though we don’t find sufficient evidence to claim that multivariate approaches outperform univariate procedures in terms of forecast ability, or vice versa."
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#the-data",
    "href": "posts/07-02-2025_portfolio-var/index.html#the-data",
    "title": "Comparing Univariate and Multivariate Models for Value at Risk Forecasting",
    "section": "2 The Data",
    "text": "2 The Data\nWe use an equally weighted portfolio of ten large-cap stocks (identical to those used by Fortin, Simonato, and Dionne (2022)): Boeing, Caterpillar, Chevron, Coca-Cola, Exxon, GE, IBM, Merck, P&G, and UTC. However, we analyze 2,767 daily returns from January 2, 2001, to December 30, 2011, rather than weekly returns.\nThe return data shows that most factors and stocks have means close to zero, with the median larger than the mean in most cases. The mean absolute deviation (MAD) is considerably smaller than the standard deviation, indicating the presence of outliers. Most returns are left-skewed with leptokurtic behavior, and all return distributions reject the assumption of normality based on Jarque-Bera statistics.\n\n\n\n\n\n\n\n\nFigure 1: ACF Plots of the Fama-French-Carhart Factors\n\n\n\n\n\nFigure 1 shows autocorrelation function (ACF) plots for the factors. The factors exhibit stronger autocorrelation in absolute returns than in the returns themselves, justifying the use of volatility models.\n\n\n\n\n\n\n\n\nFigure 2: Chi-Square Q-Q Plots of the Stock and Factor Returns\n\n\n\n\n\nFigure 2 shows Q-Q plots of the robust squared Mahalanobis distances against \\(\\chi^2\\) distributions. The non-linear relationship indicates large multivariate outliers and multivariate non-normality.\n\n\n\n\n\n\n\n\nFigure 3: Portfolio Returns\n\n\n\n\n\nFigure 3 demonstrates blatant volatility clustering (Panel A) and that the portfolio returns are not normally distributed (Panel B)."
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#univariate-models",
    "href": "posts/07-02-2025_portfolio-var/index.html#univariate-models",
    "title": "Comparing Univariate and Multivariate Models for Value at Risk Forecasting",
    "section": "3 Univariate Models",
    "text": "3 Univariate Models\nFor univariate models, we assume the following portfolio return dynamics:\n\\[\nr_{\\text{PF},t} = \\mu + \\epsilon_t, \\qquad \\epsilon_t = \\sigma_t z_t, \\quad z_t \\stackrel{iid}{\\sim} F(0,1)\n\\]\nHere, \\(F(0,1)\\) is a standardized distribution, \\(\\mu\\) is the unconditional location, and \\(\\sigma_t\\) is the scale parameter. The conditional variance \\(\\sigma_t^2 = \\mathbb{V}[r_{\\text{PF},t}|\\mathcal{F}_{t-1}]\\) is modeled as non-constant. The standard GARCH(1,1) model (Bollerslev 1986) specifies this variance as:\n\\[\n\\sigma_t^2 = \\omega + \\alpha\\epsilon_{t-1}^2 + \\beta\\sigma_{t-1}^2\n\\tag{2}\\]\nwhere \\(\\omega &gt; 0, \\alpha \\geq 0\\), and \\(\\beta \\geq 0\\), with \\(\\alpha + \\beta &lt; 1\\) required for covariance stationarity. A special case is the exponentially weighted moving average (EWMA):\n\\[\n\\sigma^2_t = \\lambda\\sigma^2_{t-1} + (1-\\lambda)\\epsilon_t^2, \\quad \\lambda \\in (0,1)\n\\tag{3}\\]\nThis formulation puts more weight on recent observations, but is not covariance stationary since \\(\\lambda + (1-\\lambda) = 1\\).\nA well-documented empirical regularity in equity returns is the “leverage effect”: negative news increases volatility more than positive news of equal magnitude. The GJR-GARCH(1,1) model (Glosten, Jagannathan, and Runkle 1993) captures this asymmetry through an indicator function:\n\\[\n\\sigma_t^2 = \\omega + (\\alpha + \\gamma I_{t-1})\\epsilon_{t-1}^2 + \\beta\\sigma_{t-1}^2\n\\tag{4}\\]\nwhere \\(I_{t-1} = \\mathbb{I}_{\\{\\epsilon_{t-1} &lt; 0\\}}\\). Another asymmetric specification is the NGARCH(1,1) (R. F. Engle and Ng 1993):\n\\[\n\\sigma_t^2 = \\omega + \\alpha\\sigma_{t-1}^2(\\epsilon_{t-1} - \\theta)^2 + \\beta\\sigma_{t-1}^2\n\\]\nFor \\(\\theta &gt; 0\\), negative innovations have a larger impact on conditional variance than positive errors of the same magnitude.\nWe also include the k-component mixed normal GARCH(1,1) (MixN(k)-GARCH) (Haas, Mittnik, and Paolella 2004), where the conditional distribution of the error term \\(\\epsilon_t\\) is assumed to be mixed normal with zero mean:\n\\[\n\\epsilon_t|\\mathcal{F}_{t-1} \\sim \\text{Mix}_k\\text{N}(p_1,...,p_k, \\mu_1,...,\\mu_k, \\sigma_{1,t}^2,...,\\sigma_{k,t}^2), \\quad \\sum_{i=1}^k p_i\\mu_i = 0\n\\]\nThe associated conditional variances follow GARCH processes:\n\\[\n\\sigma_{i,t}^2 = \\omega_i + \\alpha_i\\epsilon_{i,t-1}^2 + \\beta_i\\sigma_{i,t-1}^2, \\quad i=1,...,k\n\\]"
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#multivariate-models",
    "href": "posts/07-02-2025_portfolio-var/index.html#multivariate-models",
    "title": "Comparing Univariate and Multivariate Models for Value at Risk Forecasting",
    "section": "4 Multivariate Models",
    "text": "4 Multivariate Models\nThe factor copula model proposed by Fortin, Simonato, and Dionne (2022) uses equity factors to capture the main risks of stock returns. It utilizes the Carhart four-factor model (Carhart 1997), which adds a momentum factor to the Fama-French three-factor model (Fama and French 1993):\n\\[\nr_{k,t} - r_{f,t} = \\alpha_{k,t} + \\mathbf{\\beta}_k'\\mathbf{r}_{\\text{F},t} + \\varepsilon_{k,t}\n\\tag{5}\\]\nThis reduces dimensionality by modeling only four factors instead of all portfolio constituents. The estimated Carhart parameters for our ten stocks are shown below. All market betas are close to one, adjusted \\(R^2\\) values range from 0.29 to 0.61, and the Jarque-Bera statistics overwhelmingly reject normality of the OLS residuals.\n\n\n\n\n\n\n\n\n\nFigure 4: Chi-Square Q-Q Plot of the Carhart OLS Residuals\n\n\n\n\n\nFigure 4 confirms the non-normality of OLS residuals, with the Q-Q plot showing clear deviations from the \\(\\chi^2\\) reference line.\nFor the factor dynamics, we use the Dynamic Conditional Correlation (DCC) structure (R. Engle 2002), which decomposes the conditional covariance matrix into standard deviations and correlations:\n\\[\n\\mathbf{Y}_{t}|\\mathcal{F}_{t-1} \\sim \\mathcal{N}_n(\\mathbf{\\mu}, \\mathbf{\\Sigma_t}), \\quad \\mathbf{\\Sigma_t} = \\mathbf{D_t}\\mathbf{\\Gamma_t}\\mathbf{D_t}\n\\tag{6}\\]\nwhere \\(\\mathbf{D_t} = \\text{diag}(\\sigma_{1,t},\\sigma_{2,t},...,\\sigma_{n,t})\\) contains the conditional standard deviations. To account for non-normality, we use copulas to model the joint conditional distribution of factor returns. By Sklar’s theorem:\n\\[\n\\mathbf{F_t}(\\mathbf{z_t}) = \\mathbf{C_t}(F_{1,t}(z_{1,t}),...,F_{n,t}(z_{n,t}))\n\\tag{7}\\]\nwhere \\(\\mathbf{F_t}(\\mathbf{z_t})\\) is the joint conditional distribution, \\(F_{i,t}(\\cdot)\\) are the conditional marginals, and \\(\\mathbf{C_t}:[0,1]^n \\rightarrow [0,1]\\) is the conditional copula. Copulas allow modeling marginals independently of the multivariate distribution, giving us the flexibility to pair different marginal specifications (normal GARCH or skewed-t NGARCH) with different copula families (normal, Student-t, or skewed-t).\nThe Common Market Factor Non-Gaussian Returns (COMFORT) model (Marc S. Paolella and Polak 2015) takes a different approach, using a multivariate generalized hyperbolic (MGHyp) distribution with a CCC or DCC structure for the covariance matrix. This model can be expressed as a continuous normal mixture:\n\\[\n\\mathbf{Y_t} = \\mathbf{\\mu} + \\mathbf{\\gamma} G_t + \\mathbf{\\varepsilon_t}, \\quad \\mathbf{\\varepsilon_t} = \\mathbf{\\Sigma_t}^{1/2}\\sqrt{G_t}\\mathbf{Z_t}\n\\tag{8}\\]\nwhere \\(\\mathbf{Z_t} \\stackrel{iid}{\\sim} \\mathcal{N}_n(\\mathbf{0},\\mathbf{I_n})\\) and the mixing random variables \\(G_t|\\mathcal{F}_{t-1} \\sim \\text{GIG}(\\lambda_t,\\chi_t,\\psi_t)\\) follow a generalized inverse Gaussian distribution. An important property of the MGHyp distribution is that it is closed under linear operations. Therefore, portfolio returns \\(r_{\\text{PF},t} = \\mathbf{w}'\\mathbf{Y_t}\\) are univariate GHyp distributed:\n\\[\nr_{\\text{PF},t}|\\mathcal{F}_{t-1} \\sim \\text{GHyp}(\\mathbf{w}'\\mathbf{\\mu},\\mathbf{w}'\\mathbf{\\gamma},\\mathbf{w}'\\mathbf{\\Sigma_t}\\mathbf{w},\\lambda_t,\\chi_t, \\psi_t)\n\\tag{9}\\]"
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#forecasting-and-evaluation",
    "href": "posts/07-02-2025_portfolio-var/index.html#forecasting-and-evaluation",
    "title": "Comparing Univariate and Multivariate Models for Value at Risk Forecasting",
    "section": "5 Forecasting and Evaluation",
    "text": "5 Forecasting and Evaluation\nFor all models, we assume a constant conditional mean over time. Forecasting uses a rolling window approach with the previous 1,000 observations to predict the one-step-ahead VaR. For univariate GARCH models (except MixN(k)-GARCH), the VaR is computed analytically:\n\\[\n\\widehat{\\text{VaR}_t^p} = -\\mu_{\\text{PF}} - \\sigma_{\\text{PF}, t} Q_p(z_t|\\mathcal{F}_{t-1})\n\\tag{10}\\]\nwhere \\(\\sigma_{\\text{PF}, t}\\) is the conditional standard deviation and \\(Q_p(z_t)\\) is the p-quantile of the standardized returns. For the factor copula-DCC-(N)GARCH models, we simulate factor returns, apply the Carhart model to generate single stock returns, and calculate the portfolio return. The VaR estimate is then the negative p-quantile of the simulated portfolio returns. For the COMFORT model, we use the p-quantile function of the corresponding conditional univariate GHyp distribution directly.\nBacktesting checks whether the forecasts exhibit desirable statistical properties. Following Christoffersen (1998), we define the violation indicator \\(I_t = \\mathbb{I}_{\\{r_{\\text{PF},t} &lt; -\\text{VaR}_t^p\\}}\\) and test three hypotheses using likelihood-ratio tests. The unconditional coverage (UC) test checks whether the expected violation rate equals \\(p\\). Its test statistic is:\n\\[\nLR_{uc} = -2\\log\\left(\\frac{L(p;I_1,I_2,...,I_T)}{L(\\hat{p};I_1,I_2,...,I_T)}\\right) \\overset{\\text{asy}}{\\sim} \\chi_1^2\n\\tag{11}\\]\nwhere \\(\\hat{p} = n_1/(n_0+n_1)\\) is the maximum likelihood estimate of \\(p\\). The independence (IND) test checks whether violations are independently distributed, against a first-order Markov chain alternative:\n\\[\nLR_{ind} = -2\\log\\left(\\frac{L(\\hat{\\Pi}_2;I_2,...,I_T|I_1)}{L(\\hat{\\Pi}_1;I_2,...,I_T|I_1)}\\right) \\overset{\\text{asy}}{\\sim} \\chi_1^2\n\\tag{12}\\]\nThe conditional coverage (CC) test combines both by testing that \\(\\{I_t\\}_{t=1}^T \\overset{\\text{iid}}{\\sim} \\text{Bernoulli}(p)\\), with \\(LR_{cc} = LR_{uc} + LR_{ind} \\overset{\\text{asy}}{\\sim} \\chi_2^2\\).\nTo rank VaR estimates beyond the pass/fail logic of backtests, we use the “tick” loss function:\n\\[\nL_V(\\theta_t, r_{\\text{PF},t}) = (r_{\\text{PF},t} + \\theta_t)(p - \\mathbb{I}_{\\{r_{\\text{PF},t} &lt; -\\theta_t\\}})\n\\]\nThis loss is minimized when \\(\\theta_t = \\text{VaR}_t^p\\). For statistical inference on loss differentials between model pairs, we use the conditional predictive ability (CPA) test (Giacomini and White 2006) with null hypothesis \\(H_0: \\mathbb{E}[\\Delta L_{i,j,t}|\\mathcal{F}_{t-1}] = 0\\) and test statistic:\n\\[\nGW_{i,j} = T\\bar{Z}'\\hat{\\Omega}\\bar{Z} \\overset{d}{\\rightarrow} \\chi^2_q \\quad \\text{as } T \\rightarrow \\infty\n\\]\nwhere \\(Z_t = h_{t-1}\\Delta L_{i,j,t}\\), \\(\\bar{Z} = \\frac{1}{T}\\sum_{t=2}^T Z_t\\), and \\(\\hat{\\Omega} = \\frac{1}{T}\\sum_{t=2}^T Z_t Z_t'\\)."
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#results",
    "href": "posts/07-02-2025_portfolio-var/index.html#results",
    "title": "Comparing Univariate and Multivariate Models for Value at Risk Forecasting",
    "section": "6 Results",
    "text": "6 Results\n\n6.1 Backtest Results\nThe backtest results are summarized in the table below. While all univariate models pass the likelihood ratio test of independence, only a few show adequate conditional or unconditional coverage. In particular, only the skewed-t GJR-GARCH passes the test of conditional coverage for both VaR levels. In contrast, most multivariate models pass all three likelihood ratio tests, with the exception of the multivariate normal (MN)-DCC-GARCH.\n\nModels assuming normality of returns (GARCH, EWMA, and MN-DCC-GARCH) consistently show too many violations, with none displaying adequate conditional or unconditional coverage. For these models, the percentage of violations exceeds the corresponding VaR level. For the COMFORT and factor copula models, however, we observe fewer violations than expected. Using skewed-t NGARCH marginals in the factor copula framework leads to a particularly low percentage of violations. Surprisingly, even using a normal copula with normal GARCH marginals for factor returns produces sound VaR estimates, possibly because much of the multivariate non-normality of stock returns is captured in the bootstrapped OLS residuals.\nThe most adequate coverage is achieved by the factor copula-DCC models with GARCH marginals for the 1% VaR level and by the COMFORT models for the 5% level. The factor skewed-t copula with GARCH marginals belongs to the three models with the most appropriate coverage for both VaR percentiles.\nThe table below shows the temporal distribution of exceedances. Nearly all exceedances for the COMFORT models occurred during or after the 2008 financial crisis, despite passing the independence test. At the 1% VaR level, Gaussian models without leverage effects have approximately three times as many exceedances as expected.\n\n\n\n6.2 Predictive Ability\nIn terms of average tick loss, univariate models perform surprisingly well despite their suboptimal backtesting results. The skewed-t GJR model achieves the lowest and the MN-DCC-GARCH the highest average loss for both VaR levels. Multivariate models achieve better ranks at the 1% level than at the 5% level, with factor copula-based models showing lower average loss than COMFORT models at both levels.\n\nWithin the factor copula-DCC models, GARCH marginals achieve lower mean losses than skewed-t NGARCH marginals, reinforcing the hypothesis that bootstrapped OLS residuals account for much of the non-normality in stock returns.\nThe CPA test results at the 1% VaR level show that the MN-DCC-GARCH is significantly outperformed by every other model. Most rejections occur in univariate vs. univariate or multivariate vs. multivariate comparisons. For multivariate models, factor copula-DCC models using skewed-t NGARCH marginals have significantly higher predictive ability than their counterparts with normal GARCH marginals. The normal copula is superior for skewed-t NGARCH marginals, but for normal GARCH marginals, the t and skewed-t copula versions significantly outperform the normal copula.\n\nAt the 5% VaR level, in addition to MN-DCC-GARCH, the MixN(3)-GARCH is also significantly outperformed by all other models. The Student t GJR-GARCH, skewed-t GJR-GARCH, and MixN(2)-GARCH all display significantly higher predictive ability than the COMFORT models. Interestingly, the skewed-t GJR-GARCH, which significantly outperforms every other univariate model, does not achieve significantly better VaR forecasts than the factor copula-DCC-(N)GARCH models."
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#conclusion",
    "href": "posts/07-02-2025_portfolio-var/index.html#conclusion",
    "title": "Comparing Univariate and Multivariate Models for Value at Risk Forecasting",
    "section": "7 Conclusion",
    "text": "7 Conclusion\nOur study assessed various univariate and multivariate models for VaR forecasting. Most univariate models produced inadequate VaR estimates with too many violations, while most multivariate models displayed adequate coverage with independently occurring VaR exceedances.\nThe CPA tests revealed that the MN-DCC-GARCH model is significantly outperformed by all other models, which is expected given the evident multivariate non-normality of stock returns. However, we found no general, significant outperformance of multivariate models over univariate ones, or vice versa, at either VaR level.\nOne important finding is that using daily returns (higher frequency data) makes the factor copula-DCC-NGARCH models feasible for VaR forecasting, consistent with Kole et al. (2017) who found that data frequency is more important than model choice for VaR forecasts.\nWe also found that replacing skewed-t NGARCH marginals with normal GARCH marginals for factor returns increased predictive accuracy and yielded better unconditional coverage. This may be because OLS residuals from the Carhart model capture most of the multivariate non-normality in stock returns.\nFor future research, it would be interesting to examine how the factor copula-DCC-GARCH model performs with larger portfolios, which would highlight the advantage of its dimensionality reduction. The only computationally expensive parts, fitting the DCC-GARCH structure and the copula, depend only on the number of factors, not the portfolio size."
  },
  {
    "objectID": "posts/17-02-2026_ml4hc-pneumonia/index.html#introduction",
    "href": "posts/17-02-2026_ml4hc-pneumonia/index.html#introduction",
    "title": "Detecting Pneumonia from Chest X-Rays: CNNs, Integrated Gradients, and Grad-CAM",
    "section": "1 Introduction",
    "text": "1 Introduction\nPneumonia remains one of the leading causes of death worldwide, particularly among children and the elderly. Diagnosis typically relies on a radiologist interpreting a chest X-ray, looking for characteristic opacities in the lung fields that indicate fluid or inflammation. This is a task that deep learning models have shown remarkable aptitude for, sometimes matching or exceeding radiologist-level performance (Rajpurkar et al. 2017). But high accuracy alone is not enough. A clinician will not trust a model that simply outputs “pneumonia” without any indication of why. To be useful in practice, the model needs to point to the regions in the image that drove its decision.\nIn this post we train a CNN to classify chest X-rays as healthy or pneumonia, and then apply two popular attribution methods to visualize what the model has learned. Integrated Gradients provides pixel-level attribution scores by accumulating gradients along a path from a baseline image to the input. Grad-CAM produces coarser but more spatially coherent heatmaps by leveraging the gradients flowing into the last convolutional layer. We compare both methods and subject them to a data randomization test to verify that they produce genuine explanations rather than artifacts."
  },
  {
    "objectID": "posts/17-02-2026_ml4hc-pneumonia/index.html#the-data",
    "href": "posts/17-02-2026_ml4hc-pneumonia/index.html#the-data",
    "title": "Detecting Pneumonia from Chest X-Rays: CNNs, Integrated Gradients, and Grad-CAM",
    "section": "2 The Data",
    "text": "2 The Data\nThe dataset consists of pediatric chest X-ray images from Guangzhou Women and Children’s Medical Center, split into three classes: normal, bacterial pneumonia, and viral pneumonia. For our classification task we merge the two pneumonia subtypes into a single positive class, giving us a binary problem.\n\n\n\n\n\n\n\n\n\n\nThe label distribution is heavily skewed. Roughly 74% of images show pneumonia while only 26% are normal. A classifier that always predicts pneumonia would achieve 74% accuracy without learning anything, so balanced accuracy and F1 become the more informative metrics.\n\n2.1 Visual Differences\nEven to a non-expert eye, the two classes look different. Normal X-rays tend to be sharper with clearly delineated lung fields, visible rib structure, and a well-defined cardiac silhouette. Pneumonia images often show diffuse haziness or localized opacities that obscure parts of the lung, making it harder to distinguish the heart from the surrounding tissue.\n\n\n\n2.2 Preprocessing and Potential Bias\nRaw medical images come with practical complications. The X-rays in this dataset have varying resolutions, some contain text annotations (an “R” marker, or “A-P” labels in the corner), and the class imbalance is substantial. Each of these can introduce bias if not handled carefully. A model could learn to associate the “A-P” annotation with pneumonia rather than learning actual pathological features.\nOur preprocessing pipeline addresses these issues. We resize all images to 256x256 pixels, then apply a center crop to 224x224 to remove corner annotations. During training we additionally apply random rotations (up to 10 degrees), random horizontal flips, and random grayscale conversion for augmentation. These transformations make the model more robust to variations in X-ray orientation and contrast, while the center crop removes the text labels that could otherwise serve as spurious shortcuts."
  },
  {
    "objectID": "posts/17-02-2026_ml4hc-pneumonia/index.html#the-cnn-resnet-50",
    "href": "posts/17-02-2026_ml4hc-pneumonia/index.html#the-cnn-resnet-50",
    "title": "Detecting Pneumonia from Chest X-Rays: CNNs, Integrated Gradients, and Grad-CAM",
    "section": "3 The CNN: ResNet-50",
    "text": "3 The CNN: ResNet-50\n\n3.1 Convolutional Neural Networks for Images\nA convolutional neural network processes an image by sliding learned filters across the spatial dimensions. For a 2D input \\(\\mathbf{X} \\in \\mathbb{R}^{H \\times W}\\) and a filter \\(\\mathbf{K} \\in \\mathbb{R}^{k \\times k}\\), the convolution at position \\((i, j)\\) computes:\n\\[\n(\\mathbf{X} * \\mathbf{K})_{i,j} = \\sum_{m=0}^{k-1} \\sum_{n=0}^{k-1} K_{m,n} \\cdot X_{i+m, j+n}\n\\]\nStacking multiple convolutional layers with non-linear activations (typically ReLU) and pooling operations builds a hierarchy of feature representations. The early layers learn low-level features like edges and textures. The intermediate layers combine these into more complex patterns like shapes and anatomical structures. The deepest layers encode high-level semantic concepts that are directly relevant for classification.\n\n\n3.2 Residual Learning\nTraining very deep networks is notoriously difficult because gradients tend to vanish or explode as they propagate through many layers. The residual learning framework (He et al. 2016) addresses this by introducing skip connections that let the signal bypass blocks of layers:\n\\[\n\\mathbf{y} = \\mathcal{F}(\\mathbf{x}, \\{W_i\\}) + \\mathbf{x}\n\\]\nwhere \\(\\mathcal{F}\\) represents the stacked convolutional layers within a residual block. Instead of learning the full mapping from \\(\\mathbf{x}\\) to \\(\\mathbf{y}\\), the network only needs to learn the residual \\(\\mathcal{F}(\\mathbf{x}) = \\mathbf{y} - \\mathbf{x}\\), i.e. the correction on top of the identity. If the optimal mapping is close to the identity (which is common in deeper layers), the residual is close to zero and easy to learn. This simple modification enables the training of networks with 50, 100, or even 1000 layers.\n\n\n3.3 Transfer Learning with ResNet-50\nResNet-50 is a 50-layer residual network originally trained on ImageNet (Deng et al. 2009), a dataset of over 1.2 million natural images spanning 1000 classes. The convolutional backbone learns general visual features (edges, textures, shapes) that transfer well to other image domains, including medical imaging. We take a pretrained ResNet-50 and replace its final fully connected layer with a custom classification head consisting of linear layers, ReLU activations, and dropout for regularization. The output layer produces two logits for binary classification.\n\nThe model achieves 87% accuracy, 83% balanced accuracy, and an F1 score of 0.91. Recall is notably high at 0.99, meaning the model catches nearly every pneumonia case, but precision is lower at 0.83, indicating a non-trivial false positive rate. In a clinical screening context this trade-off is often acceptable: missing a pneumonia case (false negative) is far more dangerous than sending a healthy patient for additional testing (false positive)."
  },
  {
    "objectID": "posts/17-02-2026_ml4hc-pneumonia/index.html#explaining-the-predictions",
    "href": "posts/17-02-2026_ml4hc-pneumonia/index.html#explaining-the-predictions",
    "title": "Detecting Pneumonia from Chest X-Rays: CNNs, Integrated Gradients, and Grad-CAM",
    "section": "4 Explaining the Predictions",
    "text": "4 Explaining the Predictions\n\n4.1 Integrated Gradients\nIntegrated Gradients (Sundararajan, Taly, and Yan 2017) is an attribution method that assigns an importance score to each input pixel by asking: how does the model’s output change as we gradually transition from a blank baseline image to the actual input? Formally, for an input image \\(\\mathbf{x}\\), a baseline \\(\\mathbf{x}'\\) (typically an all-black image), and a model \\(F\\), the attribution for pixel \\(i\\) is:\n\\[\n\\text{IG}_i(\\mathbf{x}) = (x_i - x_i') \\int_{\\alpha=0}^{1} \\frac{\\partial F(\\mathbf{x}' + \\alpha(\\mathbf{x} - \\mathbf{x}'))}{\\partial x_i} \\, d\\alpha\n\\]\nThe integral accumulates the gradients of the model output with respect to pixel \\(i\\) along a straight-line path from the baseline to the input. In practice the integral is approximated by a Riemann sum over a finite number of interpolation steps. This method satisfies two desirable properties. Completeness guarantees that the attributions sum to the difference between the model’s output at the input and at the baseline, \\(F(\\mathbf{x}) - F(\\mathbf{x}')\\). Sensitivity ensures that if a feature contributes to changing the output relative to the baseline, it receives a non-zero attribution.\nThe attribution maps for healthy patients tend to highlight the lung fields and the spine in a relatively focused manner, consistent with the model confirming the absence of pathology in these regions.\n\nFor pneumonia patients, the attributions spread across a broader area of the chest, reflecting the diffuse nature of pneumonia opacities. The model does not focus on a single localized region but instead distributes importance across the entire affected area.\n\nOne important caveat is that Integrated Gradients is sensitive to the choice of baseline. Using an all-black image (pixel values of 0) versus an all-white image (pixel values of 1) produces noticeably different attribution maps. The all-black baseline tends to produce sharper, more anatomically focused attributions because the path integral traverses more of the relevant gradient landscape. The all-white baseline, being closer to the bright background regions of an X-ray, produces noisier results. This baseline dependency is a known limitation of the method and should be kept in mind when interpreting the maps.\n\n\n\n Baseline: all-black (pixel value 0)\n\n\n Baseline: all-white (pixel value 1)\n\n\n\n\n\n4.2 Grad-CAM\nGradient-weighted Class Activation Mapping (Selvaraju et al. 2017) takes a fundamentally different approach. Instead of computing per-pixel attributions, it produces a coarse localization map by leveraging the spatial information preserved in the last convolutional layer. For a target class \\(c\\), the method first computes the gradient of the class score \\(y^c\\) with respect to the feature maps \\(A^k\\) of the last convolutional layer:\n\\[\n\\alpha_k^c = \\frac{1}{Z} \\sum_i \\sum_j \\frac{\\partial y^c}{\\partial A^k_{ij}}\n\\]\nThese gradients are global-average-pooled to produce a weight \\(\\alpha_k^c\\) for each feature map \\(k\\), representing how important that feature map is for predicting class \\(c\\). The Grad-CAM heatmap is then a weighted combination of the feature maps, passed through a ReLU to retain only positive contributions:\n\\[\nL^c_{\\text{Grad-CAM}} = \\text{ReLU}\\!\\left(\\sum_k \\alpha_k^c A^k\\right)\n\\]\nThe resulting heatmap has the spatial resolution of the last convolutional layer (7x7 for ResNet-50) and is upsampled to the input image size for visualization. Because Grad-CAM operates on feature maps rather than individual pixels, it produces smoother, more spatially coherent attribution maps than Integrated Gradients. The trade-off is lower spatial resolution: it can tell you which region matters but not exactly which pixels.\nFor healthy patients, the Grad-CAM heatmaps concentrate on the central and lower lung regions, highlighting areas where the model confirms normal tissue.\n\nFor pneumonia patients, the activation maps expand to cover broader regions of the chest, often focusing on the areas where opacities are visible in the original X-ray. In several examples, the strongest activations align precisely with the haziest regions of the lung fields.\n\n\n\n4.3 Comparing the Two Methods\nThe two attribution methods tell a broadly consistent story. Both highlight the lung fields as the most decision-relevant region. Both show more diffuse attribution patterns for pneumonia cases and more localized patterns for healthy ones. The key difference is granularity. Integrated Gradients produces fine-grained, pixel-level attributions that can appear noisy, while Grad-CAM produces smoother heatmaps that are easier to interpret at a glance but lose fine spatial detail. For a radiologist who wants to quickly see where the model is “looking”, Grad-CAM is the more practical tool. For a researcher who wants to understand exactly which pixel intensities drive the output, Integrated Gradients provides more information."
  },
  {
    "objectID": "posts/17-02-2026_ml4hc-pneumonia/index.html#data-randomization-test",
    "href": "posts/17-02-2026_ml4hc-pneumonia/index.html#data-randomization-test",
    "title": "Detecting Pneumonia from Chest X-Rays: CNNs, Integrated Gradients, and Grad-CAM",
    "section": "5 Data Randomization Test",
    "text": "5 Data Randomization Test\n\n5.1 The Idea\nA natural worry with any attribution method is that it might produce plausible-looking maps regardless of whether the model has actually learned anything meaningful. The data randomization test, proposed by Adebayo et al. (2018), provides a simple sanity check. The idea is to retrain the model on randomly permuted labels and then examine whether the attribution maps change. A model trained on random labels cannot learn any genuine relationship between image content and diagnosis. If the attribution method still produces the same maps, it is reflecting properties of the input image rather than the model’s learned decision function.\n\n\n5.2 Results\nWe retrain the same ResNet-50 architecture on the pneumonia dataset with randomly permuted training labels for 200 epochs. As expected, the model’s balanced accuracy drops to 0.52, essentially random chance. The Integrated Gradients maps for the permuted-label model look qualitatively different from those of the properly trained model. The attributions for healthy and diseased patients become indistinguishable, with both showing a similar scattered pattern that outlines anatomical structures without any class-specific focus.\n\n\n\n Integrated Gradients on truly healthy patients (permuted-label model)\n\n\n Integrated Gradients on truly diseased patients (permuted-label model)\n\n\n\nThe Grad-CAM maps change even more dramatically. The permuted-label model produces much smaller, more fragmented activation regions that no longer correspond to clinically meaningful areas of the lung.\n\n\n\n Grad-CAM on truly healthy patients (permuted-label model)\n\n\n Grad-CAM on truly diseased patients (permuted-label model)\n\n\n\nBoth methods pass the data randomization test. The attribution maps change substantially when the model is trained on meaningless labels, confirming that they reflect the model’s learned decision function rather than low-level image properties. This is an important validation step that is often overlooked when deploying explainability tools in practice."
  },
  {
    "objectID": "posts/17-02-2026_ml4hc-pneumonia/index.html#reflections",
    "href": "posts/17-02-2026_ml4hc-pneumonia/index.html#reflections",
    "title": "Detecting Pneumonia from Chest X-Rays: CNNs, Integrated Gradients, and Grad-CAM",
    "section": "6 Reflections",
    "text": "6 Reflections\nThe CNN achieves reasonable classification performance (0.91 F1, 0.83 balanced accuracy), though it falls short of the near-perfect scores we saw in the ECG classification post. This is not surprising. Chest X-ray interpretation is inherently more ambiguous than ECG classification. The boundary between a normal and a mildly affected lung can be subtle, imaging conditions vary widely, and the model must learn to ignore irrelevant visual clutter (text annotations, equipment artifacts) that is absent from standardized ECG recordings.\nThe attribution maps provide reassuring evidence that the model focuses on clinically relevant regions. Grad-CAM in particular highlights areas within the chest cavity that correspond to visible opacities in pneumonia cases. For healthy patients, both methods attribute importance to the lung fields in a way that suggests the model is confirming the absence of pathological findings rather than relying on spurious shortcuts.\nThat said, the attribution maps are not perfectly consistent across all samples. Individual images occasionally show unexpected attribution patterns. This is a reminder that post-hoc explainability methods provide approximate insights into model behavior, not definitive proof of correct reasoning. The data randomization test adds a useful layer of confidence by confirming that the explanations are at least model-dependent, but it cannot tell us whether the model’s reasoning is medically sound. That judgment still requires a clinician in the loop.\nThe broader lesson is that explainability in medical imaging is not a single tool but a workflow. Train the model. Visualize its attributions with multiple methods. Verify those attributions with sanity checks. And ultimately, have a domain expert evaluate whether the highlighted regions make clinical sense. None of these steps alone is sufficient, but together they build a case for or against trusting the model’s decisions."
  },
  {
    "objectID": "posts/16-02-2026_ml4hc-ecg/index.html#introduction",
    "href": "posts/16-02-2026_ml4hc-ecg/index.html#introduction",
    "title": "Detecting Heart Attacks from ECGs: Classical ML, Deep Learning, and Representation Learning",
    "section": "1 Introduction",
    "text": "1 Introduction\nAn electrocardiogram (ECG) records the electrical impulses that coordinate every heartbeat. Cardiologists have relied on these waveforms for over a century, learning to spot the subtle deflections that distinguish a healthy heart from one that has suffered a myocardial infarction. The question we explore in this post is whether machine learning can do the same, and if so, which family of models is best suited for the task.\nThe project uses two publicly available ECG datasets. The first, PTB, contains binary-labeled recordings (healthy vs. myocardial infarction) and serves as the primary benchmark for comparing supervised classifiers. The second, MIT-BIH, contains five arrhythmia classes and is used to study transfer learning and learned representations. We train a wide range of models, from logistic regression all the way to transformers, and ask three questions along the way. Can classical methods compete with deep learning on raw ECG signals? What happens when we add hand-crafted features? And can self-supervised pretraining on one dataset help us classify another?"
  },
  {
    "objectID": "posts/16-02-2026_ml4hc-ecg/index.html#the-data",
    "href": "posts/16-02-2026_ml4hc-ecg/index.html#the-data",
    "title": "Detecting Heart Attacks from ECGs: Classical ML, Deep Learning, and Representation Learning",
    "section": "2 The Data",
    "text": "2 The Data\nThe PTB dataset consists of single-lead ECG recordings, each represented as a univariate time series. Every patient is labeled as either healthy or having suffered a myocardial infarction. The label distribution is noticeably skewed: patients with myocardial infarction outnumber healthy controls by roughly two to one.\n\nThis imbalance matters because a classifier that always predicts the majority class already achieves roughly 72% accuracy without learning anything meaningful. To counter this, we apply SMOTE (Synthetic Minority Oversampling Technique, Chawla et al. (2002)) to the training set. SMOTE generates synthetic minority-class examples by interpolating between existing ones, which both balances the class distribution and acts as a form of data augmentation. We never apply it to the validation or test data.\nAnother characteristic of the dataset is that all recordings are zero-padded to a uniform length. The amount of padding varies considerably across patients, reflecting natural differences in recording duration.\n\nEven from these raw traces, certain visual differences stand out. The healthy recordings tend to exhibit clean, well-defined waveform components, while some of the myocardial infarction recordings display broader, more irregular morphologies."
  },
  {
    "objectID": "posts/16-02-2026_ml4hc-ecg/index.html#classical-machine-learning",
    "href": "posts/16-02-2026_ml4hc-ecg/index.html#classical-machine-learning",
    "title": "Detecting Heart Attacks from ECGs: Classical ML, Deep Learning, and Representation Learning",
    "section": "3 Classical Machine Learning",
    "text": "3 Classical Machine Learning\n\n3.1 Elastic Net Logistic Regression\nLogistic regression models the probability of myocardial infarction as a linear function of the input passed through a sigmoid:\n\\[\nP(y = 1 \\mid \\mathbf{x}) = \\sigma(\\mathbf{w}^\\top \\mathbf{x} + b) = \\frac{1}{1 + e^{-(\\mathbf{w}^\\top \\mathbf{x} + b)}}\n\\]\nThe elastic net variant combines \\(\\ell_1\\) and \\(\\ell_2\\) penalties, giving the objective:\n\\[\n\\mathcal{L}(\\mathbf{w}, b) = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log \\hat{p}_i + (1 - y_i) \\log(1 - \\hat{p}_i) \\right] + \\lambda \\left( \\alpha \\|\\mathbf{w}\\|_1 + (1 - \\alpha) \\|\\mathbf{w}\\|_2^2 \\right)\n\\]\nwhere \\(\\lambda\\) controls overall regularization strength and \\(\\alpha \\in [0, 1]\\) balances the two penalty terms. The \\(\\ell_1\\) component encourages sparsity (driving irrelevant coefficients to exactly zero), while the \\(\\ell_2\\) component stabilizes the solution when features are correlated. Applied to the raw time series, where each of the roughly 180 time steps is treated as an independent feature, the model achieves a balanced accuracy of just 0.800. This is barely above the majority-class baseline, and for good reason: a linear decision boundary cannot capture the temporal structure that makes ECG interpretation possible.\n\n\n3.2 Random Forests\nA random forest constructs an ensemble of \\(B\\) decision trees, each trained on a bootstrap sample of the data with a random subset of features considered at each split. The final prediction is the majority vote:\n\\[\n\\hat{y} = \\text{mode}\\left\\{ \\hat{y}_1, \\hat{y}_2, \\dots, \\hat{y}_B \\right\\}\n\\]\nEach individual tree partitions the feature space through a sequence of axis-aligned splits that greedily maximize a purity criterion (typically the Gini impurity). Because each tree is trained on a different bootstrap sample and a different feature subset, the ensemble decorrelates the individual predictions and reduces variance. On the raw ECG time series, the random forest already reaches a balanced accuracy of 0.971, demonstrating that non-linear splits at specific time steps can capture waveform structure that a linear model entirely misses.\n\n\n3.3 Gradient Boosting\nGradient boosting (Ke et al. 2017) takes a fundamentally different approach to ensembling. Instead of training trees independently in parallel, it builds them sequentially, where each new tree fits the residual errors of the current ensemble:\n\\[\nF_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + \\eta \\cdot h_m(\\mathbf{x})\n\\]\nHere \\(F_{m-1}\\) is the ensemble after \\(m-1\\) iterations, \\(h_m\\) is a new shallow tree fitted to the negative gradient of the loss with respect to the current predictions, and \\(\\eta\\) is a learning rate that controls the contribution of each tree. This additive correction scheme means the model progressively focuses on the examples that are hardest to classify, typically reaching stronger predictive performance than a random forest of equivalent size. On the raw time series, LightGBM achieves a balanced accuracy of 0.982.\n\n\n3.4 Feature Engineering Closes the Gap\nThe zero padding at the end of each recording distorts any features computed over the full time series. Once we strip the padding and extract a rich set of time-domain statistics (autocorrelations, entropy measures, distributional summaries, and more), the picture changes dramatically. Logistic regression jumps from 0.800 to 0.941 in balanced accuracy. The tree-based models also improve, with LightGBM reaching 0.991. The message is clear: for classical methods, the representation matters more than the model. Compress the temporal structure of the ECG into the right set of features and even a linear classifier becomes competitive."
  },
  {
    "objectID": "posts/16-02-2026_ml4hc-ecg/index.html#deep-learning",
    "href": "posts/16-02-2026_ml4hc-ecg/index.html#deep-learning",
    "title": "Detecting Heart Attacks from ECGs: Classical ML, Deep Learning, and Representation Learning",
    "section": "4 Deep Learning",
    "text": "4 Deep Learning\n\n4.1 LSTMs\nLong Short-Term Memory networks (Hochreiter and Schmidhuber 1997) are designed for sequential data. At each time step \\(t\\), the LSTM updates a hidden state \\(\\mathbf{h}_t\\) and a cell state \\(\\mathbf{c}_t\\) through a system of gates:\n\\[\n\\begin{aligned}\n\\mathbf{f}_t &= \\sigma(\\mathbf{W}_f [\\mathbf{h}_{t-1}, x_t] + \\mathbf{b}_f) \\\\\n\\mathbf{i}_t &= \\sigma(\\mathbf{W}_i [\\mathbf{h}_{t-1}, x_t] + \\mathbf{b}_i) \\\\\n\\tilde{\\mathbf{c}}_t &= \\tanh(\\mathbf{W}_c [\\mathbf{h}_{t-1}, x_t] + \\mathbf{b}_c) \\\\\n\\mathbf{c}_t &= \\mathbf{f}_t \\odot \\mathbf{c}_{t-1} + \\mathbf{i}_t \\odot \\tilde{\\mathbf{c}}_t \\\\\n\\mathbf{o}_t &= \\sigma(\\mathbf{W}_o [\\mathbf{h}_{t-1}, x_t] + \\mathbf{b}_o) \\\\\n\\mathbf{h}_t &= \\mathbf{o}_t \\odot \\tanh(\\mathbf{c}_t)\n\\end{aligned}\n\\]\nThe forget gate \\(\\mathbf{f}_t\\) controls which information to discard from the cell state, the input gate \\(\\mathbf{i}_t\\) controls which new information to store, and the output gate \\(\\mathbf{o}_t\\) determines what to expose as the hidden state. This gating mechanism addresses the vanishing gradient problem that plagues vanilla RNNs, allowing the network to learn dependencies across longer stretches of the ECG.\nA bidirectional LSTM processes the signal in both directions simultaneously and concatenates the forward and backward hidden states. For ECG classification, where the full recording is available at inference time, a bidirectional model can in principle capture context that a unidirectional pass would miss. In practice, both variants achieve balanced accuracies of 0.990 on the PTB dataset, with the bidirectional model offering no statistically meaningful improvement. This is likely because the diagnostic patterns in these recordings are sufficiently localized that forward context alone is enough.\n\n\n4.2 Convolutional Neural Networks\nA 1D CNN slides learned filters across the recording and detects local patterns regardless of their absolute position in the sequence. For an input signal \\(\\mathbf{x} \\in \\mathbb{R}^T\\) and a filter \\(\\mathbf{k} \\in \\mathbb{R}^K\\), the convolution at position \\(t\\) is:\n\\[\n(\\mathbf{x} * \\mathbf{k})_t = \\sum_{i=0}^{K-1} k_i \\cdot x_{t+i}\n\\]\nStacking multiple convolutional layers with non-linear activations builds a hierarchy of increasingly abstract features: the early layers detect local waveform characteristics (slopes, peaks, inflection points) while the deeper layers combine these into higher-level diagnostic patterns. A key advantage over LSTMs is that all positions are processed in parallel, making CNNs considerably faster to train.\nAdding residual connections (He et al. 2016) allows gradients to bypass convolutional blocks via identity shortcuts:\n\\[\n\\mathbf{y} = \\mathcal{F}(\\mathbf{x}, \\{W_i\\}) + \\mathbf{x}\n\\]\nwhere \\(\\mathcal{F}\\) represents the stacked convolutional layers within a block. This formulation lets the network learn corrections on top of the identity mapping rather than learning the full transformation from scratch, which stabilizes training and enables deeper architectures. On the PTB dataset, the CNN with residual blocks achieves 0.997 accuracy and 0.996 balanced accuracy, the highest scores among all supervised models. Even the vanilla CNN without skip connections reaches 0.992, confirming that convolutional architectures are well suited for detecting the localized waveform anomalies that characterize myocardial infarction.\n\n\n4.3 Transformers\nThe transformer (Vaswani et al. 2017) replaces recurrence and convolution with self-attention. For an input sequence \\(\\mathbf{X} \\in \\mathbb{R}^{T \\times d}\\), the scaled dot-product attention is:\n\\[\n\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\!\\left(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{d_k}}\\right) \\mathbf{V}\n\\]\nwhere \\(\\mathbf{Q} = \\mathbf{X}\\mathbf{W}^Q\\), \\(\\mathbf{K} = \\mathbf{X}\\mathbf{W}^K\\), and \\(\\mathbf{V} = \\mathbf{X}\\mathbf{W}^V\\) are linear projections of the input into query, key, and value spaces. The attention weights \\(\\text{softmax}(\\mathbf{Q}\\mathbf{K}^\\top / \\sqrt{d_k})\\) form a \\(T \\times T\\) matrix that encodes how much each time step attends to every other time step. Multi-head attention runs \\(h\\) parallel attention computations with different projection matrices and concatenates the results, allowing the model to jointly attend to information from different representation subspaces.\nSince the self-attention mechanism is permutation-invariant, the model has no inherent notion of order. Sinusoidal positional encodings are added to the input embeddings to inject information about the absolute and relative positions of time steps.\nA critical consideration is computational cost. The self-attention matrix is \\(T \\times T\\), making both computation and memory \\(\\mathcal{O}(T^2)\\) in the sequence length. Recurrent networks scale linearly in \\(T\\) and convolutional networks scale as \\(\\mathcal{O}(K \\cdot T)\\) where \\(K\\) is the kernel size. For the short ECG recordings in our datasets the quadratic cost is acceptable, but it becomes a bottleneck for very long time series.\nOn the PTB dataset the transformer reaches 0.990 accuracy and 0.988 balanced accuracy. It does not surpass the residual CNN, but it offers something the CNN cannot: interpretable attention weights.\n\n\n4.4 What the Transformer Sees\nBy extracting and aggregating the attention weights across heads and layers, we can visualize which parts of the ECG the model focuses on when making its prediction. The figure below overlays the aggregated attention (blue heatmap) on the ECG signal (red) for several healthy and myocardial infarction samples.\n\nFor the myocardial infarction samples, the model places substantial attention on the QRS complex and on regions where the T wave appears abnormal (for instance, inverted T waves). For the healthy samples, attention concentrates on the same waveform components but reflects their normality rather than flagging anomalies. The zero-padded time steps receive no attention, confirming that the padding mask works as intended.\nStratifying by encoder layer reveals that earlier layers attend to broader regions of the signal while deeper layers focus more narrowly on specific peaks and segments.\n\nThese patterns align with cardiology. The QRS complex, the T wave, and the ST segment are precisely the waveform components that clinicians examine when diagnosing a myocardial infarction (Thygesen et al. 2018). The transformer learns to attend to the same regions without any explicit clinical guidance."
  },
  {
    "objectID": "posts/16-02-2026_ml4hc-ecg/index.html#representation-learning-and-transfer",
    "href": "posts/16-02-2026_ml4hc-ecg/index.html#representation-learning-and-transfer",
    "title": "Detecting Heart Attacks from ECGs: Classical ML, Deep Learning, and Representation Learning",
    "section": "5 Representation Learning and Transfer",
    "text": "5 Representation Learning and Transfer\n\n5.1 The Setup\nAll the models above are trained and evaluated on the same PTB dataset. But in clinical practice, labeled ECG data for a specific condition may be scarce. A natural strategy is to pretrain a model on a larger, readily available dataset and then adapt it to the target task. We use the MIT-BIH arrhythmia dataset (five classes) as the source domain and PTB (binary) as the target.\nThe idea is to train an encoder that maps each ECG recording into a compact, 16-dimensional embedding. A lightweight downstream classifier then operates on these embeddings for the PTB task. The question is how to pretrain the encoder: with labels (supervised transfer learning) or without them (self-supervised representation learning).\n\n\n5.2 Contrastive Learning with InfoNCE\nIn the self-supervised approach, we pretrain the encoder using the InfoNCE loss (Oord, Li, and Vinyals 2018). The objective pushes the encoder to produce similar embeddings for different augmented views of the same recording and dissimilar embeddings for different recordings. For an anchor embedding \\(\\mathbf{z}\\), a positive embedding \\(\\mathbf{z}^+\\) (augmented version of the same recording), and \\(N\\) negative embeddings \\(\\{\\mathbf{z}_i^-\\}\\), the loss is:\n\\[\n\\mathcal{L}_{\\text{InfoNCE}} = -\\log \\frac{\\exp(\\text{sim}(\\mathbf{z}, \\mathbf{z}^+) / \\tau)}{\\exp(\\text{sim}(\\mathbf{z}, \\mathbf{z}^+) / \\tau) + \\sum_{i=1}^{N} \\exp(\\text{sim}(\\mathbf{z}, \\mathbf{z}_i^-) / \\tau)}\n\\]\nwhere \\(\\text{sim}(\\cdot, \\cdot)\\) denotes cosine similarity and \\(\\tau\\) is a temperature parameter. Following Chen et al. (2020), a small projection head (an MLP) sits on top of the encoder and maps the 16-dimensional embeddings into a 32-dimensional space where the contrastive loss is computed. After pretraining, the projection head is discarded and only the encoder is kept.\nThe augmentation strategy is critical to the quality of the learned representations. For each anchor recording, we create a positive sample by applying one of three random transformations: adding Gaussian noise, scaling the amplitude, or slightly stretching the time axis. These augmentations preserve the clinical semantics of the signal while introducing enough variation to force the encoder to learn robust features. Augmentations that destroy the temporal order of the cardiac cycle (reversing, random permutation) proved harmful.\n\nThe figure above shows several examples. The original signal is in blue, the positive (augmented) sample in green, and four negative samples (randomly drawn from other recordings) in red.\n\n\n5.3 Finetuning Strategies\nGiven a pretrained encoder, there are three natural ways to adapt it to the PTB target task. Strategy A freezes the encoder entirely and trains only a new classification head on top. Strategy B unfreezes the full model and trains encoder and classifier jointly end-to-end. Strategy C starts frozen (as in A) and then unfreezes for a second training stage (as in B).\n\nFull finetuning (Strategy B) consistently performs best, because it allows the encoder weights to adapt to the specific characteristics of the PTB dataset rather than relying solely on what was learned from MIT-BIH. The frozen encoder (Strategy A) performs worst since its representations were never exposed to PTB data. Strategy C sits in between.\nThe more interesting result is that the contrastive encoder outperforms the supervised one across all three finetuning strategies. The supervised encoder is trained with MIT-BIH class labels, which biases its representations toward the five-class arrhythmia task. The contrastive encoder, by contrast, learns general-purpose signal structure that transfers more gracefully to a different classification problem on a different dataset. The best configuration overall, the contrastive encoder with full finetuning, achieves 0.996 accuracy and 0.994 balanced accuracy on PTB."
  },
  {
    "objectID": "posts/16-02-2026_ml4hc-ecg/index.html#reflections",
    "href": "posts/16-02-2026_ml4hc-ecg/index.html#reflections",
    "title": "Detecting Heart Attacks from ECGs: Classical ML, Deep Learning, and Representation Learning",
    "section": "6 Reflections",
    "text": "6 Reflections\nThe most striking finding is how competitive classical tree-based methods are once they receive well-engineered features. A LightGBM model trained on extracted time-series statistics matches or exceeds most deep learning architectures. For univariate ECG signals with relatively clear diagnostic markers, the bottleneck is often the input representation rather than the model.\nDeep learning earns its keep in two ways. First, it removes the need for manual feature engineering by learning directly from the raw signal. Second, architectures like the transformer provide attention-based interpretability that can surface clinically meaningful patterns without any domain-specific preprocessing. The fact that our transformer independently learns to focus on the QRS complex and T wave abnormalities is reassuring from a clinical trustworthiness perspective.\nThe contrastive learning results point to a promising direction for label-scarce settings. By learning general ECG representations without supervision, we obtain an encoder that transfers more effectively than one trained with task-specific labels. The practical challenge is that contrastive pretraining requires careful augmentation design and is considerably more expensive to train.\nAcross the board, the margins between top-performing models are thin. The residual CNN, the contrastive encoder with full finetuning, and LightGBM with engineered features all cluster above 0.99 on most metrics. In this regime, the choice of model matters less than the quality of data preprocessing and the degree to which the practitioner understands what the model has actually learned."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nDetecting Pneumonia from Chest X-Rays: CNNs, Integrated Gradients, and Grad-CAM\n\n\n\n\n\n\nPython\n\n\nData Science\n\n\nMachine Learning\n\n\nDeep Learning\n\n\nHealthcare\n\n\n\nWe train a ResNet-50-based convolutional neural network to classify chest X-ray images as healthy or pneumonia. To understand what the model actually learns, we apply two attribution methods, Integrated Gradients and Grad-CAM, and compare the regions they highlight. A data randomization test confirms that both methods produce genuinely model-dependent explanations rather than artifacts. The attribution maps reveal that the CNN focuses on clinically sensible lung regions, with broader activation patterns for pneumonia cases and more localized attention for healthy patients.\n\n\n\n\n\nFeb 17, 2026\n\n\nJan Schlegel\n\n\n\n\n\n\n\n\n\n\n\n\nDetecting Heart Attacks from ECGs: Classical ML, Deep Learning, and Representation Learning\n\n\n\n\n\n\nPython\n\n\nData Science\n\n\nMachine Learning\n\n\nDeep Learning\n\n\nHealthcare\n\n\n\nWe classify electrocardiogram recordings as healthy or indicative of myocardial infarction using a progression of increasingly powerful methods. Starting from logistic regression and gradient boosting on raw signals, we show that feature engineering closes much of the gap to deep learning. We then train LSTMs, CNNs, and a transformer, and use attention maps to visualize which parts of the ECG signal drive predictions. Finally, we explore whether contrastive representation learning can produce embeddings that transfer across datasets and outperform supervised pretraining.\n\n\n\n\n\nFeb 16, 2026\n\n\nJan Schlegel\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Heart Disease: When Simple Models Rival Deep Learning\n\n\n\n\n\n\nPython\n\n\nData Science\n\n\nMachine Learning\n\n\nDeep Learning\n\n\nHealthcare\n\n\n\nWe train a Lasso logistic regression, a multi-layer perceptron, and a neural additive model to predict heart disease from routine clinical data. All three achieve high and comparable accuracy. Using SHAP values, learned shape functions, and coefficient analysis to interpret each model, we find that they independently identify the same clinical features as most important, consistent with established cardiology literature.\n\n\n\n\n\nFeb 15, 2026\n\n\nJan Schlegel\n\n\n\n\n\n\n\n\n\n\n\n\nData-Driven Tactics: Analyzing Spain’s Playing Style in Euro 2024 Qualifiers\n\n\n\n\n\n\nPython\n\n\nData Science\n\n\nData Analysis\n\n\nVisualization\n\n\nSports Analytics\n\n\n\nWe examine Spain’s characteristic ‘Tiki-Taka’ playing style through multiple analytical lenses: offside patterns, passing networks, set-piece execution, formation dynamics, and defensive line positioning. Our analysis reveals how Spain maintains possession through short passing sequences involving defenders, utilizes full pitch width when in possession, and employs a high defensive line that reflects their controlling philosophy. We also identify tactical vulnerabilities, particularly Spain’s susceptibility to offsides and counter-attacks, providing actionable insights for opposition teams.\n\n\n\n\n\nFeb 14, 2026\n\n\nJan Schlegel\n\n\n\n\n\n\n\n\n\n\n\n\nComparing Univariate and Multivariate Models for Value at Risk Forecasting\n\n\n\n\n\n\nSimulation\n\n\nR\n\n\n\nThis post explores the effectiveness of univariate and multivariate GARCH-based models in forecasting Value at Risk (VaR) for a long equity portfolio. While multivariate models generally perform better in backtests, univariate models often fall short. However, neither model type consistently outperforms the other in predictive accuracy, highlighting the trade-offs between simplicity and complexity in risk forecasting.\n\n\n\n\n\nFeb 7, 2025\n\n\nJan Schlegel\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jan Schlegel",
    "section": "",
    "text": "Hi there! I’m Jan, a statistician and product manager based in Zurich. I recently completed my Master’s in Statistics at ETH Zurich, specializing in probabilistic AI, high-dimensional statistics, and causality. My work focuses on understanding and improving how machine learning systems make decisions—from interpreting generative models to building robust methods for high-stakes applications like climate forecasting and reinsurance.\nWhen I’m not working on difficult (for me) problems, you’ll find me running, in the gym, swimming, building PCs, or getting lost (intentionally) in GeoGuessr."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Jan Schlegel",
    "section": "Education",
    "text": "Education\n\n\n🎓 Master of Science (M.Sc. ETH) in Statistics\n\n🏛️ ETH Zurich | 📍 Zurich, Switzerland | 📅 Sep 2023 - Sep 2025\n\n\nGPA: 5.99 / 6.00\n\n\nFocus: (Probabilistic) Machine Learning, Statistical Learning Theory, Computational Statistics, Causality\nThesis: Mechanistic Interpretability in Text-to-Image Diffusion Models (Grade: 6.00/6.00)\nSemester Paper: Extrapolation and Distributional Robustness for Climate Downscaling\n\n\n\n🎓 Bachelor of Arts in Business and Economics\n\n🏛️ University of Zurich | 📍 Zurich, Switzerland | 📅 Sep 2019 - Feb 2023\n\n\nGPA: 5.87 / 6.00 (Class Rank 1)\n\n\nMajor: Banking and Finance; Minor: Statistics\nThesis: Portfolio Value at Risk Forecasting with Copula-GARCH Models (Grade: 6.00/6.00)\nCompleted 215 ECTS (vs. standard 180) with a strong focus on quantitative finance, statistics, and computing"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Jan Schlegel",
    "section": "Experience",
    "text": "Experience\n\n\n💼 Product Manager\n\n🏛️ Swiss Re | 📍 Zurich, Switzerland | 📅 Feb 2026 - Present\n\n\nDrive robust portfolio optimization initiatives across multiple product lines\nTransitioned to full-time role following successful quantitative internship\n\n\n\n💼 Quantitative Intern\n\n🏛️ Swiss Re | 📍 Zurich, Switzerland | 📅 Oct 2025 - Jan 2026\n\n\nDeveloped robust high-dimensional covariance estimation methods applicable to multiple reinsurance contexts\nDesigned robust portfolio optimization techniques tailored to reinsurance, providing actionable steering insights\n\n\n\n💼 Biostatistics Research Assistant\n\n🏛️ University of Zurich (EBPI) | 📍 Zurich, Switzerland | 📅 Feb 2022 - Aug 2023\n\n\nContributed to data and code management and reproducible statistical workflows for a COVID-19 cohort study published in Nature Communications\nApplied statistical modelling techniques to complex longitudinal datasets and presented the results to principal investigators and the research team\n\n\n\n💼 Accountant (Military Service)\n\n🏛️ Swiss Armed Forces | 📍 Kloten, Switzerland | 📅 Jul 2021 - Nov 2021\n\n\nSuccessfully balanced full-time university studies with mandatory military service obligations\nManaged the complete accounting for a 150-person military unit\n\n\n\n💼 Teaching Assistant Mathematics\n\n🏛️ University of Zurich | 📍 Zurich, Switzerland | 📅 Sep 2020 - Jul 2021\n\n\nLed twice-weekly tutorials covering Analysis and Linear Algebra for groups of up to 30 students"
  },
  {
    "objectID": "posts/15-02-2026_ml4hc-heart-disease/index.html#introduction",
    "href": "posts/15-02-2026_ml4hc-heart-disease/index.html#introduction",
    "title": "Predicting Heart Disease: When Simple Models Rival Deep Learning",
    "section": "1 Introduction",
    "text": "1 Introduction\nCardiovascular diseases remain by far the leading cause of death worldwide, and a lot of clinical decision-making still depends on a physician’s intuition and experience. The question we set out to answer in a course project for ETH’s Machine Learning for Healthcare class was straightforward: given a handful of routine clinical measurements, can we reliably predict whether a patient has heart disease? And more importantly, can we do so in a way that a cardiologist could actually scrutinize and trust?\nWe train three models that sit at very different points on the interpretability spectrum: a Lasso logistic regression (fully transparent), a multi-layer perceptron (a black box that needs post-hoc explanation), and a neural additive model (a recent architecture designed to combine neural network flexibility with additive model interpretability)."
  },
  {
    "objectID": "posts/15-02-2026_ml4hc-heart-disease/index.html#the-data",
    "href": "posts/15-02-2026_ml4hc-heart-disease/index.html#the-data",
    "title": "Predicting Heart Disease: When Simple Models Rival Deep Learning",
    "section": "2 The Data",
    "text": "2 The Data\nWe utilize the Heart Failure Prediction Dataset from Kaggle — a popular benchmark that combines several older heart disease studies into one clean table. Each of the 734 training samples is a patient described by 11 clinical features and a binary label: heart disease or not. The features cover the usual suspects you would find in a cardiology workup — age, resting blood pressure, cholesterol, maximum heart rate, ST segment depression during exercise — alongside categorical variables like sex, chest pain type, fasting blood sugar, resting ECG result, exercise-induced angina, and ST slope direction. For a detailed description of the features, please see the Kaggle dataset documentation.\nEven a quick pair plot of the numeric features hints at what the models will later confirm:\n\nPatients with heart disease tend to be older, reach lower maximum heart rates during exercise, and show higher Oldpeak values (a measure of ST segment depression on an electrocardiogram). None of these relationships are dramatic on their own, but in combination they start painting a picture.\nLooking at the categorical side, several features are heavily imbalanced (most study participants are male, most chest pain is asymptomatic), but the target variable itself is reasonably balanced — so no need for oversampling tricks."
  },
  {
    "objectID": "posts/15-02-2026_ml4hc-heart-disease/index.html#three-models-one-explainability-technique",
    "href": "posts/15-02-2026_ml4hc-heart-disease/index.html#three-models-one-explainability-technique",
    "title": "Predicting Heart Disease: When Simple Models Rival Deep Learning",
    "section": "3 Three Models, One Explainability Technique",
    "text": "3 Three Models, One Explainability Technique\n\n3.1 Lasso Logistic Regression\nStandard logistic regression models the probability of the positive class (heart disease) as a linear function of the features passed through a sigmoid:\n\\[\nP(y = 1 \\mid \\mathbf{x}) = \\sigma(\\mathbf{w}^\\top \\mathbf{x} + b) = \\frac{1}{1 + e^{-(\\mathbf{w}^\\top \\mathbf{x} + b)}}\n\\]\nwhere \\(\\mathbf{w} \\in \\mathbb{R}^p\\) is the weight vector, \\(b\\) is the bias, and \\(\\sigma\\) is the logistic sigmoid function. The model is trained by minimizing the negative log-likelihood (binary cross-entropy). The Lasso variant adds an \\(\\ell_1\\) penalty on the weights, giving the objective:\n\\[\n\\mathcal{L}(\\mathbf{w}, b) = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log \\hat{p}_i + (1 - y_i) \\log(1 - \\hat{p}_i) \\right] + \\lambda \\|\\mathbf{w}\\|_1\n\\]\nwhere \\(\\lambda &gt; 0\\) controls the regularization strength. The key property of the \\(\\ell_1\\) penalty is that it encourages sparsity i.e. the possibility to drive coefficients all the way to exactly zero whilst keeping the objective convex. Realize that the \\(\\ell_1\\) penalty is only applied to the weights \\(\\mathbf{w}\\), not the bias \\(b\\), to maintain translation invariance. Finally, the problem \\[\n\\arg\\min_{\\mathbf{w}, b} \\mathcal{L}(\\mathbf{w}, b)\n\\] can be solved efficiently using coordinate descent or proximal gradient methods.\nThe induced sparsity makes the Lasso a natural starting point for any clinical prediction task where interpretability is a hard requirement: each non-zero coefficient directly tells you the direction and magnitude of that feature’s influence on the log-odds of disease. Specifically, after selecting \\(\\lambda\\) through cross-validation and standardizing all numeric and dummy encoded features to ensure equal penalization, we obtained the coefficient plot below:\n\n\n\nThe two most prominent predictors are immediately visible. A flat ST slope has the largest positive coefficient in the model, while an upsloping ST slope has the largest negative coefficient, suggesting a protective association. This is consistent with cardiology literature: a flat or downsloping ST segment during exercise is a well-documented marker of myocardial ischemia, while an upsloping response is considered normal (Hodnesdal et al. 2013). Asymptomatic chest pain — which despite its name refers to a specific clinical presentation where patients experience atypical or no chest symptoms — has the third largest coefficient magnitude. This is a known red flag in cardiology, as patients who present without classic chest pain symptoms are often at higher risk because their disease goes undetected longer.\n\n\n3.2 SHAP Values\nMoving forward, we need a way to make the predictions of neural networks more explainable in a way that provides little additional overhead. Shapley additive explanations (SHAP) values (Lundberg and Lee (2017)) are the most well-known tool for such post-hoc explainability insights. SHAP is rooted in cooperative game theory with the idea to treat a prediction as a “game” where the features are “players” and the prediction is the “payout”, and then ask: how much did each player contribute? The Shapley value of feature \\(j\\) for a specific prediction is defined as:\n\\[\n\\phi_j = \\sum_{S \\subseteq \\{1, \\dots, p\\} \\setminus \\{j\\}} \\frac{|S|!\\;(p - |S| - 1)!}{p!} \\left[ f(S \\cup \\{j\\}) - f(S) \\right]\n\\]\nwhere \\(S\\) is a subset of features, \\(f(S)\\) is the model’s expected output when only the features in \\(S\\) are “present” (and the remaining features are marginalized out), and \\(p\\) is the total number of features. The sum iterates over all possible subsets \\(S\\) that exclude feature \\(j\\), and computes the marginal contribution of adding \\(j\\) to each subset, weighted by the number of permutations in which that subset would arise. This yields an explainability method that is theoretically grounded, model-agnostic, and provides local explanations for individual predictions. In practice, exact computation of SHAP values is intractable for large feature sets, so various approximation methods (like Kernel SHAP or Tree SHAP) are used depending on the model type.\n\n\n3.3 Multi-Layer Perceptron\nA multi-layer perceptron (MLP) is a feed-forward neural network consisting of an input layer, one or more hidden layers, and an output layer. Given an input \\(\\mathbf{x} \\in \\mathbb{R}^p\\), the forward pass through an MLP with \\(L\\) hidden layers computes:\n\\[\n\\mathbf{h}^{(0)} = \\mathbf{x}, \\qquad \\mathbf{h}^{(\\ell)} = \\phi\\!\\left(\\mathbf{W}^{(\\ell)} \\mathbf{h}^{(\\ell-1)} + \\mathbf{b}^{(\\ell)}\\right) \\quad \\text{for } \\ell = 1, \\dots, L\n\\]\nwhere \\(\\mathbf{W}^{(\\ell)}\\) and \\(\\mathbf{b}^{(\\ell)}\\) are the weight matrix and bias vector of layer \\(\\ell\\), and \\(\\phi\\) is a non-linear activation function (typically ReLU: \\(\\phi(z) = \\max(0, z)\\)). For binary classification, the final layer produces a scalar output passed through a sigmoid: \\(\\hat{p} = \\sigma(\\mathbf{w}^{(L+1)\\top} \\mathbf{h}^{(L)} + b^{(L+1)})\\).\nThe power of MLPs lies in their expressiveness — with sufficient width and depth, they can approximate arbitrarily complex functions (the universal approximation theorem). The downside is that the learned representations are distributed across many neurons, layers, and non-linear transformations, making it essentially impossible to attribute a prediction to any single input feature by inspecting the weights alone. To crack open this black box, we turned to SHAP values.\nThe waterfall plots below show SHAP explanations for two healthy patients and two patients with heart disease:\n\n\n\n Two healthy patients: ST Slope Upsloping is the dominant protective factor\n\n\n Two diseased patients: Old Peak and Cholesterol drive risk upward\n\n\n\nFor the healthy patients, having an upsloping ST slope was the strongest factor pushing the prediction toward “no disease”. For the diseased patients, high Oldpeak values and low cholesterol readings (likely the zero-imputed missing values) were the biggest risk drivers. The cholesterol finding might seem paradoxical at first — shouldn’t high cholesterol be bad? — but recall that the zero values were overwhelmingly associated with heart disease. The model is picking up on the missing-data signal, not on low cholesterol being protective.\nThe overall SHAP beeswarm plot across all test patients gives a broader view:\n\nEach dot is one patient’s SHAP value for a given feature, colored by whether the feature value was high (red) or low (blue). The pattern for ST Slope Upsloping is particularly clean: high feature values (red, meaning the patient does have an upsloping slope) cluster exclusively on the negative SHAP side, while low values (blue, meaning no upsloping slope) cluster on the positive side. This is exactly the same story the Lasso told us, just from a completely different model.\n\n\n3.4 Neural Additive Models\nNeural Additive Models (NAMs), introduced by Agarwal et al. (2020), are an elegant attempt to get the best of both worlds. The model belongs to the family of generalized additive models (GAMs), which restrict the prediction to a sum of univariate functions:\n\\[\ng\\!\\left(\\mathbb{E}[y \\mid \\mathbf{x}]\\right) = \\beta + f_1(x_1) + f_2(x_2) + \\dots + f_k(x_k)\n\\]\nwhere \\(g\\) is a link function (the logit for binary classification), \\(\\beta\\) is a bias term, and each \\(f_j\\) is a shape function that depends on a single feature. Classical GAMs typically use splines for the \\(f_j\\)’s. The key innovation of NAMs is to parameterize each shape function as its own small neural network:\n\\[\nP(y = 1 \\mid \\mathbf{x}) = \\sigma\\!\\left(\\beta + \\sum_{j=1}^{k} \\text{NN}_j(x_j)\\right)\n\\]\nwhere each \\(\\text{NN}_j\\) is a separate multi-layer perceptron that takes only \\(x_j\\) as input. Because each sub-network contains non-linear activation functions, the shape functions can be arbitrarily complex — unlike the linear relationships that logistic regression is limited to. But because the overall model is additive (no interactions between features), each learned shape function can be visualized independently.\n\n\n\nNAM architecture. Source: Agarwal et al. (2020), Figure 1.\n\n\nThis additive structure is the key insight. Because each feature is processed in isolation, there are no interactions between features inside the model. This means we can plot \\(\\text{NN}_j(x_j)\\) as a function of \\(x_j\\) for every feature and see exactly how the model uses it — something that is impossible with a standard MLP where all features interact through shared hidden layers.\nIn practice, we found NAMs to be sensitive to hyperparameter choices. Initial attempts with hand-tuned parameters produced unstable shape functions that changed substantially across random seeds. Following the original paper, we used Bayesian optimization (via Optuna) to search the hyperparameter space, and trained an ensemble of 100 NAMs with different random initializations to get stable estimates.\nThe resulting shape functions are shown below. The thick blue line is the ensemble mean, the thin lines are individual ensemble members, and the red bars indicate data density:\n\nThese plots are where NAMs really shine. For the continuous features, we can read off the non-linear relationships directly. The Cholesterol shape function shows a sharp spike at zero (confirming the missing-data signal), followed by a relatively flat region for normal values and a slight uptick at very high values. Oldpeak shows a roughly monotonic increase — greater ST depression means higher risk. For the binary features, the shape functions collapse to simple step functions since there are only two possible inputs.\nWe also computed SHAP values for the NAM to compare with the MLP:\n\nAn interesting difference emerges when comparing this to the MLP’s SHAP plot. Because each feature is modeled by its own sub-network in the NAM, the SHAP values for a given feature depend only on that feature’s value — not on the values of other features. This is why the SHAP dots for binary features appear as two clean vertical lines (one per category), while in the MLP’s SHAP plot the same features show much more spread. The NAM’s additive structure constrains the explanations to be simpler and more consistent."
  },
  {
    "objectID": "posts/15-02-2026_ml4hc-heart-disease/index.html#how-do-they-compare",
    "href": "posts/15-02-2026_ml4hc-heart-disease/index.html#how-do-they-compare",
    "title": "Predicting Heart Disease: When Simple Models Rival Deep Learning",
    "section": "4 How Do They Compare?",
    "text": "4 How Do They Compare?\n\n4.1 Performance\nAfter training a linear model, a black-box neural network, and an interpretable neural architecture on the same data, the natural question is: does the added complexity buy us anything? The short answer is no.\n\nThe Lasso leads on balanced accuracy (0.826) and precision (0.851). The NAM narrowly edges ahead on F1 (0.867), recall (0.891), and ROC AUC (0.894). The MLP lands somewhere in between. With only 734 patients and margins this thin, these differences are well within the range of statistical noise — the three models are, for all practical purposes, tied.\n\n\n4.2 Feature Importance Agreement\nThe performance numbers are a draw, but the more compelling finding lies in what the models agree on. Despite having fundamentally different internal mechanics, all three consistently identify the same features as the strongest predictors:\n\nST Slope is the single most important feature in all three models. An upsloping ST segment is protective; a flat one is a risk factor. This is well-established in cardiology: a flat or downsloping ST response to exercise suggests insufficient blood flow to the heart muscle.\nOldpeak (ST depression during exercise) is the second or third most important feature across all models. Higher values indicate greater risk, consistent with the clinical interpretation of exercise-induced ischemia.\nChest Pain Type: Asymptomatic is a strong positive predictor of heart disease in all three models. This reflects the clinical reality that patients with atypical presentations are often diagnosed later and with more advanced disease.\n\nThis level of agreement across fundamentally different model families is reassuring. It suggests that these features are genuinely predictive of heart disease in this population, not artifacts of any particular modeling choice."
  },
  {
    "objectID": "posts/15-02-2026_ml4hc-heart-disease/index.html#reflections",
    "href": "posts/15-02-2026_ml4hc-heart-disease/index.html#reflections",
    "title": "Predicting Heart Disease: When Simple Models Rival Deep Learning",
    "section": "5 Reflections",
    "text": "5 Reflections\nSomewhat surprisingly, all three models perform on par in raw accuracy, making the Lasso the clear preferred choice for clinical deployment. A cardiologist can inspect its coefficients in a single table and immediately see what is driving a prediction. The MLP requires post-hoc SHAP explanations; the NAM requires understanding shape functions. When patient outcomes are on the line, the model a clinician can scrutinize and override with the least friction is the one worth deploying.\nThat said, the NAM proved its value as an exploratory tool. Its shape functions surfaced non-linear relationships that a linear model would miss. Most strikingly, the sharp spike in risk at zero recorded cholesterol is a data-quality signal that a Lasso would need a manually engineered binary indicator to capture. The NAM discovers it on its own. For hypothesis generation and deeper data understanding, this is a meaningful advantage. On the practical side, however, NAMs are considerably harder to train than either the Lasso or the MLP. The shape functions are sensitive to hyperparameter choices and random initialization, and producing reliable estimates required Bayesian hyperparameter optimization and ensembling over 100 models.\nThe broader takeaway is that the most productive question in healthcare ML is rarely “which model achieves the highest AUC?” but rather “do we understand what the model has learned, and does it align with clinical knowledge?” Here, three architectures spanning the full interpretability spectrum independently converged on the same risk factors, and those factors match established cardiology. That convergence carries more weight than any single number on a leaderboard."
  },
  {
    "objectID": "posts/14-02-2026_soccer-analytics/index.html#introduction",
    "href": "posts/14-02-2026_soccer-analytics/index.html#introduction",
    "title": "Data-Driven Tactics: Analyzing Spain’s Playing Style in Euro 2024 Qualifiers",
    "section": "1 Introduction",
    "text": "1 Introduction\nModern football analytics has transformed how we understand team tactics and playing styles. By leveraging detailed match event and tracking data, we can quantify patterns that were previously only observable through subjective analysis. This post examines Spain’s playing dynamics during the Euro 2024 qualifiers, combining Wyscout event data with Skillcorner tracking data to provide a multi-faceted tactical analysis.\nSpain’s national team has long been synonymous with possession-based football and their distinctive ‘Tiki-Taka’ style—characterized by short passing sequences, high technical skill, and positional fluidity. However, this approach comes with inherent tactical trade-offs. Through systematic data analysis, we can identify not only the strengths of this philosophy but also potential weaknesses that opposition teams might exploit.\nThe full code for this analysis is available on  GitHub. Note that the underlying data is confidential and therefore not included in the repository.\n\n1.1 Offside Patterns\nBased on our viewing experience we expected that Spain would have a relatively high number of offsides compared to other teams. A first glance at the data later confirmed that Spain was indeed among the teams with the most offsides in the Euro 2024 qualifiers. This is also displayed in the table below which is based on all Wyscout data of all matches after removing all of the non-European teams. Interestingly, Albania is also among the top ten teams with the highest number of offsides per game.\nOne player particularly caught our eye: Álvaro Morata who was caught offside eight times across his six game appearances. However, as nominal data can be misleading in football, we decided to additionally have a look at the normalized offside calls per player. In the below plot we can see that albeit Álvaro Morata being flagged offside more frequently than his teammates he is not the player with the highest offside rate due to his high playing time (461 played minutes). Instead, the first place goes to Ansu Fati who, despite being caught offside only once, had the highest offside rate due to only playing for 45 minutes.\n\nMoreover, most of the offsides by the Spanish team occured after a forward pass attempt (most frequently by Dani Carvajal) into the final third of the pitch. The animations below show four examples of Spanish offside positions captured from different qualifying matches.\n\n\n\n\n\n\nCross from Dani Carvajal to Álvaro Morata (Spain vs Scotland)\n\n\n\n\n\nThrough ball from Dani Carvajal to Gavi (Spain vs Georgia)\n\n\n\n\n\n\n\nThrough ball from Pedro Porro to Álvaro Morata (Spain vs Scotland)\n\n\n\n\n\nThrough ball from Dani Carvajal to Álvaro Morata (Spain vs Scotland)\n\n\n\n\n\n1.2 Passing Patterns and Accuracy\nAdditionally, we investigated the relationship between passing accuracy and number of passes when normalizing the number of passes by the minutes played in the left plot below. It is apparent that center backs tend to have the highest number of passes and the highest accuracy whilst forwards usually have the lowest passing completion rate with few passes per minute. This is in line with the typical Spanish “Tiki-Taka” playstyle where risky passes are often avoided and instead the ball is circulated around the pitch frequently involving defenders in the build up. Moreover, when stratifying the accuracy by whether or not the player was under pressure at the time of the pass, we observe that the accuracy of the passes under pressure tends to be lower than the passes without pressure. This is displayed in the right plot below where the forwards again are in the bottom left corner and the center back in the top right corner. Our explanation for this is that the forwards often will be pressured by multiple defenders yielding a much lower accuracy when under pressure compared to under no pressure. Interestingly, the pass completion percentage of the Goalkeeper Unai Simón is much lower when under pressure probably because he just clears the ball away whenever under pressure. Finally, be aware that contains only relatively few passes under pressure and thus we had to restrict ourselves to players with a minimum of 135 minutes (equivalent to 1.5 full games) played to not overcrowd the plot with players that have made no or only few passes under pressure.\n\n\n\n\n\n\n\n\n\n\nThis “false nine” role is further emphasized in the passing sonars below with the central forward only involved in few passes. Judging from the color fill of the sonars it appears that apart from the goalkeeper and the central midfielders, the players generally tend to avoid long passes. Again, we can see that forwards often pass backwards to keep possession, while the defenders often pass the ball back and forth to each other, and the central midfielders distribute it and play out wide. These patterns demonstrate that the Spanish team remains faithful to their beloved “Tiki-Taka” style of play."
  },
  {
    "objectID": "posts/14-02-2026_soccer-analytics/index.html#set-piece-analysis",
    "href": "posts/14-02-2026_soccer-analytics/index.html#set-piece-analysis",
    "title": "Data-Driven Tactics: Analyzing Spain’s Playing Style in Euro 2024 Qualifiers",
    "section": "2 Set Piece Analysis",
    "text": "2 Set Piece Analysis\nIn the following subsections, we will examine different set pieces, how they are executed by the Spanish team and how they change the game dynamics and outcomes.\n\n2.1 Throw-ins\nThe plots below show the throw-in sonars (left) and trajectories (right) for the Spanish team across all eight qualifying matches and are based on Wyscout data. The throw-in sonars show the angle of the throw-ins, with the length of the sonar indicating the frequency of throw-ins in that direction and the color specifying the length of the throw-in in meters. The plot on the right shows the trajectory from the beginning to the end location of the throw-ins colored by whether the throw-in was accurate or not. In the plot on the right, Spain attacks from the bottom to the top of the plot.\n\n\n\n\n\n\n\n\n\n\nIt is apparent from the throw-in trajectories on the right that most of the throw-ins for the Spanish team take place in the middle and attacking thirds of the pitch. Moreover, there does not appear to be a systematic relationship between throw-in accuracy and the different thirds. In general, there are only few inaccurate throw-ins despite the average throw-in distance being almost 20 meters. This can be explained by the throw-in sonars from which it is evident that most of the throw-ins and directed backwards, especially in the attacking third. Thus, the Spanish team does not rely on throw-ins to create scoring opportunities in the attacking third but instead utilize them to maintain possession and stabilize their play.\n\n\n2.2 Corners\nThe plot below shows the corner kick trajectories for the Spanish team colored by accuracy, where a corner is considered accurate if a Spanish player successfully receives the ball. Overall, Spain’s corner accuracy is striking, especially for deliveries going directly into the box. We can also see that many corners are played short and backwards, likely to retain possession and create a better crossing angle.\nInterestingly, there is a notable asymmetry between the two sides: corners from the left side are overwhelmingly accurate, even when delivered into the middle of the box. In contrast, corners from the right side that target the area close to the goal are frequently inaccurate, suggesting that the in-swinging deliveries from the left are more effective than the out-swinging ones from the right for Spain’s set-piece setup."
  },
  {
    "objectID": "posts/14-02-2026_soccer-analytics/index.html#formation-and-positioning-analysis",
    "href": "posts/14-02-2026_soccer-analytics/index.html#formation-and-positioning-analysis",
    "title": "Data-Driven Tactics: Analyzing Spain’s Playing Style in Euro 2024 Qualifiers",
    "section": "3 Formation and Positioning Analysis",
    "text": "3 Formation and Positioning Analysis\n\n3.1 Formation Convex Hulls\nThe plots below show the formation at two random time points during the first half and two random time points in the second half for all games for which we have Skillcorner data. As in Shaw and Glickman (2019), the shaded regions indicate the convex hull of the players with the blue arrow pointing to the center of mass of this convex hull. This allows us to compare the relative positioning of the Spanish players across randomly selected time points stratified by possession type. The distinction between in possession and out of possession is made only when the team has been in or out of possession for at least 10 seconds, ensuring that only stable formations are included in the analysis. Finally, Spain attacks from the bottom to the top in all of the plots.\n\n\n\nIn Possession\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOut of Possession\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is evident, that whenever Spain is in possession, the team tries utilizing the full width of the pitch. This creates space and ensures that there are always passing options available for Spains infamous short passing game. Formation-wise, the 4-3-3 formation of Spain tends to persist whenever Spain is in possession. The midfielders often form a triangle to provide multiple passing options. This structure is crucial for maintaining possession and slowly advancing the ball up the pitch. In contrast, when Spain is out of possession, the team tends to adopt a more narrow and compact formation. The formation still resembles a 4-3-3, but the midfielder drop back which leads to the players being closer together. This limits the space available for the opposition allows Spain to press more effectively in the midfield. Additionally, this compact formation often forces the opposition to play on the sides of the pitch instead of through the densely-populated middle.\n\n\n3.2 Defensive Line Positioning\nThe heatmaps below display Spain’s defensive line positioning both in possession and out of possession across all matches for which we have access to Skillcorner data. .Note that contrary to above formation plots, these heatmaps include all data points for completeness i.e. there is no minimum amount of time in/ out of possession required. Again, Spain attacks from the bottom to the top in all of the plots.\n\n\n\nIn Possession\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOut of Possession\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe heatmaps show that the defensive line tends to be slightly more offensive when in possession but the difference between in and out of possession is not striking. In general, the defensive line is always relatively high up the pitch, with the most frequent positions being right in front of the middle line. This is in line with Spain’s controlling play style when in possession and high pressing when out of possession. Additionally, the defensive line tends to be higher when playing against weaker teams.\nMoreover, both wing backs tend to be positioned further forward than the center backs, as is typical in the 4-3-3 formation that Spain loves to run. This allows the wing backs to provide width and support when attacking whilst the more defensively positioned center-backs provide stability and try to always be available for a back pass when the team is under pressure or wants stabilize the game."
  },
  {
    "objectID": "posts/14-02-2026_soccer-analytics/index.html#tactical-recommendations-for-opposition-teams",
    "href": "posts/14-02-2026_soccer-analytics/index.html#tactical-recommendations-for-opposition-teams",
    "title": "Data-Driven Tactics: Analyzing Spain’s Playing Style in Euro 2024 Qualifiers",
    "section": "4 Tactical Recommendations for Opposition Teams",
    "text": "4 Tactical Recommendations for Opposition Teams\nBased on our comprehensive analysis of Spain’s playing style, we can identify several tactical vulnerabilities that opposition teams might exploit:\n\n4.1 Exploiting the Offside Trap\nAs was detailed in the offside analysis section, Spanish players frequently get caught offside. Hence, Albania should maintain a well-organized defensive line that moves up as a unit to catch Spanish players offside. Particularly, Albania could try to exploit the fact that Carvajal’s through ball attempts to Álvaro Morata often result in offside positions by tracking Morata and stepping up as a unit when Carvajal is about to play the ball, effectively creating an offside trap. The resulting free-kick could then be used to quickly launch a counter-attack exploiting Spain’s high positioning.\n\n\n4.2 Pressing the Forwards\nThe passing analysis showed us that the forwards are particularly susceptible to making inaccurate passes when under pressure. Moreover, we observed that in Spain’s game development style, the strikers often drop back to receive the ball and then distribute it back to the midfield or to the flanks. This suggests pressuring the forwards whenever possible, without compromising our own defensive line too much, to provoke inaccurate passes. Albania could implement this by having one of the center backs remain as attentive as possible while maintaining close one-to-one marking of the central forward.\n\n\n4.3 Exploiting the High Defensive Line\nAs seen in our defensive line analysis, Spain employs a high defensive line as a consequence of their possession-based playstyle, which renders them vulnerable to quick counter-attacks. Hence, Albania should occasionally try to exploit this high defensive line by attempting long balls behind the Spanish defense, even if this might cost Albania possession more often than not."
  },
  {
    "objectID": "posts/23-11-2025_covariance-shrinkage/index.html",
    "href": "posts/23-11-2025_covariance-shrinkage/index.html",
    "title": "Covariance Shrinkage: From Linear Shrinkage to Random Matrix Theory",
    "section": "",
    "text": "The estimation of the covariance matrix \\(\\Sigma \\in \\mathbb{R}^{N \\times N}\\) is a fundamental problem in multivariate statistics, essential for portfolio optimization, risk management, and dimensionality reduction. The standard estimator, the sample covariance matrix \\(S\\), is unbiased and converges to \\(\\Sigma\\) as the sample size \\(T \\to \\infty\\) while \\(N\\) remains fixed. However, in modern applications such as genomics and finance, we often face the “large \\(N\\), small \\(T\\)” regime, or more generally, the asymptotic regime where both \\(N, T \\to \\infty\\) such that \\(N/T \\to q \\in (0, \\infty)\\).\nIn this regime, \\(S\\) becomes ill-conditioned or singular (if \\(N &gt; T\\)). Even when invertible, its eigenvalues are systematically distorted: small eigenvalues are underestimated, and large eigenvalues are overestimated. This phenomenon is rigorously described by Random Matrix Theory (RMT).\nThis post explores three major approaches to mitigate these issues. Linear shrinkage (Ledoit and Wolf 2004) pulls the sample covariance towards a structured target to reduce estimation variance at the cost of some bias. Spectral analysis via the Marchenko-Pastur law (Marchenko and Pastur 1967) characterizes the limiting distribution of eigenvalues to understand the nature and magnitude of the distortion. Nonlinear shrinkage (Ledoit and Wolf 2022) then uses this spectral understanding to optimally correct each eigenvalue individually, recovering the population spectrum far more accurately than any linear method can."
  },
  {
    "objectID": "posts/23-11-2025_covariance-shrinkage/index.html#introduction",
    "href": "posts/23-11-2025_covariance-shrinkage/index.html#introduction",
    "title": "Covariance Shrinkage: From Linear Shrinkage to Random Matrix Theory",
    "section": "",
    "text": "The estimation of the covariance matrix \\(\\Sigma \\in \\mathbb{R}^{N \\times N}\\) is a fundamental problem in multivariate statistics, essential for portfolio optimization, risk management, and dimensionality reduction. The standard estimator, the sample covariance matrix \\(S\\), is unbiased and converges to \\(\\Sigma\\) as the sample size \\(T \\to \\infty\\) while \\(N\\) remains fixed. However, in modern applications such as genomics and finance, we often face the “large \\(N\\), small \\(T\\)” regime, or more generally, the asymptotic regime where both \\(N, T \\to \\infty\\) such that \\(N/T \\to q \\in (0, \\infty)\\).\nIn this regime, \\(S\\) becomes ill-conditioned or singular (if \\(N &gt; T\\)). Even when invertible, its eigenvalues are systematically distorted: small eigenvalues are underestimated, and large eigenvalues are overestimated. This phenomenon is rigorously described by Random Matrix Theory (RMT).\nThis post explores three major approaches to mitigate these issues. Linear shrinkage (Ledoit and Wolf 2004) pulls the sample covariance towards a structured target to reduce estimation variance at the cost of some bias. Spectral analysis via the Marchenko-Pastur law (Marchenko and Pastur 1967) characterizes the limiting distribution of eigenvalues to understand the nature and magnitude of the distortion. Nonlinear shrinkage (Ledoit and Wolf 2022) then uses this spectral understanding to optimally correct each eigenvalue individually, recovering the population spectrum far more accurately than any linear method can."
  },
  {
    "objectID": "posts/23-11-2025_covariance-shrinkage/index.html#linear-shrinkage-estimation",
    "href": "posts/23-11-2025_covariance-shrinkage/index.html#linear-shrinkage-estimation",
    "title": "Covariance Shrinkage: From Linear Shrinkage to Random Matrix Theory",
    "section": "2 Linear Shrinkage Estimation",
    "text": "2 Linear Shrinkage Estimation\nLet \\(X\\) be a \\(T \\times N\\) matrix of i.i.d. observations with mean zero and covariance \\(\\Sigma\\). The sample covariance matrix is \\(S = \\frac{1}{T} X^\\top X\\). While \\(\\mathbb{E}[S] = \\Sigma\\), the variance of the entries of \\(S\\) can be large when \\(T\\) is not sufficiently larger than \\(N\\). Linear shrinkage proposes an estimator \\(\\hat{\\Sigma}_{shrink}\\) that is a convex combination of the sample covariance \\(S\\) and a highly structured target estimator \\(F\\) (e.g., the identity matrix scaled by average variance):\n\\[\n\\hat{\\Sigma}_{shrink} = \\delta F + (1 - \\delta) S, \\quad \\delta \\in [0, 1]\n\\]\nThe shrinkage intensity \\(\\delta\\) controls the balance between the two components. The sample covariance \\(S\\) has zero bias but high variance, while the target \\(F\\) has high bias but low variance (it has few parameters). The shrinkage estimator \\(\\hat{\\Sigma}_{shrink}\\) balances bias and variance to minimize the Mean Squared Error.\nLedoit and Wolf (2004) derived the optimal \\(\\delta^*\\) that minimizes the expected Frobenius norm of the error:\n\\[\n\\delta^* = \\frac{\\mathbb{E}[\\| S - \\Sigma \\|_F^2]}{\\mathbb{E}[\\| S - F \\|_F^2]}\n\\]\nIntuitively, this ratio represents the variance of the sample covariance divided by the total mean squared error of the sample covariance relative to the target. While this formula depends on the unknown true covariance \\(\\Sigma\\), Ledoit and Wolf (2004) derived consistent estimators for the numerator and denominator using only the sample data \\(X\\). By estimating the asymptotic variance of the entries of \\(S\\), we can compute a practical \\(\\hat{\\delta}^*\\) without ever knowing \\(\\Sigma\\).\n\n\nCode\ndef ledoit_wolf_shrinkage(X):\n    \"\"\"\n    Computes the Ledoit-Wolf shrinkage estimator towards the identity matrix.\n    \"\"\"\n    T, N = X.shape\n    S = np.cov(X, rowvar=False)\n\n    # Target F: Identity scaled by average variance\n    mu = np.trace(S) / N\n    F = mu * np.eye(N)\n\n    X_c = X - X.mean(axis=0)\n    cov = S\n\n    # d2: squared Frobenius distance between S and F\n    d2 = np.sum((cov - F)**2)\n\n    # b2: estimated asymptotic variance of S\n    b2 = 0\n    for i in range(T):\n        x_i = X_c[i, :].reshape(-1, 1)\n        diff = x_i @ x_i.T - cov\n        b2 += np.sum(diff**2)\n\n    b2 = b2 / (T**2)\n    b2 = min(b2, d2)\n\n    delta = b2 / d2\n    shrunk_cov = delta * F + (1 - delta) * cov\n\n    return shrunk_cov, delta\n\n\nTo visualize what linear shrinkage does, the heatmaps below compare the sample covariance and the shrunk covariance for a small banded covariance structure with \\(N = 10\\) and \\(T = 20\\). The sample covariance exhibits noisy off-diagonal elements, while the shrunk version dampens this noise and preserves the underlying structure.\n\n\nCode\nN_small = 10\nT_small = 20\nSigma_small = np.zeros((N_small, N_small))\nfor i in range(N_small):\n    for j in range(N_small):\n        if abs(i-j) &lt; 3:\n            Sigma_small[i, j] = 0.8**abs(i-j)\n\nX_small = np.random.multivariate_normal(np.zeros(N_small), Sigma_small, T_small)\nS_small = np.cov(X_small, rowvar=False)\nS_lw_small, delta_small = ledoit_wolf_shrinkage(X_small)\n\ndef melt_matrix(M, name):\n    df = pd.DataFrame(M)\n    df['Row'] = range(M.shape[0])\n    df = df.melt(id_vars='Row', var_name='Col', value_name='Value')\n    df['Type'] = name\n    return df\n\ndf_heat = pd.concat([\n    melt_matrix(S_small, \"Sample Covariance\"),\n    melt_matrix(S_lw_small, f\"Shrunk (δ = {delta_small:.2f})\")\n])\n\ndisplay(\n    ggplot(df_heat, aes(x='Col', y='Row', fill='Value'))\n    + geom_tile()\n    + facet_wrap('~Type')\n    + scale_fill_cmap(name='viridis')\n    + scale_y_reverse()\n    + labs(x=\"\", y=\"\")\n    + THEME_ACADEMIC\n    + theme(\n        figure_size=(7, 3),\n        strip_text=element_text(size=10, weight=\"bold\"),\n    )\n)\n\n\n/tmp/ipykernel_362154/246008557.py:9: RuntimeWarning: covariance is not symmetric positive-semidefinite."
  },
  {
    "objectID": "posts/23-11-2025_covariance-shrinkage/index.html#the-marchenko-pastur-law",
    "href": "posts/23-11-2025_covariance-shrinkage/index.html#the-marchenko-pastur-law",
    "title": "Covariance Shrinkage: From Linear Shrinkage to Random Matrix Theory",
    "section": "3 The Marchenko-Pastur Law",
    "text": "3 The Marchenko-Pastur Law\nWhen \\(N\\) and \\(T\\) are both large, the eigenvalues of the sample covariance matrix \\(S\\) of a random matrix \\(X\\) with i.i.d. entries (mean 0, variance \\(\\sigma^2\\)) are not concentrated around \\(\\sigma^2\\). Instead, they spread out according to a deterministic probability density function known as the Marchenko-Pastur law (Marchenko and Pastur 1967).\nLet \\(q = T/N\\). If \\(q \\ge 1\\), the eigenvalues \\(\\lambda\\) of \\(S\\) are distributed on the interval \\([\\lambda_-, \\lambda_+]\\) with density:\n\\[\nf(\\lambda) = \\frac{T}{N} \\frac{\\sqrt{(\\lambda_+ - \\lambda)(\\lambda - \\lambda_-)}}{2\\pi \\sigma^2 \\lambda} \\mathbb{1}_{[\\lambda_-, \\lambda_+]}\n\\]\nwhere the spectral bounds are:\n\\[\n\\lambda_{\\pm} = \\sigma^2 \\left( 1 \\pm \\sqrt{\\frac{N}{T}} \\right)^2\n\\]\nIf \\(N &gt; T\\) (\\(q &lt; 1\\)), there is an additional point mass at zero of weight \\(1 - T/N\\). This distribution explains why sample covariance matrices appear to have “structure” (large leading eigenvalues) even when the data is pure noise.\nThe intuition is straightforward. The sample covariance matrix is constructed to fit the data \\(X\\). Even if \\(X\\) is pure noise, there will always be random directions in the high-dimensional space along which the variance of the projected data is larger than average, and others where it is smaller. When \\(N\\) is large relative to \\(T\\), the eigendecomposition has many degrees of freedom to find these spurious correlations. The largest sample eigenvalues systematically overestimate the true variance (they capture noise as signal), and the smallest eigenvalues underestimate it. This results in the spectral “smearing” that the Marchenko-Pastur law describes.\n\n\nCode\ndef marchenko_pastur_pdf(var, q, pts=1000):\n    \"\"\"Generates the Marchenko-Pastur PDF.\"\"\"\n    ratio = 1/q\n    lambda_min = var * (1 - np.sqrt(ratio))**2\n    lambda_max = var * (1 + np.sqrt(ratio))**2\n    x = np.linspace(lambda_min, lambda_max, pts)\n    y = (1 / (2 * np.pi * var * ratio * x)) * np.sqrt(\n        (lambda_max - x) * (x - lambda_min)\n    )\n    y = np.nan_to_num(y)\n    return pd.DataFrame({'x': x, 'y': y})\n\n\nThe plot below demonstrates this for \\(N = 200\\), \\(T = 1000\\), with all true eigenvalues equal to 1. The histogram of sample eigenvalues matches the theoretical Marchenko-Pastur curve almost perfectly. Note that while every true eigenvalue is exactly 1.0, the sample eigenvalues range from roughly 0.6 to 1.5.\n\n\nCode\nN = 200\nT = 1000\nsigma_sq = 1.0\n\nX_noise = np.random.normal(0, np.sqrt(sigma_sq), (T, N))\nS_noise = np.cov(X_noise, rowvar=False)\nevals_noise = np.linalg.eigvalsh(S_noise)\nmp_dist = marchenko_pastur_pdf(sigma_sq, q=T/N)\n\ndf_evals = pd.DataFrame({'eigenvalue': evals_noise})\n\ndisplay(\n    ggplot()\n    + geom_histogram(\n        aes(x='eigenvalue', y='stat(density)'),\n        data=df_evals,\n        bins=30,\n        fill=PALETTE[1],\n        alpha=0.7,\n        color=\"white\",\n    )\n    + geom_line(\n        aes(x='x', y='y'),\n        data=mp_dist,\n        color=PALETTE[2],\n        size=1.5,\n    )\n    + labs(\n        title=\"Sample Eigenvalues vs. Marchenko-Pastur Density\",\n        x=\"Eigenvalue\",\n        y=\"Density\",\n    )\n    + THEME_ACADEMIC\n)"
  },
  {
    "objectID": "posts/23-11-2025_covariance-shrinkage/index.html#nonlinear-shrinkage-estimation",
    "href": "posts/23-11-2025_covariance-shrinkage/index.html#nonlinear-shrinkage-estimation",
    "title": "Covariance Shrinkage: From Linear Shrinkage to Random Matrix Theory",
    "section": "4 Nonlinear Shrinkage Estimation",
    "text": "4 Nonlinear Shrinkage Estimation\nLinear shrinkage is limited because it applies the same shrinkage intensity to all eigenvalues, pulling them uniformly towards the mean. But RMT tells us that the distortion is not uniform: small sample eigenvalues are biased downwards and large ones upwards, and the degree of bias depends on where the eigenvalue sits in the spectrum. A more optimal approach applies a non-linear transformation \\(\\phi(\\lambda_i)\\) to each sample eigenvalue \\(\\lambda_i\\) individually.\nLedoit and Wolf (2022) developed a method to estimate this optimal non-linear shrinkage formula. The key quantity is the Stieltjes transform \\(m(z)\\) of the limiting spectral distribution of the sample covariance matrix. For a complex number \\(z \\in \\mathbb{C}^+\\):\n\\[\nm(z) = \\int \\frac{1}{\\lambda - z} dF(\\lambda)\n\\]\nThe optimal non-linear shrinkage formula for the eigenvalues is then:\n\\[\nd_i^* = \\frac{\\lambda_i}{|1 - q^{-1} - q^{-1} \\lambda_i \\breve{m}(\\lambda_i)|^2}\n\\]\nwhere \\(\\breve{m}(\\lambda) = \\lim_{\\eta \\to 0^+} m(\\lambda + i\\eta)\\) is the boundary value of the Stieltjes transform on the real axis. The “Direct Kernel” method estimates \\(\\breve{m}(\\lambda)\\) directly from the sample eigenvalues using a Cauchy kernel, avoiding numerical inversion of the QuEST function. In practice, the implementation proceeds in three steps. First, compute the eigendecomposition of \\(S\\) to obtain eigenvalues \\(\\lambda_i\\) and eigenvectors \\(u_i\\). Second, estimate the Stieltjes transform at each sample eigenvalue using the kernel estimator \\(\\hat{m}(\\lambda_i) = \\frac{1}{N} \\sum_j \\frac{1}{\\lambda_j - \\lambda_i - ih}\\) where \\(h\\) is a bandwidth parameter. Third, apply the correction formula to map each \\(\\lambda_i\\) to \\(d_i^*\\) and reconstruct the estimator as \\(\\hat{\\Sigma}_{nl} = \\sum_{i=1}^N d_i^* u_i u_i^\\top\\).\n\n\nCode\ndef nonlinear_shrinkage(X):\n    \"\"\"\n    Computes the Nonlinear Shrinkage estimator (Direct Kernel method).\n    Reference: Ledoit & Wolf (2020).\n    \"\"\"\n    T, N = X.shape\n    S = np.cov(X, rowvar=False)\n    evals, evecs = np.linalg.eigh(S)\n\n    idx = evals.argsort()\n    evals = evals[idx]\n    evecs = evecs[:, idx]\n\n    # Bandwidth for the Cauchy kernel\n    h = T**(-0.35)\n\n    # Stieltjes transform via Cauchy kernel\n    z = evals + 1j * h\n    diff = evals.reshape(1, -1) - z.reshape(-1, 1)\n    m_hat = np.mean(1 / diff, axis=1)\n\n    # Apply the nonlinear shrinkage formula\n    c = N / T\n    denom = np.abs(1 - c - c * evals * m_hat)**2\n    d_star = evals / denom\n\n    # Reconstruct\n    Sigma_nonlinear = evecs @ np.diag(d_star) @ evecs.T\n\n    return Sigma_nonlinear, d_star\n\n\nThe core of nonlinear shrinkage is the mapping from sample eigenvalues \\(\\lambda_i\\) to corrected population eigenvalues \\(d_i^*\\). The plot below shows this mapping for \\(N = 500\\), \\(T = 1000\\), with the identity as the true covariance. The characteristic “S-shape” is clearly visible: small eigenvalues are pulled up (above the dashed 45-degree line) and large eigenvalues are pulled down (below it), counteracting the spectral smearing effect predicted by the Marchenko-Pastur law.\n\n\nCode\nN_viz = 500\nT_viz = 1000\nX_viz = np.random.normal(0, 1, (T_viz, N_viz))\n_, d_star_viz = nonlinear_shrinkage(X_viz)\nS_viz = np.cov(X_viz, rowvar=False)\nevals_viz = np.linalg.eigvalsh(S_viz)\n\ndf_shrinkage = pd.DataFrame({\n    'Sample Eigenvalue': evals_viz,\n    'Shrunk Eigenvalue': d_star_viz,\n})\n\nline_df = pd.DataFrame({\n    'x': [0, max(evals_viz)],\n    'y': [0, max(evals_viz)],\n})\n\ndisplay(\n    ggplot(df_shrinkage, aes(x='Sample Eigenvalue', y='Shrunk Eigenvalue'))\n    + geom_point(color=PALETTE[0], alpha=0.3, size=0.8)\n    + geom_line(\n        aes(x='x', y='y'),\n        data=line_df,\n        linetype='dashed',\n        color='black',\n        size=0.8,\n    )\n    + labs(\n        title=\"Nonlinear Shrinkage Function\",\n        x=\"Sample Eigenvalue\",\n        y=\"Shrunk Eigenvalue\",\n    )\n    + THEME_ACADEMIC\n)"
  },
  {
    "objectID": "posts/23-11-2025_covariance-shrinkage/index.html#simulation-study",
    "href": "posts/23-11-2025_covariance-shrinkage/index.html#simulation-study",
    "title": "Covariance Shrinkage: From Linear Shrinkage to Random Matrix Theory",
    "section": "5 Simulation Study",
    "text": "5 Simulation Study\nWe now simulate two scenarios to validate the methods. The first uses pure noise to confirm the Marchenko-Pastur fit (shown above). The second uses a structured covariance matrix to compare the estimation accuracy of all three approaches as the sample size \\(T\\) increases.\nWe define the normalized Frobenius loss \\(\\text{Loss}(\\hat{\\Sigma}, \\Sigma) = \\| \\hat{\\Sigma} - \\Sigma \\|_F / \\| \\Sigma \\|_F\\) and generate data from an AR(1) Toeplitz covariance matrix with \\(N = 100\\) and correlation parameter \\(\\rho = 0.5\\). For each value of \\(T\\), we run 20 trials and report the mean loss.\n\n\nCode\ndef generate_structured_covariance(N, rho=0.7):\n    \"\"\"Generates a Toeplitz covariance matrix (AR(1) process).\"\"\"\n    return np.array([[rho**abs(i - j) for j in range(N)] for i in range(N)])\n\nN = 100\nT_values = [50, 100, 200, 500, 1000]\nrho = 0.5\nSigma_true = generate_structured_covariance(N, rho)\n\nresults = []\nfor t in T_values:\n    for trial in range(20):\n        Z = np.random.normal(0, 1, (t, N))\n        L = np.linalg.cholesky(Sigma_true)\n        X = Z @ L.T\n\n        S = np.cov(X, rowvar=False)\n        loss_sample = np.linalg.norm(S - Sigma_true, 'fro') / np.linalg.norm(Sigma_true, 'fro')\n\n        S_lw, _ = ledoit_wolf_shrinkage(X)\n        loss_lw = np.linalg.norm(S_lw - Sigma_true, 'fro') / np.linalg.norm(Sigma_true, 'fro')\n\n        S_nl, _ = nonlinear_shrinkage(X)\n        loss_nl = np.linalg.norm(S_nl - Sigma_true, 'fro') / np.linalg.norm(Sigma_true, 'fro')\n\n        results.append({'T': t, 'Method': 'Sample Covariance', 'Loss': loss_sample})\n        results.append({'T': t, 'Method': 'Linear Shrinkage', 'Loss': loss_lw})\n        results.append({'T': t, 'Method': 'Nonlinear Shrinkage', 'Loss': loss_nl})\n\ndf_results = pd.DataFrame(results)\ndf_summary = df_results.groupby(['T', 'Method'])['Loss'].mean().reset_index()\n\ndisplay(\n    ggplot(df_summary, aes(x='T', y='Loss', color='Method', group='Method'))\n    + geom_line(size=1.2)\n    + geom_point(size=2.5, fill=\"white\", stroke=1.2)\n    + scale_color_manual(values=[PALETTE[0], PALETTE[2], PALETTE[7]])\n    + scale_y_log10()\n    + labs(\n        title=\"Estimator Convergence (N = 100)\",\n        x=\"Sample Size (T)\",\n        y=\"Normalized Frobenius Loss\",\n    )\n    + THEME_ACADEMIC\n)\n\n\n\n\n\n\n\n\n\nAll three estimators converge as \\(T\\) increases, but the rates differ substantially. In the most challenging regime (\\(T = 50\\), so \\(N/T = 2\\)), the sample covariance is by far the worst. Linear shrinkage provides a large improvement by pulling the estimate towards the identity, and nonlinear shrinkage improves further by adapting to the specific spectral shape of the Toeplitz structure. As \\(T\\) grows, the gap between the methods narrows because the sample covariance itself becomes increasingly accurate.\nTo visualize the eigenvalue-level correction, we use a spiked covariance model where 10 eigenvalues equal 10 and the remaining 190 equal 1. The scree plot below compares the true population spectrum, the noisy sample eigenvalues, and the nonlinearly shrunk eigenvalues.\n\n\nCode\nN = 200\nT = 400\nn_spikes = 10\nspike_val = 10.0\n\nSigma_spiked = np.eye(N)\nSigma_spiked[:n_spikes, :n_spikes] = spike_val * np.eye(n_spikes)\n\nZ = np.random.normal(0, 1, (T, N))\nL = np.linalg.cholesky(Sigma_spiked)\nX_spiked = Z @ L.T\n\nS_spiked = np.cov(X_spiked, rowvar=False)\nS_nl_spiked, evals_nl = nonlinear_shrinkage(X_spiked)\n\nevals_sample = np.sort(np.linalg.eigvalsh(S_spiked))[::-1]\nevals_true = np.sort(np.linalg.eigvalsh(Sigma_spiked))[::-1]\nevals_nl = np.sort(evals_nl)[::-1]\n\ndf_scree = pd.DataFrame({\n    'Rank': np.tile(np.arange(1, N+1), 3),\n    'Eigenvalue': np.concatenate([evals_true, evals_sample, evals_nl]),\n    'Type': ['True Population'] * N + ['Sample (Noisy)'] * N + ['Nonlinear Shrinkage'] * N,\n})\n\ndisplay(\n    ggplot(df_scree, aes(x='Rank', y='Eigenvalue', color='Type'))\n    + geom_line(size=1)\n    + scale_y_log10()\n    + scale_color_manual(values=[PALETTE[7], PALETTE[0], PALETTE[2]])\n    + labs(\n        title=\"Scree Plot: Spiked Covariance Model\",\n        x=\"Eigenvalue Rank\",\n        y=\"Eigenvalue (Log Scale)\",\n    )\n    + THEME_ACADEMIC\n)\n\n\n\n\n\n\n\n\n\nThe sample eigenvalues overestimate the signal spikes and underestimate the noise floor. The nonlinear shrinkage successfully pulls the large eigenvalues down and the small eigenvalues up, recovering the population spectrum far more accurately."
  },
  {
    "objectID": "posts/23-11-2025_covariance-shrinkage/index.html#conclusion",
    "href": "posts/23-11-2025_covariance-shrinkage/index.html#conclusion",
    "title": "Covariance Shrinkage: From Linear Shrinkage to Random Matrix Theory",
    "section": "6 Conclusion",
    "text": "6 Conclusion\nIn high-dimensional statistics, the sample covariance matrix is untrustworthy due to the “curse of dimensionality,” which manifests as a deterministic spreading of eigenvalues described by the Marchenko-Pastur law. Linear shrinkage (Ledoit and Wolf 2004) offers a robust improvement by pulling the matrix towards a stable target, effectively reducing variance at the cost of some bias. Nonlinear shrinkage (Ledoit and Wolf 2022) takes this a step further by using Random Matrix Theory to optimally correct individual eigenvalues. It effectively inverts the Marchenko-Pastur equation to recover the population spectrum.\nFor practitioners, the choice between methods depends on the dimensionality ratio \\(N/T\\). When this ratio is small (below roughly 0.1), the sample covariance is generally sufficient. In the moderate regime, linear shrinkage is a robust and easy-to-implement baseline that has become standard in finance. When \\(N/T\\) is large but below 1, nonlinear shrinkage provides significant gains by correcting the specific spectral distortion predicted by RMT, making it ideal for high-dimensional portfolios. In the singular regime (\\(N &gt; T\\)), shrinkage is strictly necessary. Nonlinear shrinkage remains preferred when \\(T\\) is reasonably large (say, above 100), but for extremely small samples the estimation of the spectral density becomes noisy and linear shrinkage towards a structured target is often safer."
  }
]