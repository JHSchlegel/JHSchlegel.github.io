<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jan Schlegel">
<meta name="dcterms.date" content="2025-11-23">

<title>Jan Schlegel – Covariance Shrinkage: From Linear Shrinkage to Random Matrix Theory</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../logo.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<meta name="color-scheme" content="dark light">
<meta name="quarto:status" content="draft">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner"><div id="quarto-draft-alert" class="alert alert-warning"><i class="bi bi-pencil-square"></i>Draft</div>
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Jan Schlegel</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/jan-heinrich-schlegel/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/JHSchlegel"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Covariance Shrinkage: From Linear Shrinkage to Random Matrix Theory</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Quantitative Finance</div>
                <div class="quarto-category">Python</div>
                <div class="quarto-category">High-Dimensional Statistics</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Jan Schlegel </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 23, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  <div>
    <div class="abstract">
      <div class="block-title">Abstract</div>
      In high-dimensional regimes where the number of variables <span class="math inline">\(N\)</span> is comparable to the sample size <span class="math inline">\(T\)</span>, the sample covariance matrix is known to be an ill-conditioned and noisy estimator of the population covariance. This post provides a rigorous mathematical treatment of covariance shrinkage, exploring the bias-variance tradeoff inherent in linear shrinkage estimators (Ledoit-Wolf). We further ground these methods in Random Matrix Theory, specifically the Marchenko-Pastur law, to characterize the asymptotic behavior of eigenvalues. Finally, we introduce and implement Nonlinear Shrinkage (Ledoit-Wolf 2020), which applies a non-linear transformation to the sample eigenvalues based on the estimation of the Stieltjes transform. Simulation studies demonstrate the efficacy of these methods in recovering true spectral properties.
    </div>
  </div>
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#linear-shrinkage-estimation" id="toc-linear-shrinkage-estimation" class="nav-link" data-scroll-target="#linear-shrinkage-estimation"><span class="header-section-number">2</span> Linear Shrinkage Estimation</a></li>
  <li><a href="#the-marchenko-pastur-law" id="toc-the-marchenko-pastur-law" class="nav-link" data-scroll-target="#the-marchenko-pastur-law"><span class="header-section-number">3</span> The Marchenko-Pastur Law</a></li>
  <li><a href="#nonlinear-shrinkage-estimation" id="toc-nonlinear-shrinkage-estimation" class="nav-link" data-scroll-target="#nonlinear-shrinkage-estimation"><span class="header-section-number">4</span> Nonlinear Shrinkage Estimation</a></li>
  <li><a href="#simulation-study" id="toc-simulation-study" class="nav-link" data-scroll-target="#simulation-study"><span class="header-section-number">5</span> Simulation Study</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">6</span> Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<p>The estimation of the covariance matrix <span class="math inline">\(\Sigma \in \mathbb{R}^{N \times N}\)</span> is a fundamental problem in multivariate statistics, essential for portfolio optimization, risk management, and dimensionality reduction. The standard estimator, the sample covariance matrix <span class="math inline">\(S\)</span>, is unbiased and converges to <span class="math inline">\(\Sigma\)</span> as the sample size <span class="math inline">\(T \to \infty\)</span> while <span class="math inline">\(N\)</span> remains fixed. However, in modern applications such as genomics and finance, we often face the “large <span class="math inline">\(N\)</span>, small <span class="math inline">\(T\)</span>” regime, or more generally, the asymptotic regime where both <span class="math inline">\(N, T \to \infty\)</span> such that <span class="math inline">\(N/T \to q \in (0, \infty)\)</span>.</p>
<p>In this regime, <span class="math inline">\(S\)</span> becomes ill-conditioned or singular (if <span class="math inline">\(N &gt; T\)</span>). Even when invertible, its eigenvalues are systematically distorted: small eigenvalues are underestimated, and large eigenvalues are overestimated. This phenomenon is rigorously described by Random Matrix Theory (RMT).</p>
<p>This post explores three major approaches to mitigate these issues. Linear shrinkage <span class="citation" data-cites="ledoit2004well">(<a href="#ref-ledoit2004well" role="doc-biblioref">Ledoit and Wolf 2004</a>)</span> pulls the sample covariance towards a structured target to reduce estimation variance at the cost of some bias. Spectral analysis via the Marchenko-Pastur law <span class="citation" data-cites="marchenko1967distribution">(<a href="#ref-marchenko1967distribution" role="doc-biblioref">Marchenko and Pastur 1967</a>)</span> characterizes the limiting distribution of eigenvalues to understand the nature and magnitude of the distortion. Nonlinear shrinkage <span class="citation" data-cites="ledoit2020power">(<a href="#ref-ledoit2020power" role="doc-biblioref">Ledoit and Wolf 2022</a>)</span> then uses this spectral understanding to optimally correct each eigenvalue individually, recovering the population spectrum far more accurately than any linear method can.</p>
</section>
<section id="linear-shrinkage-estimation" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="linear-shrinkage-estimation"><span class="header-section-number">2</span> Linear Shrinkage Estimation</h2>
<p>Let <span class="math inline">\(X\)</span> be a <span class="math inline">\(T \times N\)</span> matrix of i.i.d. observations with mean zero and covariance <span class="math inline">\(\Sigma\)</span>. The sample covariance matrix is <span class="math inline">\(S = \frac{1}{T} X^\top X\)</span>. While <span class="math inline">\(\mathbb{E}[S] = \Sigma\)</span>, the variance of the entries of <span class="math inline">\(S\)</span> can be large when <span class="math inline">\(T\)</span> is not sufficiently larger than <span class="math inline">\(N\)</span>. Linear shrinkage proposes an estimator <span class="math inline">\(\hat{\Sigma}_{shrink}\)</span> that is a convex combination of the sample covariance <span class="math inline">\(S\)</span> and a highly structured target estimator <span class="math inline">\(F\)</span> (e.g., the identity matrix scaled by average variance):</p>
<p><span class="math display">\[
\hat{\Sigma}_{shrink} = \delta F + (1 - \delta) S, \quad \delta \in [0, 1]
\]</span></p>
<p>The shrinkage intensity <span class="math inline">\(\delta\)</span> controls the balance between the two components. The sample covariance <span class="math inline">\(S\)</span> has zero bias but high variance, while the target <span class="math inline">\(F\)</span> has high bias but low variance (it has few parameters). The shrinkage estimator <span class="math inline">\(\hat{\Sigma}_{shrink}\)</span> balances bias and variance to minimize the Mean Squared Error.</p>
<p><span class="citation" data-cites="ledoit2004well">Ledoit and Wolf (<a href="#ref-ledoit2004well" role="doc-biblioref">2004</a>)</span> derived the optimal <span class="math inline">\(\delta^*\)</span> that minimizes the expected Frobenius norm of the error:</p>
<p><span class="math display">\[
\delta^* = \frac{\mathbb{E}[\| S - \Sigma \|_F^2]}{\mathbb{E}[\| S - F \|_F^2]}
\]</span></p>
<p>Intuitively, this ratio represents the variance of the sample covariance divided by the total mean squared error of the sample covariance relative to the target. While this formula depends on the unknown true covariance <span class="math inline">\(\Sigma\)</span>, <span class="citation" data-cites="ledoit2004well">Ledoit and Wolf (<a href="#ref-ledoit2004well" role="doc-biblioref">2004</a>)</span> derived consistent estimators for the numerator and denominator using only the sample data <span class="math inline">\(X\)</span>. By estimating the asymptotic variance of the entries of <span class="math inline">\(S\)</span>, we can compute a practical <span class="math inline">\(\hat{\delta}^*\)</span> without ever knowing <span class="math inline">\(\Sigma\)</span>.</p>
<div id="508af644" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ledoit_wolf_shrinkage(X):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Computes the Ledoit-Wolf shrinkage estimator towards the identity matrix.</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    T, N <span class="op">=</span> X.shape</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    S <span class="op">=</span> np.cov(X, rowvar<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Target F: Identity scaled by average variance</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> np.trace(S) <span class="op">/</span> N</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    F <span class="op">=</span> mu <span class="op">*</span> np.eye(N)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    X_c <span class="op">=</span> X <span class="op">-</span> X.mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    cov <span class="op">=</span> S</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># d2: squared Frobenius distance between S and F</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    d2 <span class="op">=</span> np.<span class="bu">sum</span>((cov <span class="op">-</span> F)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># b2: estimated asymptotic variance of S</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    b2 <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(T):</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        x_i <span class="op">=</span> X_c[i, :].reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        diff <span class="op">=</span> x_i <span class="op">@</span> x_i.T <span class="op">-</span> cov</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        b2 <span class="op">+=</span> np.<span class="bu">sum</span>(diff<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    b2 <span class="op">=</span> b2 <span class="op">/</span> (T<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    b2 <span class="op">=</span> <span class="bu">min</span>(b2, d2)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    delta <span class="op">=</span> b2 <span class="op">/</span> d2</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    shrunk_cov <span class="op">=</span> delta <span class="op">*</span> F <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> delta) <span class="op">*</span> cov</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> shrunk_cov, delta</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>To visualize what linear shrinkage does, the heatmaps below compare the sample covariance and the shrunk covariance for a small banded covariance structure with <span class="math inline">\(N = 10\)</span> and <span class="math inline">\(T = 20\)</span>. The sample covariance exhibits noisy off-diagonal elements, while the shrunk version dampens this noise and preserves the underlying structure.</p>
<div id="04848e6f" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>N_small <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>T_small <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>Sigma_small <span class="op">=</span> np.zeros((N_small, N_small))</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N_small):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(N_small):</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">abs</span>(i<span class="op">-</span>j) <span class="op">&lt;</span> <span class="dv">3</span>:</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>            Sigma_small[i, j] <span class="op">=</span> <span class="fl">0.8</span><span class="op">**</span><span class="bu">abs</span>(i<span class="op">-</span>j)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>X_small <span class="op">=</span> np.random.multivariate_normal(np.zeros(N_small), Sigma_small, T_small)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>S_small <span class="op">=</span> np.cov(X_small, rowvar<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>S_lw_small, delta_small <span class="op">=</span> ledoit_wolf_shrinkage(X_small)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> melt_matrix(M, name):</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.DataFrame(M)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'Row'</span>] <span class="op">=</span> <span class="bu">range</span>(M.shape[<span class="dv">0</span>])</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.melt(id_vars<span class="op">=</span><span class="st">'Row'</span>, var_name<span class="op">=</span><span class="st">'Col'</span>, value_name<span class="op">=</span><span class="st">'Value'</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'Type'</span>] <span class="op">=</span> name</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>df_heat <span class="op">=</span> pd.concat([</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    melt_matrix(S_small, <span class="st">"Sample Covariance"</span>),</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    melt_matrix(S_lw_small, <span class="ss">f"Shrunk (δ = </span><span class="sc">{</span>delta_small<span class="sc">:.2f}</span><span class="ss">)"</span>)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>display(</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    ggplot(df_heat, aes(x<span class="op">=</span><span class="st">'Col'</span>, y<span class="op">=</span><span class="st">'Row'</span>, fill<span class="op">=</span><span class="st">'Value'</span>))</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> geom_tile()</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> facet_wrap(<span class="st">'~Type'</span>)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> scale_fill_cmap(name<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> scale_y_reverse()</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> labs(x<span class="op">=</span><span class="st">""</span>, y<span class="op">=</span><span class="st">""</span>)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> THEME_ACADEMIC</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> theme(</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>        figure_size<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">3</span>),</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        strip_text<span class="op">=</span>element_text(size<span class="op">=</span><span class="dv">10</span>, weight<span class="op">=</span><span class="st">"bold"</span>),</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_362154/246008557.py:9: RuntimeWarning: covariance is not symmetric positive-semidefinite.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-4-output-2.png" class="quarto-figure quarto-figure-center figure-img" width="672" height="288"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="the-marchenko-pastur-law" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="the-marchenko-pastur-law"><span class="header-section-number">3</span> The Marchenko-Pastur Law</h2>
<p>When <span class="math inline">\(N\)</span> and <span class="math inline">\(T\)</span> are both large, the eigenvalues of the sample covariance matrix <span class="math inline">\(S\)</span> of a random matrix <span class="math inline">\(X\)</span> with i.i.d. entries (mean 0, variance <span class="math inline">\(\sigma^2\)</span>) are not concentrated around <span class="math inline">\(\sigma^2\)</span>. Instead, they spread out according to a deterministic probability density function known as the Marchenko-Pastur law <span class="citation" data-cites="marchenko1967distribution">(<a href="#ref-marchenko1967distribution" role="doc-biblioref">Marchenko and Pastur 1967</a>)</span>.</p>
<p>Let <span class="math inline">\(q = T/N\)</span>. If <span class="math inline">\(q \ge 1\)</span>, the eigenvalues <span class="math inline">\(\lambda\)</span> of <span class="math inline">\(S\)</span> are distributed on the interval <span class="math inline">\([\lambda_-, \lambda_+]\)</span> with density:</p>
<p><span class="math display">\[
f(\lambda) = \frac{T}{N} \frac{\sqrt{(\lambda_+ - \lambda)(\lambda - \lambda_-)}}{2\pi \sigma^2 \lambda} \mathbb{1}_{[\lambda_-, \lambda_+]}
\]</span></p>
<p>where the spectral bounds are:</p>
<p><span class="math display">\[
\lambda_{\pm} = \sigma^2 \left( 1 \pm \sqrt{\frac{N}{T}} \right)^2
\]</span></p>
<p>If <span class="math inline">\(N &gt; T\)</span> (<span class="math inline">\(q &lt; 1\)</span>), there is an additional point mass at zero of weight <span class="math inline">\(1 - T/N\)</span>. This distribution explains why sample covariance matrices appear to have “structure” (large leading eigenvalues) even when the data is pure noise.</p>
<p>The intuition is straightforward. The sample covariance matrix is constructed to fit the data <span class="math inline">\(X\)</span>. Even if <span class="math inline">\(X\)</span> is pure noise, there will always be random directions in the high-dimensional space along which the variance of the projected data is larger than average, and others where it is smaller. When <span class="math inline">\(N\)</span> is large relative to <span class="math inline">\(T\)</span>, the eigendecomposition has many degrees of freedom to find these spurious correlations. The largest sample eigenvalues systematically overestimate the true variance (they capture noise as signal), and the smallest eigenvalues underestimate it. This results in the spectral “smearing” that the Marchenko-Pastur law describes.</p>
<div id="996311b4" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> marchenko_pastur_pdf(var, q, pts<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Generates the Marchenko-Pastur PDF."""</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    ratio <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>q</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    lambda_min <span class="op">=</span> var <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> np.sqrt(ratio))<span class="op">**</span><span class="dv">2</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    lambda_max <span class="op">=</span> var <span class="op">*</span> (<span class="dv">1</span> <span class="op">+</span> np.sqrt(ratio))<span class="op">**</span><span class="dv">2</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.linspace(lambda_min, lambda_max, pts)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> (<span class="dv">1</span> <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">*</span> var <span class="op">*</span> ratio <span class="op">*</span> x)) <span class="op">*</span> np.sqrt(</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        (lambda_max <span class="op">-</span> x) <span class="op">*</span> (x <span class="op">-</span> lambda_min)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> np.nan_to_num(y)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.DataFrame({<span class="st">'x'</span>: x, <span class="st">'y'</span>: y})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The plot below demonstrates this for <span class="math inline">\(N = 200\)</span>, <span class="math inline">\(T = 1000\)</span>, with all true eigenvalues equal to 1. The histogram of sample eigenvalues matches the theoretical Marchenko-Pastur curve almost perfectly. Note that while every true eigenvalue is exactly 1.0, the sample eigenvalues range from roughly 0.6 to 1.5.</p>
<div id="3d68a45d" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>sigma_sq <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>X_noise <span class="op">=</span> np.random.normal(<span class="dv">0</span>, np.sqrt(sigma_sq), (T, N))</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>S_noise <span class="op">=</span> np.cov(X_noise, rowvar<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>evals_noise <span class="op">=</span> np.linalg.eigvalsh(S_noise)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>mp_dist <span class="op">=</span> marchenko_pastur_pdf(sigma_sq, q<span class="op">=</span>T<span class="op">/</span>N)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>df_evals <span class="op">=</span> pd.DataFrame({<span class="st">'eigenvalue'</span>: evals_noise})</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>display(</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    ggplot()</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> geom_histogram(</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        aes(x<span class="op">=</span><span class="st">'eigenvalue'</span>, y<span class="op">=</span><span class="st">'stat(density)'</span>),</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        data<span class="op">=</span>df_evals,</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        bins<span class="op">=</span><span class="dv">30</span>,</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        fill<span class="op">=</span>PALETTE[<span class="dv">1</span>],</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        alpha<span class="op">=</span><span class="fl">0.7</span>,</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        color<span class="op">=</span><span class="st">"white"</span>,</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> geom_line(</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        aes(x<span class="op">=</span><span class="st">'x'</span>, y<span class="op">=</span><span class="st">'y'</span>),</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        data<span class="op">=</span>mp_dist,</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>        color<span class="op">=</span>PALETTE[<span class="dv">2</span>],</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        size<span class="op">=</span><span class="fl">1.5</span>,</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> labs(</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        title<span class="op">=</span><span class="st">"Sample Eigenvalues vs. Marchenko-Pastur Density"</span>,</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="st">"Eigenvalue"</span>,</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>        y<span class="op">=</span><span class="st">"Density"</span>,</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> THEME_ACADEMIC</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-6-output-1.png" class="quarto-figure quarto-figure-center figure-img" width="672" height="384"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="nonlinear-shrinkage-estimation" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="nonlinear-shrinkage-estimation"><span class="header-section-number">4</span> Nonlinear Shrinkage Estimation</h2>
<p>Linear shrinkage is limited because it applies the same shrinkage intensity to all eigenvalues, pulling them uniformly towards the mean. But RMT tells us that the distortion is not uniform: small sample eigenvalues are biased downwards and large ones upwards, and the degree of bias depends on where the eigenvalue sits in the spectrum. A more optimal approach applies a non-linear transformation <span class="math inline">\(\phi(\lambda_i)\)</span> to each sample eigenvalue <span class="math inline">\(\lambda_i\)</span> individually.</p>
<p><span class="citation" data-cites="ledoit2020power">Ledoit and Wolf (<a href="#ref-ledoit2020power" role="doc-biblioref">2022</a>)</span> developed a method to estimate this optimal non-linear shrinkage formula. The key quantity is the Stieltjes transform <span class="math inline">\(m(z)\)</span> of the limiting spectral distribution of the sample covariance matrix. For a complex number <span class="math inline">\(z \in \mathbb{C}^+\)</span>:</p>
<p><span class="math display">\[
m(z) = \int \frac{1}{\lambda - z} dF(\lambda)
\]</span></p>
<p>The optimal non-linear shrinkage formula for the eigenvalues is then:</p>
<p><span class="math display">\[
d_i^* = \frac{\lambda_i}{|1 - q^{-1} - q^{-1} \lambda_i \breve{m}(\lambda_i)|^2}
\]</span></p>
<p>where <span class="math inline">\(\breve{m}(\lambda) = \lim_{\eta \to 0^+} m(\lambda + i\eta)\)</span> is the boundary value of the Stieltjes transform on the real axis. The “Direct Kernel” method estimates <span class="math inline">\(\breve{m}(\lambda)\)</span> directly from the sample eigenvalues using a Cauchy kernel, avoiding numerical inversion of the QuEST function. In practice, the implementation proceeds in three steps. First, compute the eigendecomposition of <span class="math inline">\(S\)</span> to obtain eigenvalues <span class="math inline">\(\lambda_i\)</span> and eigenvectors <span class="math inline">\(u_i\)</span>. Second, estimate the Stieltjes transform at each sample eigenvalue using the kernel estimator <span class="math inline">\(\hat{m}(\lambda_i) = \frac{1}{N} \sum_j \frac{1}{\lambda_j - \lambda_i - ih}\)</span> where <span class="math inline">\(h\)</span> is a bandwidth parameter. Third, apply the correction formula to map each <span class="math inline">\(\lambda_i\)</span> to <span class="math inline">\(d_i^*\)</span> and reconstruct the estimator as <span class="math inline">\(\hat{\Sigma}_{nl} = \sum_{i=1}^N d_i^* u_i u_i^\top\)</span>.</p>
<div id="7fdc0377" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> nonlinear_shrinkage(X):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Computes the Nonlinear Shrinkage estimator (Direct Kernel method).</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Reference: Ledoit &amp; Wolf (2020).</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    T, N <span class="op">=</span> X.shape</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    S <span class="op">=</span> np.cov(X, rowvar<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    evals, evecs <span class="op">=</span> np.linalg.eigh(S)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> evals.argsort()</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    evals <span class="op">=</span> evals[idx]</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    evecs <span class="op">=</span> evecs[:, idx]</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bandwidth for the Cauchy kernel</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> T<span class="op">**</span>(<span class="op">-</span><span class="fl">0.35</span>)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Stieltjes transform via Cauchy kernel</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> evals <span class="op">+</span> <span class="ot">1j</span> <span class="op">*</span> h</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    diff <span class="op">=</span> evals.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>) <span class="op">-</span> z.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    m_hat <span class="op">=</span> np.mean(<span class="dv">1</span> <span class="op">/</span> diff, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply the nonlinear shrinkage formula</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    c <span class="op">=</span> N <span class="op">/</span> T</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    denom <span class="op">=</span> np.<span class="bu">abs</span>(<span class="dv">1</span> <span class="op">-</span> c <span class="op">-</span> c <span class="op">*</span> evals <span class="op">*</span> m_hat)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    d_star <span class="op">=</span> evals <span class="op">/</span> denom</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reconstruct</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    Sigma_nonlinear <span class="op">=</span> evecs <span class="op">@</span> np.diag(d_star) <span class="op">@</span> evecs.T</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Sigma_nonlinear, d_star</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The core of nonlinear shrinkage is the mapping from sample eigenvalues <span class="math inline">\(\lambda_i\)</span> to corrected population eigenvalues <span class="math inline">\(d_i^*\)</span>. The plot below shows this mapping for <span class="math inline">\(N = 500\)</span>, <span class="math inline">\(T = 1000\)</span>, with the identity as the true covariance. The characteristic “S-shape” is clearly visible: small eigenvalues are pulled up (above the dashed 45-degree line) and large eigenvalues are pulled down (below it), counteracting the spectral smearing effect predicted by the Marchenko-Pastur law.</p>
<div id="ea362fb2" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>N_viz <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>T_viz <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>X_viz <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, (T_viz, N_viz))</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>_, d_star_viz <span class="op">=</span> nonlinear_shrinkage(X_viz)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>S_viz <span class="op">=</span> np.cov(X_viz, rowvar<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>evals_viz <span class="op">=</span> np.linalg.eigvalsh(S_viz)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>df_shrinkage <span class="op">=</span> pd.DataFrame({</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Sample Eigenvalue'</span>: evals_viz,</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Shrunk Eigenvalue'</span>: d_star_viz,</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>line_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">'x'</span>: [<span class="dv">0</span>, <span class="bu">max</span>(evals_viz)],</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">'y'</span>: [<span class="dv">0</span>, <span class="bu">max</span>(evals_viz)],</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>display(</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    ggplot(df_shrinkage, aes(x<span class="op">=</span><span class="st">'Sample Eigenvalue'</span>, y<span class="op">=</span><span class="st">'Shrunk Eigenvalue'</span>))</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> geom_point(color<span class="op">=</span>PALETTE[<span class="dv">0</span>], alpha<span class="op">=</span><span class="fl">0.3</span>, size<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> geom_line(</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>        aes(x<span class="op">=</span><span class="st">'x'</span>, y<span class="op">=</span><span class="st">'y'</span>),</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>        data<span class="op">=</span>line_df,</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>        linetype<span class="op">=</span><span class="st">'dashed'</span>,</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>        color<span class="op">=</span><span class="st">'black'</span>,</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>        size<span class="op">=</span><span class="fl">0.8</span>,</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> labs(</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>        title<span class="op">=</span><span class="st">"Nonlinear Shrinkage Function"</span>,</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="st">"Sample Eigenvalue"</span>,</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>        y<span class="op">=</span><span class="st">"Shrunk Eigenvalue"</span>,</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> THEME_ACADEMIC</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-8-output-1.png" class="quarto-figure quarto-figure-center figure-img" width="672" height="384"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="simulation-study" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="simulation-study"><span class="header-section-number">5</span> Simulation Study</h2>
<p>We now simulate two scenarios to validate the methods. The first uses pure noise to confirm the Marchenko-Pastur fit (shown above). The second uses a structured covariance matrix to compare the estimation accuracy of all three approaches as the sample size <span class="math inline">\(T\)</span> increases.</p>
<p>We define the normalized Frobenius loss <span class="math inline">\(\text{Loss}(\hat{\Sigma}, \Sigma) = \| \hat{\Sigma} - \Sigma \|_F / \| \Sigma \|_F\)</span> and generate data from an AR(1) Toeplitz covariance matrix with <span class="math inline">\(N = 100\)</span> and correlation parameter <span class="math inline">\(\rho = 0.5\)</span>. For each value of <span class="math inline">\(T\)</span>, we run 20 trials and report the mean loss.</p>
<div id="78d69c1c" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_structured_covariance(N, rho<span class="op">=</span><span class="fl">0.7</span>):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Generates a Toeplitz covariance matrix (AR(1) process)."""</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array([[rho<span class="op">**</span><span class="bu">abs</span>(i <span class="op">-</span> j) <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(N)] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N)])</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>T_values <span class="op">=</span> [<span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">500</span>, <span class="dv">1000</span>]</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>rho <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>Sigma_true <span class="op">=</span> generate_structured_covariance(N, rho)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> []</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> T_values:</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> trial <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>):</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        Z <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, (t, N))</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>        L <span class="op">=</span> np.linalg.cholesky(Sigma_true)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        X <span class="op">=</span> Z <span class="op">@</span> L.T</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        S <span class="op">=</span> np.cov(X, rowvar<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>        loss_sample <span class="op">=</span> np.linalg.norm(S <span class="op">-</span> Sigma_true, <span class="st">'fro'</span>) <span class="op">/</span> np.linalg.norm(Sigma_true, <span class="st">'fro'</span>)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>        S_lw, _ <span class="op">=</span> ledoit_wolf_shrinkage(X)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>        loss_lw <span class="op">=</span> np.linalg.norm(S_lw <span class="op">-</span> Sigma_true, <span class="st">'fro'</span>) <span class="op">/</span> np.linalg.norm(Sigma_true, <span class="st">'fro'</span>)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>        S_nl, _ <span class="op">=</span> nonlinear_shrinkage(X)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>        loss_nl <span class="op">=</span> np.linalg.norm(S_nl <span class="op">-</span> Sigma_true, <span class="st">'fro'</span>) <span class="op">/</span> np.linalg.norm(Sigma_true, <span class="st">'fro'</span>)</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>        results.append({<span class="st">'T'</span>: t, <span class="st">'Method'</span>: <span class="st">'Sample Covariance'</span>, <span class="st">'Loss'</span>: loss_sample})</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>        results.append({<span class="st">'T'</span>: t, <span class="st">'Method'</span>: <span class="st">'Linear Shrinkage'</span>, <span class="st">'Loss'</span>: loss_lw})</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>        results.append({<span class="st">'T'</span>: t, <span class="st">'Method'</span>: <span class="st">'Nonlinear Shrinkage'</span>, <span class="st">'Loss'</span>: loss_nl})</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>df_results <span class="op">=</span> pd.DataFrame(results)</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>df_summary <span class="op">=</span> df_results.groupby([<span class="st">'T'</span>, <span class="st">'Method'</span>])[<span class="st">'Loss'</span>].mean().reset_index()</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>display(</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>    ggplot(df_summary, aes(x<span class="op">=</span><span class="st">'T'</span>, y<span class="op">=</span><span class="st">'Loss'</span>, color<span class="op">=</span><span class="st">'Method'</span>, group<span class="op">=</span><span class="st">'Method'</span>))</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> geom_line(size<span class="op">=</span><span class="fl">1.2</span>)</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> geom_point(size<span class="op">=</span><span class="fl">2.5</span>, fill<span class="op">=</span><span class="st">"white"</span>, stroke<span class="op">=</span><span class="fl">1.2</span>)</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> scale_color_manual(values<span class="op">=</span>[PALETTE[<span class="dv">0</span>], PALETTE[<span class="dv">2</span>], PALETTE[<span class="dv">7</span>]])</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> scale_y_log10()</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> labs(</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>        title<span class="op">=</span><span class="st">"Estimator Convergence (N = 100)"</span>,</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="st">"Sample Size (T)"</span>,</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>        y<span class="op">=</span><span class="st">"Normalized Frobenius Loss"</span>,</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> THEME_ACADEMIC</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-9-output-1.png" class="quarto-figure quarto-figure-center figure-img" width="672" height="384"></p>
</figure>
</div>
</div>
</div>
<p>All three estimators converge as <span class="math inline">\(T\)</span> increases, but the rates differ substantially. In the most challenging regime (<span class="math inline">\(T = 50\)</span>, so <span class="math inline">\(N/T = 2\)</span>), the sample covariance is by far the worst. Linear shrinkage provides a large improvement by pulling the estimate towards the identity, and nonlinear shrinkage improves further by adapting to the specific spectral shape of the Toeplitz structure. As <span class="math inline">\(T\)</span> grows, the gap between the methods narrows because the sample covariance itself becomes increasingly accurate.</p>
<p>To visualize the eigenvalue-level correction, we use a spiked covariance model where 10 eigenvalues equal 10 and the remaining 190 equal 1. The scree plot below compares the true population spectrum, the noisy sample eigenvalues, and the nonlinearly shrunk eigenvalues.</p>
<div id="dc81604b" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">400</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>n_spikes <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>spike_val <span class="op">=</span> <span class="fl">10.0</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>Sigma_spiked <span class="op">=</span> np.eye(N)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>Sigma_spiked[:n_spikes, :n_spikes] <span class="op">=</span> spike_val <span class="op">*</span> np.eye(n_spikes)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, (T, N))</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> np.linalg.cholesky(Sigma_spiked)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>X_spiked <span class="op">=</span> Z <span class="op">@</span> L.T</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>S_spiked <span class="op">=</span> np.cov(X_spiked, rowvar<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>S_nl_spiked, evals_nl <span class="op">=</span> nonlinear_shrinkage(X_spiked)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>evals_sample <span class="op">=</span> np.sort(np.linalg.eigvalsh(S_spiked))[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>evals_true <span class="op">=</span> np.sort(np.linalg.eigvalsh(Sigma_spiked))[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>evals_nl <span class="op">=</span> np.sort(evals_nl)[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>df_scree <span class="op">=</span> pd.DataFrame({</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Rank'</span>: np.tile(np.arange(<span class="dv">1</span>, N<span class="op">+</span><span class="dv">1</span>), <span class="dv">3</span>),</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Eigenvalue'</span>: np.concatenate([evals_true, evals_sample, evals_nl]),</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Type'</span>: [<span class="st">'True Population'</span>] <span class="op">*</span> N <span class="op">+</span> [<span class="st">'Sample (Noisy)'</span>] <span class="op">*</span> N <span class="op">+</span> [<span class="st">'Nonlinear Shrinkage'</span>] <span class="op">*</span> N,</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>display(</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>    ggplot(df_scree, aes(x<span class="op">=</span><span class="st">'Rank'</span>, y<span class="op">=</span><span class="st">'Eigenvalue'</span>, color<span class="op">=</span><span class="st">'Type'</span>))</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> geom_line(size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> scale_y_log10()</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> scale_color_manual(values<span class="op">=</span>[PALETTE[<span class="dv">7</span>], PALETTE[<span class="dv">0</span>], PALETTE[<span class="dv">2</span>]])</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> labs(</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>        title<span class="op">=</span><span class="st">"Scree Plot: Spiked Covariance Model"</span>,</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="st">"Eigenvalue Rank"</span>,</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>        y<span class="op">=</span><span class="st">"Eigenvalue (Log Scale)"</span>,</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> THEME_ACADEMIC</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cell-10-output-1.png" class="quarto-figure quarto-figure-center figure-img" width="672" height="384"></p>
</figure>
</div>
</div>
</div>
<p>The sample eigenvalues overestimate the signal spikes and underestimate the noise floor. The nonlinear shrinkage successfully pulls the large eigenvalues down and the small eigenvalues up, recovering the population spectrum far more accurately.</p>
</section>
<section id="conclusion" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">6</span> Conclusion</h2>
<p>In high-dimensional statistics, the sample covariance matrix is untrustworthy due to the “curse of dimensionality,” which manifests as a deterministic spreading of eigenvalues described by the Marchenko-Pastur law. Linear shrinkage <span class="citation" data-cites="ledoit2004well">(<a href="#ref-ledoit2004well" role="doc-biblioref">Ledoit and Wolf 2004</a>)</span> offers a robust improvement by pulling the matrix towards a stable target, effectively reducing variance at the cost of some bias. Nonlinear shrinkage <span class="citation" data-cites="ledoit2020power">(<a href="#ref-ledoit2020power" role="doc-biblioref">Ledoit and Wolf 2022</a>)</span> takes this a step further by using Random Matrix Theory to optimally correct individual eigenvalues. It effectively inverts the Marchenko-Pastur equation to recover the population spectrum.</p>
<p>For practitioners, the choice between methods depends on the dimensionality ratio <span class="math inline">\(N/T\)</span>. When this ratio is small (below roughly 0.1), the sample covariance is generally sufficient. In the moderate regime, linear shrinkage is a robust and easy-to-implement baseline that has become standard in finance. When <span class="math inline">\(N/T\)</span> is large but below 1, nonlinear shrinkage provides significant gains by correcting the specific spectral distortion predicted by RMT, making it ideal for high-dimensional portfolios. In the singular regime (<span class="math inline">\(N &gt; T\)</span>), shrinkage is strictly necessary. Nonlinear shrinkage remains preferred when <span class="math inline">\(T\)</span> is reasonably large (say, above 100), but for extremely small samples the estimation of the spectral density becomes noisy and linear shrinkage towards a structured target is often safer.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-ledoit2004well" class="csl-entry" role="listitem">
Ledoit, Olivier, and Michael Wolf. 2004. <span>“A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices.”</span> <em>Journal of Multivariate Analysis</em> 88 (2): 365–411.
</div>
<div id="ref-ledoit2020power" class="csl-entry" role="listitem">
———. 2022. <span>“The Power of (Non-) Linear Shrinking: A Review and Guide to Covariance Matrix Estimation.”</span> <em>Journal of Financial Econometrics</em> 20 (1): 187–218.
</div>
<div id="ref-marchenko1967distribution" class="csl-entry" role="listitem">
Marchenko, Vladimir A, and Leonid A Pastur. 1967. <span>“Distribution of Eigenvalues for Some Sets of Random Matrices.”</span> <em>Matematicheskii Sbornik</em> 114 (4): 507–36.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>