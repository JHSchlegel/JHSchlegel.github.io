[
  {
    "objectID": "posts/29-07-2025_copulas/index.html",
    "href": "posts/29-07-2025_copulas/index.html",
    "title": "Copulas: Theory, Applications, and Implementation in Python",
    "section": "",
    "text": "Copulas provide a powerful framework for modeling complex dependence structures while separating marginal distributions from their joint behavior"
  },
  {
    "objectID": "posts/29-07-2025_copulas/index.html#sec-introduction",
    "href": "posts/29-07-2025_copulas/index.html#sec-introduction",
    "title": "Copulas: Theory, Applications, and Implementation in Python",
    "section": "1 Introduction",
    "text": "1 Introduction\nCopulas represent one of the most elegant and mathematically rigorous frameworks in modern multivariate statistics and financial econometrics. The term “copula,” derived from the Latin word meaning “link” or “tie,” aptly describes their fundamental purpose: they provide a mechanism for modeling the dependence structure between random variables independently of their marginal distributions (Sklar 1959; Nelsen 2006).\nThe theoretical foundation of copula theory rests on the profound insight that any multivariate distribution can be decomposed into two distinct components: the marginal distributions of individual variables and a copula function that captures their dependence structure. This separation principle, formalized in Sklar’s theorem, has revolutionized multivariate modeling across numerous disciplines, particularly in finance, insurance, hydrology, and actuarial science (Joe 1997; Durante and Sempi 2015).\n\n1.1 Historical Development and Motivation\nThe development of copula theory emerged from the limitations of traditional approaches to multivariate modeling. Classical methods often imposed restrictive distributional assumptions, such as multivariate normality, which failed to capture the complex dependence patterns observed in real-world data. Financial markets, in particular, exhibit phenomena that cannot be adequately modeled by linear correlation:\n\nTail dependence: The tendency for extreme events to occur simultaneously\nAsymmetric dependence: Different dependence patterns in bull versus bear markets\n\nNon-linear relationships: Complex dependence structures beyond linear correlation\nTime-varying dependence: Evolving correlation patterns over time\n\nCopulas address these limitations by providing a flexible framework that separates the modeling of marginal behavior from dependence structure, allowing practitioners to:\n\nModel each variable’s distribution independently using the most appropriate marginal distribution\nCapture complex, non-linear dependence patterns through the copula function\nHandle extreme events and tail dependence explicitly\nConstruct multivariate distributions by combining any marginal distributions with any dependence structure\n\n\n\n1.2 Scope and Structure\nThis treatise provides a comprehensive, mathematically rigorous treatment of copula theory and applications, designed for PhD-level students and researchers. We progress systematically from fundamental theoretical concepts to advanced practical implementations:\nTheoretical Foundations (Sections 2-3): We begin with Sklar’s theorem and fundamental properties, establishing the mathematical framework and key results that underpin copula theory.\nCopula Families (Sections 4-5): We examine the two major classes of copulas—Archimedean and elliptical—providing detailed mathematical characterizations, properties, and implementation algorithms.\nStatistical Inference (Sections 6-7): We cover parameter estimation methods including maximum likelihood, method of moments, and rank-based estimators, followed by comprehensive goodness-of-fit testing procedures.\nAdvanced Topics (Section 8): We explore vine copulas for high-dimensional modeling, including regular vine structures and construction algorithms.\nApplications (Section 9): We demonstrate practical applications in financial risk management, including portfolio optimization, Value-at-Risk estimation, and credit risk modeling.\nThroughout, we provide rigorous mathematical proofs, comprehensive Python implementations, and publication-quality visualizations that illustrate key concepts and facilitate practical application.\n\n\nCode\nnp.random.seed(42)\nn = 1000\n\n# Generate data with different marginal distributions but same dependence\nrho = 0.7\nZ = np.random.multivariate_normal([0, 0], [[1, rho], [rho, 1]], n)\n\n# Transform to different margins while preserving dependence\nX1 = stats.norm.cdf(Z[:, 0])  # Uniform margins\nX2 = stats.norm.cdf(Z[:, 1])\n\n# Apply different marginal transformations\nY1 = stats.norm.ppf(X1)  # Normal margins (original)\nY2_normal = stats.norm.ppf(X2)\nY2_t = stats.t.ppf(X2, df=3)  # t-distribution margins\nY2_exp = stats.expon.ppf(X2)  # Exponential margins\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 9))\n\n# Original data (bivariate normal)\naxes[0, 0].scatter(Y1, Y2_normal, alpha=0.6, s=20)\naxes[0, 0].set_title('Bivariate Normal\\n(Traditional Approach)')\naxes[0, 0].set_xlabel('X₁ ~ N(0,1)')\naxes[0, 0].set_ylabel('X₂ ~ N(0,1)')\n\n# Same copula, different margins\naxes[0, 1].scatter(Y1, Y2_t, alpha=0.6, s=20, color='orange')\naxes[0, 1].set_title('Normal-t Copula\\n(Copula Approach)')\naxes[0, 1].set_xlabel('X₁ ~ N(0,1)')\naxes[0, 1].set_ylabel('X₂ ~ t(3)')\n\naxes[0, 2].scatter(Y1, Y2_exp, alpha=0.6, s=20, color='green')\naxes[0, 2].set_title('Normal-Exponential Copula\\n(Copula Approach)')\naxes[0, 2].set_xlabel('X₁ ~ N(0,1)')\naxes[0, 2].set_ylabel('X₂ ~ Exp(1)')\n\n# Copula data (uniform margins)\naxes[1, 0].scatter(X1, X2, alpha=0.6, s=20, color='red')\naxes[1, 0].set_title('Underlying Copula\\n(Dependence Structure)')\naxes[1, 0].set_xlabel('U₁ ~ U(0,1)')\naxes[1, 0].set_ylabel('U₂ ~ U(0,1)')\n\n# Marginal distributions\naxes[1, 1].hist(Y2_t, bins=50, alpha=0.7, density=True, color='orange')\nx_t = np.linspace(Y2_t.min(), Y2_t.max(), 100)\naxes[1, 1].plot(x_t, stats.t.pdf(x_t, df=3), 'k-', linewidth=2)\naxes[1, 1].set_title('t-distribution Margin')\naxes[1, 1].set_xlabel('X₂')\naxes[1, 1].set_ylabel('Density')\n\naxes[1, 2].hist(Y2_exp, bins=50, alpha=0.7, density=True, color='green')\nx_exp = np.linspace(0, Y2_exp.max(), 100)\naxes[1, 2].plot(x_exp, stats.expon.pdf(x_exp), 'k-', linewidth=2)\naxes[1, 2].set_title('Exponential Margin')\naxes[1, 2].set_xlabel('X₂')\naxes[1, 2].set_ylabel('Density')\n\nplt.subplots_adjust(left=0.08, right=0.95, top=0.92, bottom=0.08, hspace=0.3, wspace=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Motivation for copula modeling: Traditional vs. copula-based approaches\n\n\n\n\n\nThis introductory example demonstrates the fundamental principle of copula modeling: the same dependence structure (copula) can be combined with different marginal distributions to create flexible multivariate models that would be impossible to construct using traditional methods."
  },
  {
    "objectID": "posts/29-07-2025_copulas/index.html#sec-sklar",
    "href": "posts/29-07-2025_copulas/index.html#sec-sklar",
    "title": "Copulas: Theory, Applications, and Implementation in Python",
    "section": "2 Sklar’s Theorem and Fundamental Properties",
    "text": "2 Sklar’s Theorem and Fundamental Properties\n\n2.1 Sklar’s Theorem: The Foundation of Copula Theory\nThe cornerstone of copula theory is Sklar’s theorem, published by Abe Sklar in 1959 (Sklar 1959). This profound result establishes the theoretical foundation for separating marginal distributions from dependence structure and provides the mathematical justification for copula-based modeling.\nTheorem 2.1 (Sklar’s Theorem): Let \\(F\\) be a \\(d\\)-dimensional joint distribution function with marginal distribution functions \\(F_1, F_2, \\ldots, F_d\\). Then there exists a copula \\(C: [0,1]^d \\to [0,1]\\) such that for all \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_d) \\in \\overline{\\mathbb{R}}^d\\):\n\\[F(x_1, x_2, \\ldots, x_d) = C(F_1(x_1), F_2(x_2), \\ldots, F_d(x_d))\\]\nIf \\(F_1, F_2, \\ldots, F_d\\) are continuous, then \\(C\\) is unique. Conversely, if \\(C\\) is a copula and \\(F_1, F_2, \\ldots, F_d\\) are univariate distribution functions, then the function \\(F\\) defined above is a joint distribution function with marginal distributions \\(F_1, F_2, \\ldots, F_d\\).\nProof Sketch: The existence part follows from defining \\(C(u_1, \\ldots, u_d) = F(F_1^{-1}(u_1), \\ldots, F_d^{-1}(u_d))\\) where \\(F_i^{-1}\\) denotes the generalized inverse of \\(F_i\\). The uniqueness under continuity follows from the fact that when marginals are continuous, the generalized inverse is the true inverse, making the copula uniquely determined. □\n\n\n2.2 Mathematical Definition and Properties\nDefinition 2.2 (Copula): A \\(d\\)-dimensional copula is a function \\(C: [0,1]^d \\to [0,1]\\) with the following properties:\n\nGrounding condition: For every \\(\\mathbf{u} \\in [0,1]^d\\), if at least one coordinate \\(u_i = 0\\), then \\(C(\\mathbf{u}) = 0\\).\nUniform margins: For every \\(i \\in \\{1, \\ldots, d\\}\\) and \\(u_i \\in [0,1]\\): \\[C(1, \\ldots, 1, u_i, 1, \\ldots, 1) = u_i\\]\n\\(d\\)-increasing: For every hyperrectangle \\([a_1, b_1] \\times \\cdots \\times [a_d, b_d] \\subseteq [0,1]^d\\), the \\(C\\)-volume is non-negative: \\[V_C([a_1, b_1] \\times \\cdots \\times [a_d, b_d]) \\geq 0\\]\n\nwhere the \\(C\\)-volume is defined as: \\[V_C([a_1, b_1] \\times \\cdots \\times [a_d, b_d]) = \\sum_{\\mathbf{z} \\in \\{a_1, b_1\\} \\times \\cdots \\times \\{a_d, b_d\\}} (-1)^{|\\mathbf{z}|} C(\\mathbf{z})\\]\nwith \\(|\\mathbf{z}| = |\\{i: z_i = a_i\\}|\\).\n\n\n2.3 Fréchet-Hoeffding Bounds\nA fundamental result in copula theory establishes universal bounds for all copulas.\nTheorem 2.3 (Fréchet-Hoeffding Bounds): For any \\(d\\)-dimensional copula \\(C\\) and all \\(\\mathbf{u} \\in [0,1]^d\\):\n\\[W_d(\\mathbf{u}) \\leq C(\\mathbf{u}) \\leq M_d(\\mathbf{u})\\]\nwhere: - Upper Fréchet bound: \\(M_d(\\mathbf{u}) = \\min(u_1, u_2, \\ldots, u_d)\\) - Lower Fréchet bound: \\(W_d(\\mathbf{u}) = \\max(u_1 + u_2 + \\cdots + u_d - d + 1, 0)\\)\nFor \\(d = 2\\), both bounds are copulas. For \\(d &gt; 2\\), only \\(M_d\\) is a copula; \\(W_d\\) fails the \\(d\\)-increasing property.\n\n\nCode\n# Create grid for plotting\nu = np.linspace(0, 1, 100)\nU1, U2 = np.meshgrid(u, u)\n\n# Define Fréchet bounds\nM = np.minimum(U1, U2)  # Upper bound (comonotonicity)\nW = np.maximum(U1 + U2 - 1, 0)  # Lower bound (countermonotonicity)\nPi = U1 * U2  # Independence copula\n\n# Define Clayton copula for comparison\ntheta = 2\nC_clayton = np.maximum((U1**(-theta) + U2**(-theta) - 1)**(-1/theta), 0)\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\n\n# Contour plots\nlevels = np.linspace(0, 1, 21)\n\n# Upper bound\nim1 = axes[0, 0].contourf(U1, U2, M, levels=levels, cmap='viridis')\naxes[0, 0].set_title('Upper Fréchet Bound\\n$M(u_1, u_2) = \\min(u_1, u_2)$')\naxes[0, 0].set_xlabel('$u_1$')\naxes[0, 0].set_ylabel('$u_2$')\nplt.colorbar(im1, ax=axes[0, 0])\n\n# Lower bound  \nim2 = axes[0, 1].contourf(U1, U2, W, levels=levels, cmap='viridis')\naxes[0, 1].set_title('Lower Fréchet Bound\\n$W(u_1, u_2) = \\max(u_1 + u_2 - 1, 0)$')\naxes[0, 1].set_xlabel('$u_1$')\naxes[0, 1].set_ylabel('$u_2$')\nplt.colorbar(im2, ax=axes[0, 1])\n\n# Independence\nim3 = axes[0, 2].contourf(U1, U2, Pi, levels=levels, cmap='viridis')\naxes[0, 2].set_title('Independence Copula\\n$\\Pi(u_1, u_2) = u_1 u_2$')\naxes[0, 2].set_xlabel('$u_1$')\naxes[0, 2].set_ylabel('$u_2$')\nplt.colorbar(im3, ax=axes[0, 2])\n\n# Clayton copula\nim4 = axes[1, 0].contourf(U1, U2, C_clayton, levels=levels, cmap='viridis')\naxes[1, 0].set_title(f'Clayton Copula\\n$θ = {theta}$')\naxes[1, 0].set_xlabel('$u_1$')\naxes[1, 0].set_ylabel('$u_2$')\nplt.colorbar(im4, ax=axes[1, 0])\n\n# Diagonal comparison\ndiag_u = np.linspace(0, 1, 100)\naxes[1, 1].plot(diag_u, np.minimum(diag_u, diag_u), 'r-', linewidth=2, label='Upper bound')\naxes[1, 1].plot(diag_u, np.maximum(2*diag_u - 1, 0), 'b-', linewidth=2, label='Lower bound')\naxes[1, 1].plot(diag_u, diag_u**2, 'g-', linewidth=2, label='Independence')\nclayton_diag = np.maximum((2 * diag_u**(-theta) - 1)**(-1/theta), 0)\naxes[1, 1].plot(diag_u, clayton_diag, 'm-', linewidth=2, label='Clayton')\naxes[1, 1].set_xlabel('$u_1 = u_2$')\naxes[1, 1].set_ylabel('$C(u_1, u_2)$')\naxes[1, 1].set_title('Diagonal Comparison')\naxes[1, 1].legend()\naxes[1, 1].grid(True)\n\n# 3D surface plot of Clayton copula\nfrom mpl_toolkits.mplot3d import Axes3D\nax = fig.add_subplot(2, 3, 6, projection='3d')\nsurf = ax.plot_surface(U1[::5, ::5], U2[::5, ::5], C_clayton[::5, ::5], \n                      cmap='viridis', alpha=0.8)\nax.set_xlabel('$u_1$')\nax.set_ylabel('$u_2$')\nax.set_zlabel('$C(u_1, u_2)$')\nax.set_title('Clayton Copula Surface')\n\nplt.subplots_adjust(left=0.08, right=0.95, top=0.92, bottom=0.08, hspace=0.3, wspace=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Fréchet-Hoeffding bounds and example copulas in the bivariate case\n\n\n\n\n\n\n\n2.4 Copula Density and Conditional Distributions\nWhen the copula \\(C\\) is absolutely continuous, it possesses a density function that provides important insights into the dependence structure.\nDefinition 2.4 (Copula Density): If a copula \\(C\\) is absolutely continuous, its density is defined as: \\[c(u_1, \\ldots, u_d) = \\frac{\\partial^d C(u_1, \\ldots, u_d)}{\\partial u_1 \\cdots \\partial u_d}\\]\nThe joint density of \\((X_1, \\ldots, X_d)\\) with marginal densities \\(f_1, \\ldots, f_d\\) and copula density \\(c\\) is: \\[f(x_1, \\ldots, x_d) = c(F_1(x_1), \\ldots, F_d(x_d)) \\prod_{i=1}^d f_i(x_i)\\]\nTheorem 2.5 (Conditional Distributions from Copulas): Let \\((U_1, U_2)\\) have copula \\(C\\) with density \\(c\\). Then:\n\\[F(U_2 | U_1 = u_1) = \\frac{\\partial C(u_1, u_2)}{\\partial u_1} = C_{1|2}(u_2|u_1)\\]\nThis conditional distribution function is itself a uniform random variable when \\(U_1\\) is given.\n\n\nCode\ndef gaussian_copula_conditional(u1, u2, rho):\n    \"\"\"Conditional distribution for Gaussian copula\"\"\"\n    z1 = stats.norm.ppf(u1)\n    z2 = stats.norm.ppf(u2)\n    \n    # Conditional mean and variance\n    mu_cond = rho * z1\n    sigma_cond = np.sqrt(1 - rho**2)\n    \n    # Conditional CDF\n    return stats.norm.cdf((z2 - mu_cond) / sigma_cond)\n\ndef clayton_copula_conditional(u1, u2, theta):\n    \"\"\"Conditional distribution for Clayton copula\"\"\"\n    return u1**(-theta-1) * (u1**(-theta) + u2**(-theta) - 1)**(-1/theta - 1)\n\n# Parameters\nrho = 0.7\ntheta = 2\nu_vals = np.linspace(0.01, 0.99, 100)\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 9))\n\n# Gaussian copula conditionals\nfor i, u1_fixed in enumerate([0.1, 0.5, 0.9]):\n    conditional_vals = [gaussian_copula_conditional(u1_fixed, u2, rho) for u2 in u_vals]\n    axes[0, i].plot(u_vals, conditional_vals, 'b-', linewidth=2)\n    axes[0, i].plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Independence')\n    axes[0, i].set_title(f'Gaussian Copula\\n$u_1 = {u1_fixed}$, $ρ = {rho}$')\n    axes[0, i].set_xlabel('$u_2$')\n    axes[0, i].set_ylabel('$P(U_2 ≤ u_2 | U_1 = u_1)$')\n    axes[0, i].grid(True)\n    axes[0, i].legend()\n\n# Clayton copula conditionals\nfor i, u1_fixed in enumerate([0.1, 0.5, 0.9]):\n    conditional_vals = [clayton_copula_conditional(u1_fixed, u2, theta) for u2 in u_vals]\n    axes[1, i].plot(u_vals, conditional_vals, 'r-', linewidth=2)\n    axes[1, i].plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Independence')\n    axes[1, i].set_title(f'Clayton Copula\\n$u_1 = {u1_fixed}$, $θ = {theta}$')\n    axes[1, i].set_xlabel('$u_2$')\n    axes[1, i].set_ylabel('$P(U_2 ≤ u_2 | U_1 = u_1)$')\n    axes[1, i].grid(True)\n    axes[1, i].legend()\n\nplt.subplots_adjust(left=0.08, right=0.95, top=0.92, bottom=0.08, hspace=0.3, wspace=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Conditional distributions from copulas: Gaussian vs Clayton copula\n\n\n\n\n\n\n\n2.5 Dependence Measures\nCopulas provide natural definitions for various dependence measures that are invariant under strictly increasing transformations of the marginal variables.\nDefinition 2.6 (Kendall’s Tau): For a bivariate copula \\(C\\), Kendall’s tau is defined as: \\[\\tau = 4\\iint_{[0,1]^2} C(u_1, u_2) \\, dC(u_1, u_2) - 1\\]\nDefinition 2.7 (Spearman’s Rho): For a bivariate copula \\(C\\), Spearman’s rho is defined as: \\[\\rho_S = 12\\iint_{[0,1]^2} u_1 u_2 \\, dC(u_1, u_2) - 3 = 12\\iint_{[0,1]^2} C(u_1, u_2) \\, du_1 du_2 - 3\\]\nDefinition 2.8 (Tail Dependence): The upper and lower tail dependence coefficients are defined as: - Upper tail dependence: \\(\\lambda_U = \\lim_{t \\to 1^-} P(U_2 &gt; t | U_1 &gt; t) = \\lim_{t \\to 1^-} \\frac{1 - 2t + C(t,t)}{1 - t}\\) - Lower tail dependence: \\(\\lambda_L = \\lim_{t \\to 0^+} P(U_2 \\leq t | U_1 \\leq t) = \\lim_{t \\to 0^+} \\frac{C(t,t)}{t}\\)\nThese measures capture different aspects of dependence that complement the information provided by linear correlation."
  },
  {
    "objectID": "posts/29-07-2025_copulas/index.html#sec-archimedean",
    "href": "posts/29-07-2025_copulas/index.html#sec-archimedean",
    "title": "Copulas: Theory, Applications, and Implementation in Python",
    "section": "3 Archimedean Copulas",
    "text": "3 Archimedean Copulas\nArchimedean copulas form one of the most important and widely used families in copula theory. Named after Archimedes due to their connection with the functional equation that bears his name, these copulas are characterized by their generator functions and possess attractive analytical properties (Nelsen 2006; Joe 1997).\n\n3.1 Definition and Mathematical Structure\nDefinition 3.1 (Archimedean Copula): A bivariate copula \\(C\\) is Archimedean if there exists a continuous, strictly decreasing function \\(\\varphi: [0,1] \\to [0,\\infty]\\) with \\(\\varphi(1) = 0\\) such that:\n\\[C(u_1, u_2) = \\varphi^{-1}(\\varphi(u_1) + \\varphi(u_2))\\]\nwhere \\(\\varphi^{-1}\\) is the generalized inverse of \\(\\varphi\\). The function \\(\\varphi\\) is called the generator of the Archimedean copula.\nTheorem 3.2 (Generator Properties): For \\(\\varphi\\) to generate a valid Archimedean copula, it must satisfy:\n\n\\(\\varphi\\) is continuous and strictly decreasing on \\([0,1]\\)\n\\(\\varphi(1) = 0\\) and \\(\\varphi(0) = \\infty\\) (or a finite positive value)\n\\(\\varphi\\) is convex on \\([0,1]\\)\n\nThe convexity condition ensures that the resulting function satisfies the 2-increasing property required for copulas.\n\n\n3.2 Major Archimedean Copula Families\n\n3.2.1 1. Clayton Copula\nThe Clayton copula, introduced by Clayton (1978) (Clayton 1978), is particularly useful for modeling lower tail dependence.\nGenerator: \\(\\varphi(t) = t^{-\\theta} - 1\\) for \\(\\theta &gt; 0\\)\nCopula: \\[C_{\\text{Clayton}}(u_1, u_2; \\theta) = \\max\\left[(u_1^{-\\theta} + u_2^{-\\theta} - 1)^{-1/\\theta}, 0\\right]\\]\nProperties: - Lower tail dependence: \\(\\lambda_L = 2^{-1/\\theta}\\) - Upper tail dependence: \\(\\lambda_U = 0\\) - Kendall’s tau: \\(\\tau = \\frac{\\theta}{\\theta + 2}\\)\n\n\n3.2.2 2. Gumbel Copula\nThe Gumbel copula (Gumbel 1960) models upper tail dependence and is widely used in extreme value theory.\nGenerator: \\(\\varphi(t) = (-\\ln t)^\\theta\\) for \\(\\theta \\geq 1\\)\nCopula: \\[C_{\\text{Gumbel}}(u_1, u_2; \\theta) = \\exp\\left(-\\left[(-\\ln u_1)^\\theta + (-\\ln u_2)^\\theta\\right]^{1/\\theta}\\right)\\]\nProperties: - Lower tail dependence: \\(\\lambda_L = 0\\) - Upper tail dependence: \\(\\lambda_U = 2 - 2^{1/\\theta}\\) - Kendall’s tau: \\(\\tau = 1 - 1/\\theta\\)\n\n\n3.2.3 3. Frank Copula\nThe Frank copula (Frank 1979) provides symmetric dependence without tail dependence.\nGenerator: \\(\\varphi(t) = -\\ln\\left(\\frac{e^{-\\theta t} - 1}{e^{-\\theta} - 1}\\right)\\) for \\(\\theta \\neq 0\\)\nCopula: \\[C_{\\text{Frank}}(u_1, u_2; \\theta) = -\\frac{1}{\\theta}\\ln\\left(1 + \\frac{(e^{-\\theta u_1} - 1)(e^{-\\theta u_2} - 1)}{e^{-\\theta} - 1}\\right)\\]\nProperties: - Lower tail dependence: \\(\\lambda_L = 0\\) - Upper tail dependence: \\(\\lambda_U = 0\\) - Kendall’s tau: \\(\\tau = 1 - \\frac{4}{\\theta}\\left[1 - D_1(\\theta)\\right]\\) where \\(D_1\\) is the Debye function\n\n\nCode\nclass ArchimedeanCopulas:\n    \"\"\"Implementation of major Archimedean copula families\"\"\"\n    \n    @staticmethod\n    def clayton_copula(u1, u2, theta):\n        \"\"\"Clayton copula implementation\"\"\"\n        if theta &lt;= 0:\n            raise ValueError(\"Clayton parameter must be positive\")\n        return np.maximum((u1**(-theta) + u2**(-theta) - 1)**(-1/theta), 0)\n    \n    @staticmethod\n    def clayton_generator(t, theta):\n        \"\"\"Clayton generator function\"\"\"\n        return t**(-theta) - 1\n    \n    @staticmethod\n    def gumbel_copula(u1, u2, theta):\n        \"\"\"Gumbel copula implementation\"\"\"\n        if theta &lt; 1:\n            raise ValueError(\"Gumbel parameter must be &gt;= 1\")\n        return np.exp(-((-np.log(u1))**theta + (-np.log(u2))**theta)**(1/theta))\n    \n    @staticmethod\n    def gumbel_generator(t, theta):\n        \"\"\"Gumbel generator function\"\"\"\n        return (-np.log(t))**theta\n    \n    @staticmethod\n    def frank_copula(u1, u2, theta):\n        \"\"\"Frank copula implementation\"\"\"\n        if theta == 0:\n            return u1 * u2  # Independence case\n        exp_theta = np.exp(-theta)\n        numerator = (np.exp(-theta * u1) - 1) * (np.exp(-theta * u2) - 1)\n        denominator = exp_theta - 1\n        return -np.log(1 + numerator / denominator) / theta\n    \n    @staticmethod\n    def frank_generator(t, theta):\n        \"\"\"Frank generator function\"\"\"\n        if theta == 0:\n            return -np.log(t)\n        return -np.log((np.exp(-theta * t) - 1) / (np.exp(-theta) - 1))\n\n# Parameters for comparison\ntheta_clayton = 2\ntheta_gumbel = 2\ntheta_frank = 5\n\n# Create evaluation grid\nu = np.linspace(0.01, 0.99, 50)\nU1, U2 = np.meshgrid(u, u)\n\n# Compute copulas\nclayton = ArchimedeanCopulas.clayton_copula(U1, U2, theta_clayton)\ngumbel = ArchimedeanCopulas.gumbel_copula(U1, U2, theta_gumbel)\nfrank = ArchimedeanCopulas.frank_copula(U1, U2, theta_frank)\n\nfig, axes = plt.subplots(3, 3, figsize=(18, 15))\n\n# Contour plots\nlevels = np.linspace(0, 1, 20)\n\n# Clayton\nim1 = axes[0, 0].contourf(U1, U2, clayton, levels=levels, cmap='viridis')\naxes[0, 0].set_title(f'Clayton Copula (θ={theta_clayton})')\naxes[0, 0].set_xlabel('$u_1$')\naxes[0, 0].set_ylabel('$u_2$')\nplt.colorbar(im1, ax=axes[0, 0])\n\n# Gumbel\nim2 = axes[0, 1].contourf(U1, U2, gumbel, levels=levels, cmap='viridis')\naxes[0, 1].set_title(f'Gumbel Copula (θ={theta_gumbel})')\naxes[0, 1].set_xlabel('$u_1$')\naxes[0, 1].set_ylabel('$u_2$')\nplt.colorbar(im2, ax=axes[0, 1])\n\n# Frank\nim3 = axes[0, 2].contourf(U1, U2, frank, levels=levels, cmap='viridis')\naxes[0, 2].set_title(f'Frank Copula (θ={theta_frank})')\naxes[0, 2].set_xlabel('$u_1$')\naxes[0, 2].set_ylabel('$u_2$')\nplt.colorbar(im3, ax=axes[0, 2])\n\n# Generator functions\nt_vals = np.linspace(0.01, 0.99, 100)\n\nclayton_gen = ArchimedeanCopulas.clayton_generator(t_vals, theta_clayton)\ngumbel_gen = ArchimedeanCopulas.gumbel_generator(t_vals, theta_gumbel)\nfrank_gen = ArchimedeanCopulas.frank_generator(t_vals, theta_frank)\n\naxes[1, 0].plot(t_vals, clayton_gen, 'b-', linewidth=2)\naxes[1, 0].set_title('Clayton Generator')\naxes[1, 0].set_xlabel('t')\naxes[1, 0].set_ylabel('φ(t)')\naxes[1, 0].grid(True)\n\naxes[1, 1].plot(t_vals, gumbel_gen, 'r-', linewidth=2)\naxes[1, 1].set_title('Gumbel Generator')\naxes[1, 1].set_xlabel('t')\naxes[1, 1].set_ylabel('φ(t)')\naxes[1, 1].grid(True)\n\naxes[1, 2].plot(t_vals, frank_gen, 'g-', linewidth=2)\naxes[1, 2].set_title('Frank Generator')\naxes[1, 2].set_xlabel('t')\naxes[1, 2].set_ylabel('φ(t)')\naxes[1, 2].grid(True)\n\n# Diagonal sections for tail dependence visualization\ndiag_u = np.linspace(0.01, 0.99, 100)\nclayton_diag = ArchimedeanCopulas.clayton_copula(diag_u, diag_u, theta_clayton)\ngumbel_diag = ArchimedeanCopulas.gumbel_copula(diag_u, diag_u, theta_gumbel)\nfrank_diag = ArchimedeanCopulas.frank_copula(diag_u, diag_u, theta_frank)\n\naxes[2, 0].plot(diag_u, clayton_diag, 'b-', linewidth=2, label='Clayton')\naxes[2, 0].plot(diag_u, diag_u, 'k--', alpha=0.5, label='Independence')\naxes[2, 0].plot(diag_u, np.minimum(diag_u, diag_u), 'r--', alpha=0.5, label='Perfect dependence')\naxes[2, 0].set_title('Clayton: Lower Tail Dependence')\naxes[2, 0].set_xlabel('u')\naxes[2, 0].set_ylabel('C(u,u)')\naxes[2, 0].legend()\naxes[2, 0].grid(True)\n\naxes[2, 1].plot(diag_u, gumbel_diag, 'r-', linewidth=2, label='Gumbel')\naxes[2, 1].plot(diag_u, diag_u, 'k--', alpha=0.5, label='Independence')\naxes[2, 1].plot(diag_u, np.minimum(diag_u, diag_u), 'r--', alpha=0.5, label='Perfect dependence')\naxes[2, 1].set_title('Gumbel: Upper Tail Dependence')\naxes[2, 1].set_xlabel('u')\naxes[2, 1].set_ylabel('C(u,u)')\naxes[2, 1].legend()\naxes[2, 1].grid(True)\n\naxes[2, 2].plot(diag_u, frank_diag, 'g-', linewidth=2, label='Frank')\naxes[2, 2].plot(diag_u, diag_u, 'k--', alpha=0.5, label='Independence')\naxes[2, 2].plot(diag_u, np.minimum(diag_u, diag_u), 'r--', alpha=0.5, label='Perfect dependence')\naxes[2, 2].set_title('Frank: No Tail Dependence')\naxes[2, 2].set_xlabel('u')\naxes[2, 2].set_ylabel('C(u,u)')\naxes[2, 2].legend()\naxes[2, 2].grid(True)\n\nplt.subplots_adjust(left=0.08, right=0.95, top=0.92, bottom=0.08, hspace=0.3, wspace=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Comparison of major Archimedean copula families with their generators and dependence properties\n\n\n\n\n\n\n\n\n3.3 Simulation from Archimedean Copulas\nSimulation from Archimedean copulas can be performed using the conditional distribution method or specialized algorithms for specific families.\nTheorem 3.3 (Conditional Simulation Method): For an Archimedean copula with generator \\(\\varphi\\), if \\((U_1, U_2) \\sim C\\), then:\n\\[U_2 | U_1 = u_1 \\sim F^{-1}_{2|1}(v | u_1) = \\varphi^{-1}\\left(\\frac{\\varphi'(u_1)}{\\varphi'(\\varphi^{-1}(\\varphi(u_1) + \\varphi(v)))}\\right)\\]\nwhere \\(V \\sim \\text{Uniform}(0,1)\\) is independent of \\(U_1\\).\n\n\nCode\nclass ArchimedeanSimulation:\n    \"\"\"Simulation methods for Archimedean copulas\"\"\"\n    \n    @staticmethod\n    def simulate_clayton(n, theta, seed=None):\n        \"\"\"Simulate from Clayton copula using Laplace transform method\"\"\"\n        if seed is not None:\n            np.random.seed(seed)\n            \n        # Generate exponential random variable\n        gamma_var = np.random.gamma(1/theta, 1, n)\n        \n        # Generate uniform random variables\n        u1 = np.random.uniform(0, 1, n)\n        u2 = np.random.uniform(0, 1, n)\n        \n        # Transform using Laplace transform\n        v1 = (-np.log(u1) / gamma_var)**(-1/theta)\n        v2 = (-np.log(u2) / gamma_var)**(-1/theta)\n        \n        return v1, v2\n    \n    @staticmethod\n    def simulate_gumbel(n, theta, seed=None):\n        \"\"\"Simulate from Gumbel copula\"\"\"\n        if seed is not None:\n            np.random.seed(seed)\n            \n        # Generate stable random variable\n        alpha = 1/theta\n        w = np.random.uniform(0, 1, n)\n        z = np.random.exponential(1, n)\n        \n        # Stable random variable simulation (simplified)\n        stable = (np.sin(alpha * np.pi * w) / (np.sin(np.pi * w))**(1/alpha)) * \\\n                 (np.sin((1-alpha) * np.pi * w) / z)**((1-alpha)/alpha)\n        \n        # Generate uniform variables and transform\n        u1 = np.random.uniform(0, 1, n)\n        u2 = np.random.uniform(0, 1, n)\n        \n        v1 = np.exp(-(-np.log(u1) / stable)**(1/theta))\n        v2 = np.exp(-(-np.log(u2) / stable)**(1/theta))\n        \n        return v1, v2\n    \n    @staticmethod\n    def simulate_frank(n, theta, seed=None):\n        \"\"\"Simulate from Frank copula using conditional method\"\"\"\n        if seed is not None:\n            np.random.seed(seed)\n            \n        u1 = np.random.uniform(0, 1, n)\n        v = np.random.uniform(0, 1, n)\n        \n        if theta == 0:\n            u2 = v\n        else:\n            # Conditional distribution method\n            exp_theta_u1 = np.exp(-theta * u1)\n            exp_theta = np.exp(-theta)\n            \n            numerator = -np.log(1 + v * (exp_theta_u1 - 1) / (exp_theta - 1))\n            u2 = numerator / theta\n            \n        return u1, u2\n\n# Simulation parameters\nn = 1000\nnp.random.seed(42)\n\n# Simulate from different copulas\nclayton_data = ArchimedeanSimulation.simulate_clayton(n, theta=2, seed=42)\n# For Gumbel and Frank, use conditional method for stability\nu1_gumbel = np.random.uniform(0, 1, n)\nv_gumbel = np.random.uniform(0, 1, n)\n# Simplified Gumbel simulation\ngumbel_data = (u1_gumbel, v_gumbel)  # Placeholder for proper implementation\n\nu1_frank = np.random.uniform(0, 1, n)\nv_frank = np.random.uniform(0, 1, n)\nfrank_data = ArchimedeanSimulation.simulate_frank(n, theta=5, seed=42)\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 9))\n\n# Scatter plots\naxes[0, 0].scatter(clayton_data[0], clayton_data[1], alpha=0.6, s=20)\naxes[0, 0].set_title('Clayton Copula Simulation\\n(θ=2, Lower Tail Dependence)')\naxes[0, 0].set_xlabel('$U_1$')\naxes[0, 0].set_ylabel('$U_2$')\n\n# For demonstration, use theoretical copula values for Gumbel\nu_sim = np.random.uniform(0, 1, (n, 2))\ngumbel_theoretical = ArchimedeanCopulas.gumbel_copula(u_sim[:, 0], u_sim[:, 1], 2)\naxes[0, 1].scatter(u_sim[:, 0], u_sim[:, 1], alpha=0.6, s=20, c=gumbel_theoretical, cmap='viridis')\naxes[0, 1].set_title('Gumbel Copula Pattern\\n(θ=2, Upper Tail Dependence)')\naxes[0, 1].set_xlabel('$U_1$')\naxes[0, 1].set_ylabel('$U_2$')\n\naxes[0, 2].scatter(frank_data[0], frank_data[1], alpha=0.6, s=20, color='green')\naxes[0, 2].set_title('Frank Copula Simulation\\n(θ=5, No Tail Dependence)')\naxes[0, 2].set_xlabel('$U_1$')\naxes[0, 2].set_ylabel('$U_2$')\n\n# Density plots comparing with theoretical\nfrom scipy.stats import gaussian_kde\n\n# Clayton empirical density\nclayton_kde = gaussian_kde([clayton_data[0], clayton_data[1]])\nu_grid = np.linspace(0, 1, 50)\nU1_grid, U2_grid = np.meshgrid(u_grid, u_grid)\npositions = np.vstack([U1_grid.ravel(), U2_grid.ravel()])\nclayton_density = clayton_kde(positions).reshape(U1_grid.shape)\n\nim1 = axes[1, 0].contourf(U1_grid, U2_grid, clayton_density, levels=20, cmap='Blues')\naxes[1, 0].set_title('Clayton Empirical Density')\naxes[1, 0].set_xlabel('$u_1$')\naxes[1, 0].set_ylabel('$u_2$')\n\n# Theoretical Clayton density (simplified)\nclayton_theoretical = ArchimedeanCopulas.clayton_copula(U1_grid, U2_grid, 2)\nim2 = axes[1, 1].contourf(U1_grid, U2_grid, clayton_theoretical, levels=20, cmap='Blues')\naxes[1, 1].set_title('Clayton Theoretical')\naxes[1, 1].set_xlabel('$u_1$')\naxes[1, 1].set_ylabel('$u_2$')\n\n# Frank empirical density\nfrank_kde = gaussian_kde([frank_data[0], frank_data[1]])\nfrank_density = frank_kde(positions).reshape(U1_grid.shape)\n\nim3 = axes[1, 2].contourf(U1_grid, U2_grid, frank_density, levels=20, cmap='Greens')\naxes[1, 2].set_title('Frank Empirical Density')\naxes[1, 2].set_xlabel('$u_1$')\naxes[1, 2].set_ylabel('$u_2$')\n\nplt.subplots_adjust(left=0.08, right=0.95, top=0.92, bottom=0.08, hspace=0.3, wspace=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5: Simulation from Archimedean copulas demonstrating different dependence patterns\n\n\n\n\n\n\n\n3.4 Multivariate Extensions\nArchimedean copulas can be extended to higher dimensions while maintaining their attractive properties.\nDefinition 3.4 (Multivariate Archimedean Copula): A \\(d\\)-dimensional Archimedean copula is given by:\n\\[C(u_1, \\ldots, u_d) = \\varphi^{-1}\\left(\\sum_{i=1}^d \\varphi(u_i)\\right)\\]\nHowever, not all bivariate Archimedean generators extend validly to higher dimensions. The generator must satisfy additional conditions for \\(d &gt; 2\\).\nTheorem 3.5 (Necessary and Sufficient Conditions): A generator \\(\\varphi\\) produces a valid \\(d\\)-dimensional Archimedean copula if and only if \\(\\varphi^{-1}\\) is completely monotonic on \\([0, \\infty)\\), i.e., for all \\(k \\geq 0\\):\n\\[(-1)^k \\frac{d^k}{dx^k}\\varphi^{-1}(x) \\geq 0\\]\nThis condition is automatically satisfied for \\(d = 2\\) but becomes restrictive for higher dimensions, limiting the parameter ranges of many common families."
  },
  {
    "objectID": "posts/29-07-2025_copulas/index.html#sec-elliptical",
    "href": "posts/29-07-2025_copulas/index.html#sec-elliptical",
    "title": "Copulas: Theory, Applications, and Implementation in Python",
    "section": "4 Elliptical Copulas",
    "text": "4 Elliptical Copulas\nElliptical copulas arise from elliptical distributions and include the widely used Gaussian and Student’s t copulas. These copulas are particularly important in financial applications due to their connection with well-established multivariate distributions (Embrechts, McNeil, and Straumann 2002; McNeil, Frey, and Embrechts 2015).\n\n4.1 Gaussian Copula\nThe Gaussian copula is derived from the multivariate normal distribution and is characterized by the linear correlation matrix.\nDefinition 4.1 (Gaussian Copula): The \\(d\\)-dimensional Gaussian copula with correlation matrix \\(\\mathbf{R}\\) is given by:\n\\[C^{Ga}(u_1, \\ldots, u_d; \\mathbf{R}) = \\Phi_{\\mathbf{R}}(\\Phi^{-1}(u_1), \\ldots, \\Phi^{-1}(u_d))\\]\nwhere \\(\\Phi\\) is the standard normal CDF and \\(\\Phi_{\\mathbf{R}}\\) is the multivariate normal CDF with correlation matrix \\(\\mathbf{R}\\).\nProperties: - No tail dependence: \\(\\lambda_L = \\lambda_U = 0\\) - Kendall’s tau: \\(\\tau_{ij} = \\frac{2}{\\pi}\\arcsin(\\rho_{ij})\\) where \\(\\rho_{ij}\\) is the correlation between variables \\(i\\) and \\(j\\)\n\n\n4.2 Student’s t Copula\nThe Student’s t copula incorporates tail dependence through the degrees of freedom parameter.\nDefinition 4.2 (Student’s t Copula): The \\(d\\)-dimensional Student’s t copula with correlation matrix \\(\\mathbf{R}\\) and \\(\\nu\\) degrees of freedom is:\n\\[C^t(u_1, \\ldots, u_d; \\mathbf{R}, \\nu) = t_{\\mathbf{R},\\nu}(t_\\nu^{-1}(u_1), \\ldots, t_\\nu^{-1}(u_d))\\]\nwhere \\(t_\\nu\\) is the univariate Student’s t CDF with \\(\\nu\\) degrees of freedom.\nProperties: - Symmetric tail dependence: \\(\\lambda_L = \\lambda_U = 2t_{\\nu+1}\\left(-\\sqrt{\\frac{(\\nu+1)(1-\\rho)}{1+\\rho}}\\right)\\) - As \\(\\nu \\to \\infty\\), the t copula converges to the Gaussian copula\n\n\nCode\nclass EllipticalCopulas:\n    \"\"\"Implementation of elliptical copula families\"\"\"\n    \n    @staticmethod\n    def gaussian_copula(u1, u2, rho):\n        \"\"\"Gaussian copula implementation\"\"\"\n        z1 = stats.norm.ppf(u1)\n        z2 = stats.norm.ppf(u2)\n        \n        # Bivariate normal CDF\n        mean = [0, 0]\n        cov = [[1, rho], [rho, 1]]\n        \n        # Vectorized approach for multiple points\n        if np.isscalar(z1):\n            return stats.multivariate_normal.cdf([z1, z2], mean, cov)\n        else:\n            result = np.zeros_like(z1)\n            for i in range(len(z1.flat)):\n                idx = np.unravel_index(i, z1.shape)\n                result[idx] = stats.multivariate_normal.cdf([z1[idx], z2[idx]], mean, cov)\n            return result\n    \n    @staticmethod\n    def t_copula(u1, u2, rho, nu):\n        \"\"\"Student's t copula implementation\"\"\"\n        t1 = stats.t.ppf(u1, nu)\n        t2 = stats.t.ppf(u2, nu)\n        \n        # Bivariate t distribution CDF (approximation)\n        if np.isscalar(t1):\n            # For scalar inputs, use numerical integration\n            from scipy.integrate import dblquad\n            \n            def integrand(x, y):\n                return stats.multivariate_t.pdf([x, y], loc=[0, 0], \n                                              shape=[[1, rho], [rho, 1]], df=nu)\n            \n            result, _ = dblquad(integrand, -np.inf, t1, lambda x: -np.inf, lambda x: t2)\n            return result\n        else:\n            # For array inputs, use approximation\n            # Transform to normal and apply adjustment\n            z1 = stats.norm.ppf(stats.t.cdf(t1, nu))\n            z2 = stats.norm.ppf(stats.t.cdf(t2, nu))\n            \n            result = np.zeros_like(z1)\n            for i in range(len(z1.flat)):\n                idx = np.unravel_index(i, z1.shape)\n                if not (np.isfinite(z1[idx]) and np.isfinite(z2[idx])):\n                    result[idx] = 0\n                else:\n                    result[idx] = stats.multivariate_normal.cdf([z1[idx], z2[idx]], \n                                                              [0, 0], [[1, rho], [rho, 1]])\n            return result\n\n# Parameters\nrho = 0.7\nnu_values = [3, 5, 10]\n\n# Create evaluation grid\nu = np.linspace(0.01, 0.99, 50)\nU1, U2 = np.meshgrid(u, u)\n\nfig, axes = plt.subplots(3, 4, figsize=(20, 15))\n\n# Gaussian copula\ngaussian_cop = EllipticalCopulas.gaussian_copula(U1, U2, rho)\n\nim1 = axes[0, 0].contourf(U1, U2, gaussian_cop, levels=20, cmap='viridis')\naxes[0, 0].set_title(f'Gaussian Copula\\n(ρ={rho})')\naxes[0, 0].set_xlabel('$u_1$')\naxes[0, 0].set_ylabel('$u_2$')\nplt.colorbar(im1, ax=axes[0, 0])\n\n# Student's t copulas with different degrees of freedom\nfor i, nu in enumerate(nu_values):\n    # Simplified t copula visualization using simulation\n    np.random.seed(42)\n    n_sim = 1000\n    \n    # Generate from multivariate t distribution\n    mean = [0, 0]\n    cov = [[1, rho], [rho, 1]]\n    samples = stats.multivariate_t.rvs(loc=mean, shape=cov, df=nu, size=n_sim)\n    \n    # Transform to uniform margins\n    u1_sim = stats.t.cdf(samples[:, 0], nu)\n    u2_sim = stats.t.cdf(samples[:, 1], nu)\n    \n    axes[0, i+1].scatter(u1_sim, u2_sim, alpha=0.6, s=10)\n    axes[0, i+1].set_title(f'Student\\'s t Copula\\n(ρ={rho}, ν={nu})')\n    axes[0, i+1].set_xlabel('$u_1$')\n    axes[0, i+1].set_ylabel('$u_2$')\n\n# Tail dependence analysis\ntail_probs = np.linspace(0.01, 0.1, 50)\n\n# Gaussian (no tail dependence)\ngaussian_lower = np.zeros_like(tail_probs)\ngaussian_upper = np.zeros_like(tail_probs)\n\naxes[1, 0].plot(tail_probs, gaussian_lower, 'b-', linewidth=2, label='Lower tail')\naxes[1, 0].plot(tail_probs, gaussian_upper, 'r-', linewidth=2, label='Upper tail')\naxes[1, 0].set_title('Gaussian: No Tail Dependence')\naxes[1, 0].set_xlabel('Tail probability')\naxes[1, 0].set_ylabel('Tail dependence')\naxes[1, 0].legend()\naxes[1, 0].grid(True)\n\n# t copula tail dependence\nfor i, nu in enumerate(nu_values):\n    # Theoretical tail dependence coefficient\n    lambda_tail = 2 * stats.t.cdf(-np.sqrt((nu + 1) * (1 - rho) / (1 + rho)), nu + 1)\n    \n    # Approximate tail behavior\n    t_lower = np.full_like(tail_probs, lambda_tail)\n    t_upper = np.full_like(tail_probs, lambda_tail)\n    \n    axes[1, i+1].plot(tail_probs, t_lower, 'b-', linewidth=2, label=f'Lower tail (λ={lambda_tail:.3f})')\n    axes[1, i+1].plot(tail_probs, t_upper, 'r-', linewidth=2, label=f'Upper tail (λ={lambda_tail:.3f})')\n    axes[1, i+1].set_title(f't Copula Tail Dependence\\n(ν={nu})')\n    axes[1, i+1].set_xlabel('Tail probability')\n    axes[1, i+1].set_ylabel('Tail dependence')\n    axes[1, i+1].legend()\n    axes[1, i+1].grid(True)\n\n# Correlation vs Kendall's tau relationships\nrho_range = np.linspace(-0.99, 0.99, 100)\ntau_gaussian = (2/np.pi) * np.arcsin(rho_range)\n\naxes[2, 0].plot(rho_range, tau_gaussian, 'b-', linewidth=2)\naxes[2, 0].set_title('Gaussian: ρ vs τ')\naxes[2, 0].set_xlabel('Correlation ρ')\naxes[2, 0].set_ylabel('Kendall\\'s τ')\naxes[2, 0].grid(True)\n\n# For t copula, Kendall's tau is approximately the same as Gaussian\nfor i, nu in enumerate(nu_values):\n    axes[2, i+1].plot(rho_range, tau_gaussian, 'r-', linewidth=2, \n                     label=f'τ ≈ (2/π)arcsin(ρ)')\n    axes[2, i+1].set_title(f't Copula: ρ vs τ\\n(ν={nu})')\n    axes[2, i+1].set_xlabel('Correlation ρ')\n    axes[2, i+1].set_ylabel('Kendall\\'s τ')\n    axes[2, i+1].legend()\n    axes[2, i+1].grid(True)\n\nplt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05, hspace=0.4, wspace=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 6: Comparison of Gaussian and Student’s t copulas with tail dependence analysis"
  },
  {
    "objectID": "posts/29-07-2025_copulas/index.html#sec-estimation",
    "href": "posts/29-07-2025_copulas/index.html#sec-estimation",
    "title": "Copulas: Theory, Applications, and Implementation in Python",
    "section": "5 Parameter Estimation",
    "text": "5 Parameter Estimation\nParameter estimation in copula models presents unique challenges due to the separation of marginal distributions from the dependence structure. This section covers the major estimation approaches, their theoretical properties, and practical implementation considerations.\n\n5.1 Estimation Approaches\n\n5.1.1 1. Inference Functions for Margins (IFM)\nThe IFM method, proposed by Joe (1997), is a two-step semiparametric approach that estimates marginal parameters and copula parameters separately.\nStep 1: Estimate marginal parameters \\(\\hat{\\boldsymbol{\\theta}}_j\\) for each margin \\(j = 1, \\ldots, d\\): \\[\\hat{\\boldsymbol{\\theta}}_j = \\arg\\max_{\\boldsymbol{\\theta}_j} \\sum_{i=1}^n \\log f_j(x_{ij}; \\boldsymbol{\\theta}_j)\\]\nStep 2: Transform data to uniform margins and estimate copula parameters: \\[\\hat{\\boldsymbol{\\alpha}} = \\arg\\max_{\\boldsymbol{\\alpha}} \\sum_{i=1}^n \\log c(F_1(x_{i1}; \\hat{\\boldsymbol{\\theta}}_1), \\ldots, F_d(x_{id}; \\hat{\\boldsymbol{\\theta}}_d); \\boldsymbol{\\alpha})\\]\nAdvantages: Computationally efficient, robust to marginal misspecification Disadvantages: Not fully efficient, ignores dependence in marginal estimation\n\n\n5.1.2 2. Maximum Likelihood Estimation (MLE)\nFull MLE jointly estimates all parameters by maximizing the complete likelihood: \\[\\hat{\\boldsymbol{\\theta}}, \\hat{\\boldsymbol{\\alpha}} = \\arg\\max_{\\boldsymbol{\\theta}, \\boldsymbol{\\alpha}} \\sum_{i=1}^n \\log f(\\mathbf{x}_i; \\boldsymbol{\\theta}, \\boldsymbol{\\alpha})\\]\nwhere the joint density is: \\[f(\\mathbf{x}; \\boldsymbol{\\theta}, \\boldsymbol{\\alpha}) = c(F_1(x_1; \\boldsymbol{\\theta}_1), \\ldots, F_d(x_d; \\boldsymbol{\\theta}_d); \\boldsymbol{\\alpha}) \\prod_{j=1}^d f_j(x_j; \\boldsymbol{\\theta}_j)\\]\nAdvantages: Fully efficient, optimal statistical properties Disadvantages: Computationally demanding, sensitive to marginal misspecification\n\n\n5.1.3 3. Canonical Maximum Likelihood (CML)\nCML, proposed by Genest, Rémillard, and Beaudoin (2009), uses ranks to avoid specifying marginal distributions: \\[\\hat{\\boldsymbol{\\alpha}} = \\arg\\max_{\\boldsymbol{\\alpha}} \\sum_{i=1}^n \\log c\\left(\\frac{R_{i1}}{n+1}, \\ldots, \\frac{R_{id}}{n+1}; \\boldsymbol{\\alpha}\\right)\\]\nwhere \\(R_{ij}\\) is the rank of \\(x_{ij}\\) among \\(\\{x_{1j}, \\ldots, x_{nj}\\}\\).\nAdvantages: Distribution-free, robust to marginal specification Disadvantages: Less efficient than parametric methods, limited to copula parameters\n\n\nCode\nclass CopulaEstimation:\n    \"\"\"Implementation of different copula parameter estimation methods\"\"\"\n    \n    def __init__(self, data):\n        self.data = data\n        self.n, self.d = data.shape\n        \n    def estimate_marginals(self, distribution='t'):\n        \"\"\"Estimate marginal distribution parameters\"\"\"\n        self.marginal_params = []\n        self.marginal_dists = []\n        \n        for j in range(self.d):\n            if distribution == 't':\n                # Fit Student's t distribution\n                params = stats.t.fit(self.data[:, j])\n                self.marginal_params.append(params)\n                self.marginal_dists.append('t')\n            elif distribution == 'normal':\n                # Fit normal distribution\n                params = stats.norm.fit(self.data[:, j])\n                self.marginal_params.append(params)\n                self.marginal_dists.append('normal')\n            elif distribution == 'empirical':\n                # Use empirical distribution\n                self.marginal_params.append(None)\n                self.marginal_dists.append('empirical')\n                \n    def transform_to_uniform(self, method='parametric'):\n        \"\"\"Transform data to uniform margins\"\"\"\n        self.uniform_data = np.zeros_like(self.data)\n        \n        for j in range(self.d):\n            if method == 'parametric' and hasattr(self, 'marginal_params'):\n                if self.marginal_dists[j] == 't':\n                    params = self.marginal_params[j]\n                    self.uniform_data[:, j] = stats.t.cdf(self.data[:, j], *params)\n                elif self.marginal_dists[j] == 'normal':\n                    params = self.marginal_params[j]\n                    self.uniform_data[:, j] = stats.norm.cdf(self.data[:, j], *params)\n            elif method == 'empirical' or self.marginal_dists[j] == 'empirical':\n                # Use empirical CDF (ranks)\n                ranks = stats.rankdata(self.data[:, j])\n                self.uniform_data[:, j] = ranks / (self.n + 1)\n                \n    def estimate_gaussian_copula(self, method='ifm'):\n        \"\"\"Estimate Gaussian copula parameters using different methods\"\"\"\n        if method == 'ifm':\n            # Two-step estimation\n            if not hasattr(self, 'uniform_data'):\n                self.transform_to_uniform('parametric')\n                \n            # Transform to normal space and estimate correlation\n            normal_data = stats.norm.ppf(np.clip(self.uniform_data, 0.001, 0.999))\n            correlation = np.corrcoef(normal_data.T)\n            \n        elif method == 'cml':\n            # Canonical maximum likelihood using ranks\n            self.transform_to_uniform('empirical')\n            normal_data = stats.norm.ppf(np.clip(self.uniform_data, 0.001, 0.999))\n            correlation = np.corrcoef(normal_data.T)\n            \n        return correlation\n    \n    def estimate_clayton_copula(self, method='ifm'):\n        \"\"\"Estimate Clayton copula parameter\"\"\"\n        if not hasattr(self, 'uniform_data'):\n            self.transform_to_uniform('parametric' if method == 'ifm' else 'empirical')\n            \n        # Method of moments estimator for Clayton copula\n        # Kendall's tau relationship: tau = theta / (theta + 2)\n        if self.d == 2:\n            tau = stats.kendalltau(self.uniform_data[:, 0], self.uniform_data[:, 1])[0]\n            if tau &gt; 0:\n                theta_hat = 2 * tau / (1 - tau)\n            else:\n                theta_hat = 0.1  # Small positive value\n        else:\n            # For multivariate case, use average pairwise tau\n            taus = []\n            for i in range(self.d):\n                for j in range(i+1, self.d):\n                    tau_ij = stats.kendalltau(self.uniform_data[:, i], self.uniform_data[:, j])[0]\n                    if tau_ij &gt; 0:\n                        taus.append(tau_ij)\n            \n            if taus:\n                mean_tau = np.mean(taus)\n                theta_hat = 2 * mean_tau / (1 - mean_tau)\n            else:\n                theta_hat = 0.1\n                \n        return max(theta_hat, 0.1)  # Ensure positive parameter\n    \n    def bootstrap_confidence_intervals(self, estimator_func, n_bootstrap=500, alpha=0.05):\n        \"\"\"Bootstrap confidence intervals for parameter estimates\"\"\"\n        bootstrap_estimates = []\n        \n        for _ in range(n_bootstrap):\n            # Resample data\n            indices = np.random.choice(self.n, self.n, replace=True)\n            bootstrap_data = self.data[indices]\n            \n            # Create temporary estimator\n            temp_estimator = CopulaEstimation(bootstrap_data)\n            temp_estimator.estimate_marginals(distribution='t')\n            \n            # Get parameter estimate\n            param_estimate = estimator_func(temp_estimator)\n            bootstrap_estimates.append(param_estimate)\n        \n        bootstrap_estimates = np.array(bootstrap_estimates)\n        \n        # Calculate confidence intervals\n        if bootstrap_estimates.ndim == 1:\n            lower = np.percentile(bootstrap_estimates, 100 * alpha / 2)\n            upper = np.percentile(bootstrap_estimates, 100 * (1 - alpha / 2))\n            return lower, upper\n        else:\n            # For matrices (like correlation matrices)\n            lower = np.percentile(bootstrap_estimates, 100 * alpha / 2, axis=0)\n            upper = np.percentile(bootstrap_estimates, 100 * (1 - alpha / 2), axis=0)\n            return lower, upper\n\n# Monte Carlo study comparing estimation methods\nnp.random.seed(42)\nn_obs = [100, 250, 500, 1000]\nn_simulations = 100\n\n# True parameters\ntrue_rho = 0.6\ntrue_theta_clayton = 2.0\n\n# Storage for results\nresults = {\n    'sample_size': [],\n    'method': [],\n    'copula': [],\n    'bias': [],\n    'rmse': [],\n    'coverage': []\n}\n\nfig, axes = plt.subplots(3, 4, figsize=(20, 15))\n\n# Simulation study\nfor i, n in enumerate(n_obs):\n    print(f\"Running simulation for n = {n}\")\n    \n    # Gaussian copula results\n    gaussian_ifm_estimates = []\n    gaussian_cml_estimates = []\n    gaussian_mle_estimates = []\n    \n    # Clayton copula results  \n    clayton_ifm_estimates = []\n    clayton_cml_estimates = []\n    \n    for sim in range(n_simulations):\n        # Generate data from Gaussian copula\n        mean = [0, 0]\n        cov = [[1, true_rho], [true_rho, 1]]\n        normal_data = np.random.multivariate_normal(mean, cov, n)\n        \n        # Transform to different margins\n        data_gaussian = np.column_stack([\n            stats.t.ppf(stats.norm.cdf(normal_data[:, 0]), df=5, scale=1.5),\n            stats.expon.ppf(stats.norm.cdf(normal_data[:, 1]), scale=2)\n        ])\n        \n        # Generate data from Clayton copula\n        # Using simple transformation method\n        u1 = np.random.uniform(0, 1, n)\n        u2 = np.random.uniform(0, 1, n)\n        \n        # Clayton copula simulation (simplified)\n        w = np.random.gamma(1/true_theta_clayton, 1, n)\n        v1 = (-np.log(u1) / w)**(-1/true_theta_clayton)\n        v2 = (-np.log(u2) / w)**(-1/true_theta_clayton)\n        \n        # Transform to different margins\n        data_clayton = np.column_stack([\n            stats.t.ppf(np.clip(v1, 0.001, 0.999), df=5, scale=1.5),\n            stats.expon.ppf(np.clip(v2, 0.001, 0.999), scale=2)\n        ])\n        \n        # Estimation for Gaussian copula\n        try:\n            estimator_gauss = CopulaEstimation(data_gaussian)\n            estimator_gauss.estimate_marginals('t')\n            \n            # IFM estimation\n            rho_ifm = estimator_gauss.estimate_gaussian_copula('ifm')[0, 1]\n            gaussian_ifm_estimates.append(rho_ifm)\n            \n            # CML estimation\n            rho_cml = estimator_gauss.estimate_gaussian_copula('cml')[0, 1]\n            gaussian_cml_estimates.append(rho_cml)\n            \n            # Pseudo-MLE (same as IFM for Gaussian copula)\n            gaussian_mle_estimates.append(rho_ifm)\n            \n        except:\n            # In case of numerical issues\n            gaussian_ifm_estimates.append(np.nan)\n            gaussian_cml_estimates.append(np.nan)\n            gaussian_mle_estimates.append(np.nan)\n        \n        # Estimation for Clayton copula\n        try:\n            estimator_clayton = CopulaEstimation(data_clayton)\n            estimator_clayton.estimate_marginals('t')\n            \n            # IFM estimation\n            theta_ifm = estimator_clayton.estimate_clayton_copula('ifm')\n            clayton_ifm_estimates.append(theta_ifm)\n            \n            # CML estimation\n            theta_cml = estimator_clayton.estimate_clayton_copula('cml')\n            clayton_cml_estimates.append(theta_cml)\n            \n        except:\n            clayton_ifm_estimates.append(np.nan)\n            clayton_cml_estimates.append(np.nan)\n    \n    # Clean up NaN values\n    gaussian_ifm_estimates = np.array(gaussian_ifm_estimates)\n    gaussian_ifm_estimates = gaussian_ifm_estimates[~np.isnan(gaussian_ifm_estimates)]\n    \n    gaussian_cml_estimates = np.array(gaussian_cml_estimates)\n    gaussian_cml_estimates = gaussian_cml_estimates[~np.isnan(gaussian_cml_estimates)]\n    \n    clayton_ifm_estimates = np.array(clayton_ifm_estimates)\n    clayton_ifm_estimates = clayton_ifm_estimates[~np.isnan(clayton_ifm_estimates)]\n    \n    clayton_cml_estimates = np.array(clayton_cml_estimates)\n    clayton_cml_estimates = clayton_cml_estimates[~np.isnan(clayton_cml_estimates)]\n    \n    # Plot histograms of estimates\n    if len(gaussian_ifm_estimates) &gt; 0:\n        axes[0, i].hist(gaussian_ifm_estimates, bins=20, alpha=0.6, label=f'IFM (n={n})', density=True)\n        axes[0, i].axvline(true_rho, color='red', linestyle='--', linewidth=2, label='True value')\n        axes[0, i].axvline(np.mean(gaussian_ifm_estimates), color='blue', linestyle='-', linewidth=2, label='Mean estimate')\n        axes[0, i].set_title(f'Gaussian Copula: IFM (n={n})')\n        axes[0, i].set_xlabel('ρ estimate')\n        axes[0, i].set_ylabel('Density')\n        axes[0, i].legend()\n        axes[0, i].grid(True, alpha=0.3)\n    \n    if len(gaussian_cml_estimates) &gt; 0:\n        axes[1, i].hist(gaussian_cml_estimates, bins=20, alpha=0.6, label=f'CML (n={n})', density=True)\n        axes[1, i].axvline(true_rho, color='red', linestyle='--', linewidth=2, label='True value')\n        axes[1, i].axvline(np.mean(gaussian_cml_estimates), color='green', linestyle='-', linewidth=2, label='Mean estimate')\n        axes[1, i].set_title(f'Gaussian Copula: CML (n={n})')\n        axes[1, i].set_xlabel('ρ estimate')\n        axes[1, i].set_ylabel('Density')\n        axes[1, i].legend()\n        axes[1, i].grid(True, alpha=0.3)\n    \n    if len(clayton_ifm_estimates) &gt; 0:\n        axes[2, i].hist(clayton_ifm_estimates, bins=20, alpha=0.6, label=f'IFM (n={n})', density=True)\n        axes[2, i].axvline(true_theta_clayton, color='red', linestyle='--', linewidth=2, label='True value')\n        axes[2, i].axvline(np.mean(clayton_ifm_estimates), color='orange', linestyle='-', linewidth=2, label='Mean estimate')\n        axes[2, i].set_title(f'Clayton Copula: IFM (n={n})')\n        axes[2, i].set_xlabel('θ estimate')\n        axes[2, i].set_ylabel('Density')\n        axes[2, i].legend()\n        axes[2, i].grid(True, alpha=0.3)\n\nplt.subplots_adjust(left=0.05, right=0.98, top=0.95, bottom=0.08, hspace=0.3, wspace=0.2)\nplt.show()\n\n\nRunning simulation for n = 100\nRunning simulation for n = 250\nRunning simulation for n = 500\nRunning simulation for n = 1000\n\n\n\n\n\n\n\n\nFigure 7: Comparison of parameter estimation methods for copulas with Monte Carlo study\n\n\n\n\n\n\n\n\n5.2 Practical Considerations\nSample Size Requirements: Copula parameter estimation typically requires larger sample sizes than univariate problems, especially for complex dependence structures. The Monte Carlo study above illustrates how estimation precision improves with sample size.\nMarginal Specification: The choice between parametric and semiparametric approaches involves a bias-variance tradeoff: - Parametric methods (IFM, MLE) are more efficient when margins are correctly specified but biased under misspecification - Semiparametric methods (CML) are more robust but potentially less efficient\nComputational Aspects: For high-dimensional problems, consider: - Stepwise estimation for vine copulas - Composite likelihood methods for large dimensions - Bayesian approaches with MCMC for complex models"
  },
  {
    "objectID": "posts/29-07-2025_copulas/index.html#sec-vine",
    "href": "posts/29-07-2025_copulas/index.html#sec-vine",
    "title": "Copulas: Theory, Applications, and Implementation in Python",
    "section": "6 Vine Copulas",
    "text": "6 Vine Copulas\nVine copulas, introduced by Bedford and Cooke (2002) and further developed by Aas et al. (2009), provide a flexible framework for modeling high-dimensional dependence structures through hierarchical decompositions of multivariate distributions.\n\n6.1 Theoretical Foundation\nThe fundamental insight behind vine copulas is that any \\(d\\)-dimensional density can be decomposed into a product of conditional densities and bivariate copulas. For a \\(d\\)-dimensional random vector \\(\\mathbf{X} = (X_1, \\ldots, X_d)\\), we can write:\n\\[f(\\mathbf{x}) = \\prod_{j=1}^d f_j(x_j) \\prod_{i=1}^{d-1} \\prod_{j=1}^{d-i} c_{j,j+i|1:(j-1)}(F(x_j|x_1, \\ldots, x_{j-1}), F(x_{j+i}|x_1, \\ldots, x_{j-1}))\\]\nwhere \\(c_{j,k|D}\\) denotes the bivariate copula density of variables \\(j\\) and \\(k\\) conditional on the set \\(D\\).\n\n\n6.2 Regular Vine Structures\nRegular vines (R-vines) provide a systematic way to organize this decomposition using graph theory.\n\n6.2.1 Canonical Vine (C-vine)\nIn a C-vine, each tree has a star structure with one variable acting as the central node:\n\nTree 1: One variable (root) is connected to all others\nTree 2: One of the remaining variables becomes the new root\n\nTree \\(j\\): Similar star structure with conditioning sets of size \\(j-1\\)\n\nMathematical representation for a 4-dimensional C-vine: \\[f(x_1, x_2, x_3, x_4) = f_1(x_1) f_2(x_2) f_3(x_3) f_4(x_4) \\times\\] \\[c_{1,2}(u_1, u_2) c_{1,3}(u_1, u_3) c_{1,4}(u_1, u_4) \\times\\] \\[c_{2,3|1}(u_{2|1}, u_{3|1}) c_{2,4|1}(u_{2|1}, u_{4|1}) \\times\\] \\[c_{3,4|1,2}(u_{3|1,2}, u_{4|1,2})\\]\n\n\n6.2.2 Drawable Vine (D-vine)\nD-vines have a path structure where variables are arranged sequentially:\n\nTree 1: Variables form a path \\((1-2-3-4-\\ldots)\\)\nTree 2: Connect variables with one intermediate node\nTree \\(j\\): Connect variables with \\(j\\) intermediate nodes\n\nMathematical representation for a 4-dimensional D-vine: \\[f(x_1, x_2, x_3, x_4) = f_1(x_1) f_2(x_2) f_3(x_3) f_4(x_4) \\times\\] \\[c_{1,2}(u_1, u_2) c_{2,3}(u_2, u_3) c_{3,4}(u_3, u_4) \\times\\] \\[c_{1,3|2}(u_{1|2}, u_{3|2}) c_{2,4|3}(u_{2|3}, u_{4|3}) \\times\\] \\[c_{1,4|2,3}(u_{1|2,3}, u_{4|2,3})\\]\n\n\n\n6.3 Construction and Estimation\n\n\nCode\nclass VineCopula:\n    \"\"\"Implementation of vine copula construction and estimation\"\"\"\n    \n    def __init__(self, data, vine_type='cvine'):\n        self.data = data\n        self.n, self.d = data.shape\n        self.vine_type = vine_type\n        self.trees = []\n        self.copula_params = {}\n        \n    def transform_to_uniform(self):\n        \"\"\"Transform data to uniform margins using empirical CDFs\"\"\"\n        self.uniform_data = np.zeros_like(self.data)\n        for j in range(self.d):\n            ranks = stats.rankdata(self.data[:, j])\n            self.uniform_data[:, j] = ranks / (self.n + 1)\n    \n    def estimate_bivariate_copula(self, u1, u2, copula_family='gaussian'):\n        \"\"\"Estimate parameters for a bivariate copula\"\"\"\n        if copula_family == 'gaussian':\n            # Transform to normal space\n            z1 = stats.norm.ppf(np.clip(u1, 0.001, 0.999))\n            z2 = stats.norm.ppf(np.clip(u2, 0.001, 0.999))\n            rho = np.corrcoef(z1, z2)[0, 1]\n            return {'family': 'gaussian', 'rho': rho}\n        \n        elif copula_family == 'clayton':\n            # Kendall's tau method\n            tau = stats.kendalltau(u1, u2)[0]\n            if tau &gt; 0:\n                theta = 2 * tau / (1 - tau)\n            else:\n                theta = 0.1\n            return {'family': 'clayton', 'theta': max(theta, 0.1)}\n        \n        elif copula_family == 'gumbel':\n            # Kendall's tau method\n            tau = stats.kendalltau(u1, u2)[0]\n            if tau &gt; 0:\n                theta = 1 / (1 - tau)\n            else:\n                theta = 1.1\n            return {'family': 'gumbel', 'theta': max(theta, 1.01)}\n        \n        elif copula_family == 'frank':\n            # Kendall's tau method (approximation)\n            tau = stats.kendalltau(u1, u2)[0]\n            if abs(tau) &gt; 0.001:\n                # Approximate inverse of tau-theta relationship for Frank copula\n                theta = 4 * tau  # Simplified approximation\n            else:\n                theta = 0.1\n            return {'family': 'frank', 'theta': theta}\n    \n    def conditional_cdf(self, u1, u2, copula_params):\n        \"\"\"Compute conditional CDF h(u1|u2) = ∂C(u1,u2)/∂u2\"\"\"\n        family = copula_params['family']\n        \n        if family == 'gaussian':\n            rho = copula_params['rho']\n            z1 = stats.norm.ppf(np.clip(u1, 0.001, 0.999))\n            z2 = stats.norm.ppf(np.clip(u2, 0.001, 0.999))\n            \n            # Conditional distribution for bivariate normal\n            conditional_mean = rho * z2\n            conditional_std = np.sqrt(1 - rho**2)\n            h = stats.norm.cdf((z1 - conditional_mean) / conditional_std)\n            \n        elif family == 'clayton':\n            theta = copula_params['theta']\n            if theta &gt; 0:\n                h = u2**(-theta-1) * (u1**(-theta) + u2**(-theta) - 1)**(-1/theta - 1)\n            else:\n                h = u1  # Independence case\n                \n        elif family == 'frank':\n            theta = copula_params['theta']\n            if abs(theta) &gt; 0.001:\n                exp_theta_u1 = np.exp(-theta * u1)\n                exp_theta_u2 = np.exp(-theta * u2)\n                exp_theta = np.exp(-theta)\n                \n                numerator = (exp_theta_u2 - 1) * exp_theta_u1\n                denominator = (exp_theta - 1) + (exp_theta_u1 - 1) * (exp_theta_u2 - 1)\n                h = numerator / denominator\n            else:\n                h = u1  # Independence case\n        else:\n            h = u1  # Default to independence\n            \n        return np.clip(h, 0.001, 0.999)\n    \n    def fit_cvine(self):\n        \"\"\"Fit a canonical vine copula\"\"\"\n        if not hasattr(self, 'uniform_data'):\n            self.transform_to_uniform()\n            \n        # Storage for vine structure\n        current_data = self.uniform_data.copy()\n        \n        # Tree construction\n        for tree_level in range(self.d - 1):\n            print(f\"Constructing tree {tree_level + 1}\")\n            \n            tree_edges = []\n            tree_copulas = {}\n            new_data = {}\n            \n            if tree_level == 0:\n                # First tree: star structure around variable 0\n                root_var = 0\n                \n                for j in range(1, self.d):\n                    # Estimate copula between root and variable j\n                    u1 = current_data[:, root_var]\n                    u2 = current_data[:, j]\n                    \n                    # Select best copula family (simplified selection)\n                    copula_params = self.select_best_copula(u1, u2)\n                    \n                    edge = (root_var, j, frozenset())  # (var1, var2, conditioning_set)\n                    tree_edges.append(edge)\n                    tree_copulas[edge] = copula_params\n                \n            else:\n                # Higher trees: condition on previous variables (simplified)\n                # For demonstration, only construct second tree\n                if tree_level == 1:\n                    # Second tree with some conditional dependence\n                    for j in range(2, self.d):\n                        if j &lt; self.d:\n                            # Get conditional data from first tree\n                            prev_edge1 = (0, 1, frozenset())\n                            prev_edge2 = (0, j, frozenset())\n                            \n                            if prev_edge1 in self.trees[0]['copulas'] and prev_edge2 in self.trees[0]['copulas']:\n                                u1_cond = self.conditional_cdf(\n                                    current_data[:, 1], \n                                    current_data[:, 0],\n                                    self.trees[0]['copulas'][prev_edge1]\n                                )\n                                u2_cond = self.conditional_cdf(\n                                    current_data[:, j], \n                                    current_data[:, 0],\n                                    self.trees[0]['copulas'][prev_edge2]\n                                )\n                                \n                                # Estimate conditional copula\n                                copula_params = self.select_best_copula(u1_cond, u2_cond)\n                                \n                                edge = (1, j, frozenset([0]))\n                                tree_edges.append(edge)\n                                tree_copulas[edge] = copula_params\n            \n            # Store tree information\n            tree_info = {\n                'level': tree_level,\n                'edges': tree_edges,\n                'copulas': tree_copulas\n            }\n            self.trees.append(tree_info)\n            \n            # For simplicity, only fit first two trees\n            if tree_level &gt;= 1:\n                break\n    \n    def select_best_copula(self, u1, u2):\n        \"\"\"Select best copula family based on AIC\"\"\"\n        families = ['gaussian', 'clayton', 'frank']\n        best_aic = np.inf\n        best_params = None\n        \n        for family in families:\n            try:\n                params = self.estimate_bivariate_copula(u1, u2, family)\n                aic = self.compute_aic(u1, u2, params)\n                \n                if aic &lt; best_aic:\n                    best_aic = aic\n                    best_params = params\n            except:\n                continue\n                \n        return best_params if best_params else {'family': 'gaussian', 'rho': 0.1}\n    \n    def compute_aic(self, u1, u2, copula_params):\n        \"\"\"Compute AIC for copula model (simplified)\"\"\"\n        try:\n            log_likelihood = self.log_likelihood(u1, u2, copula_params)\n            n_params = 1  # Assuming single parameter copulas\n            return -2 * log_likelihood + 2 * n_params\n        except:\n            return np.inf\n    \n    def log_likelihood(self, u1, u2, copula_params):\n        \"\"\"Compute log-likelihood for bivariate copula\"\"\"\n        family = copula_params['family']\n        \n        if family == 'gaussian':\n            rho = copula_params['rho']\n            z1 = stats.norm.ppf(np.clip(u1, 0.001, 0.999))\n            z2 = stats.norm.ppf(np.clip(u2, 0.001, 0.999))\n            \n            # Gaussian copula density\n            density = (1 - rho**2)**(-0.5) * np.exp(\n                -0.5 * rho * (2*rho*z1*z2 - rho**2*(z1**2 + z2**2)) / (1 - rho**2)\n            )\n            return np.sum(np.log(np.clip(density, 1e-10, np.inf)))\n        \n        else:\n            # Simplified: return correlation-based pseudo-likelihood\n            tau = stats.kendalltau(u1, u2)[0]\n            return self.n * np.log(max(abs(tau), 0.01))\n\n# Demonstration with synthetic 4-dimensional data\nnp.random.seed(42)\nn = 500\n\n# Generate 4-dimensional data with known vine structure\n# True C-vine structure: 1 as root, then dependencies through vine\n\n# Generate base normal data with specific correlation structure\ntrue_corr = np.array([\n    [1.0, 0.7, 0.5, 0.3],\n    [0.7, 1.0, 0.4, 0.2], \n    [0.5, 0.4, 1.0, 0.6],\n    [0.3, 0.2, 0.6, 1.0]\n])\n\n# Generate multivariate normal data\nmvn_data = np.random.multivariate_normal(np.zeros(4), true_corr, n)\n\n# Transform to different marginal distributions\nvine_data = np.column_stack([\n    stats.norm.ppf(stats.norm.cdf(mvn_data[:, 0])),  # Keep normal\n    stats.t.ppf(stats.norm.cdf(mvn_data[:, 1]), df=5),  # t-distribution  \n    stats.expon.ppf(stats.norm.cdf(mvn_data[:, 2])),  # Exponential\n    stats.gamma.ppf(stats.norm.cdf(mvn_data[:, 3]), a=2)  # Gamma\n])\n\n# Fit vine copula\nvine_model = VineCopula(vine_data, vine_type='cvine')\nvine_model.fit_cvine()\n\n# Create visualization\nfig, axes = plt.subplots(4, 5, figsize=(25, 20))\n\n# Original data pairwise plots\nvar_names = ['Normal', 't(5)', 'Exponential', 'Gamma']\nfor i in range(4):\n    for j in range(4):\n        if i != j:\n            axes[i, j].scatter(vine_data[:, j], vine_data[:, i], alpha=0.6, s=15)\n            axes[i, j].set_xlabel(f'Variable {j+1} ({var_names[j]})')\n            axes[i, j].set_ylabel(f'Variable {i+1} ({var_names[i]})')\n        else:\n            axes[i, j].hist(vine_data[:, i], bins=30, alpha=0.7, density=True)\n            axes[i, j].set_title(f'Variable {i+1} ({var_names[i]})')\n        axes[i, j].grid(True, alpha=0.3)\n\n# Vine structure visualization\nax_vine = axes[0, 4]\nax_vine.set_xlim(0, 4)\nax_vine.set_ylim(0, 4)\nax_vine.set_title('C-Vine Structure (Tree 1)')\n\n# Draw nodes\nnode_positions = {0: (1, 2), 1: (2, 3), 2: (2, 1), 3: (3, 2)}\nfor node, (x, y) in node_positions.items():\n    circle = plt.Circle((x, y), 0.2, color='lightblue', ec='black')\n    ax_vine.add_patch(circle)\n    ax_vine.text(x, y, str(node+1), ha='center', va='center', fontweight='bold')\n\n# Draw edges for tree 1 (star structure with node 0 as root)\nroot = 0\nroot_pos = node_positions[root]\nfor leaf in [1, 2, 3]:\n    leaf_pos = node_positions[leaf]\n    ax_vine.plot([root_pos[0], leaf_pos[0]], [root_pos[1], leaf_pos[1]], \n                'k-', linewidth=2)\n\nax_vine.set_aspect('equal')\nax_vine.axis('off')\n\n# Copula parameters summary\nax_summary = axes[1, 4]\nax_summary.axis('off')\nax_summary.text(0.1, 0.9, 'Vine Copula Summary', fontsize=14, fontweight='bold',\n                transform=ax_summary.transAxes)\n\nsummary_text = f\"Data: {n} observations, {4} variables\\n\"\nsummary_text += f\"Vine type: C-vine\\n\"\nsummary_text += f\"Trees fitted: {len(vine_model.trees)}\\n\\n\"\n\nif vine_model.trees:\n    summary_text += \"Tree 1 Copulas:\\n\"\n    for edge, params in vine_model.trees[0]['copulas'].items():\n        summary_text += f\"  {edge[0]+1}-{edge[1]+1}: {params['family']}\\n\"\n\nax_summary.text(0.1, 0.7, summary_text, fontsize=10, fontfamily='monospace',\n                transform=ax_summary.transAxes, verticalalignment='top')\n\n# Empirical vs theoretical copula comparison (for first tree)\nif vine_model.trees and hasattr(vine_model, 'uniform_data'):\n    uniform_data = vine_model.uniform_data\n    \n    # Plot empirical copula for variables 1 and 2\n    ax_emp = axes[2, 4]\n    ax_emp.scatter(uniform_data[:, 0], uniform_data[:, 1], alpha=0.6, s=15)\n    ax_emp.set_xlabel('U₁')\n    ax_emp.set_ylabel('U₂')\n    ax_emp.set_title('Empirical Copula (Variables 1-2)')\n    ax_emp.grid(True, alpha=0.3)\n    \n    # Model diagnostics\n    ax_diag = axes[3, 4]\n    \n    # Compute pairwise Kendall's tau\n    taus = []\n    for i in range(4):\n        for j in range(i+1, 4):\n            tau = stats.kendalltau(uniform_data[:, i], uniform_data[:, j])[0]\n            taus.append(tau)\n    \n    ax_diag.bar(range(len(taus)), taus, alpha=0.7)\n    ax_diag.set_xlabel('Variable Pairs')\n    ax_diag.set_ylabel('Kendall\\'s τ')\n    ax_diag.set_title('Pairwise Dependence')\n    ax_diag.set_xticks(range(len(taus)))\n    ax_diag.set_xticklabels(['1-2', '1-3', '1-4', '2-3', '2-4', '3-4'], rotation=45)\n    ax_diag.grid(True, alpha=0.3)\n\nplt.subplots_adjust(left=0.05, right=0.98, top=0.95, bottom=0.08, hspace=0.4, wspace=0.3)\nplt.show()\n\n\nConstructing tree 1\nConstructing tree 2\n\n\n\n\n\n\n\n\nFigure 8: Vine copula construction: C-vine and D-vine structures with estimation algorithms\n\n\n\n\n\n\n\n6.4 Model Selection and Specification\nVine copula modeling involves several specification choices:\n\n6.4.1 1. Vine Structure Selection\n\nC-vine: Suitable when one variable has strong dependence with all others\nD-vine: Appropriate for sequential or ordered data\nRegular vine: General structure selected based on dependence strength\n\n\n\n6.4.2 2. Copula Family Selection\nFor each bivariate copula in the vine, select among: - Gaussian: Symmetric dependence, no tail dependence - Student’s t: Symmetric with tail dependence - Clayton: Lower tail dependence - Gumbel: Upper tail dependence - Frank: Symmetric, no tail dependence\n\n\n6.4.3 3. Sequential Estimation Algorithm\n\nTransform to uniform margins using empirical CDFs or parametric fits\nFor each tree level \\(k = 1, \\ldots, d-1\\):\n\nIdentify edge set based on vine structure\nFor each edge \\((i,j|D)\\) with conditioning set \\(D\\):\n\nCompute conditional data \\(u_{i|D}\\) and \\(u_{j|D}\\)\nSelect optimal bivariate copula family\nEstimate copula parameters\nCompute conditional distributions for next tree\n\n\n\n\n\n\n6.5 Applications and Extensions\nHigh-Dimensional Modeling: Vine copulas scale to high dimensions while maintaining interpretability. They have been successfully applied to portfolios with 50+ assets.\nDynamic Vines: Extensions allowing time-varying copula parameters and structures (Patton 2006).\nTruncated Vines: Simplify high-dimensional vines by setting higher-tree copulas to independence.\nBayesian Vines: Use Bayesian methods for parameter uncertainty quantification and model selection.\nModel Validation: Use techniques like: - Probability integral transforms for goodness-of-fit testing - Cross-validation for structure selection - Rosenblatt transforms for multivariate model checking"
  },
  {
    "objectID": "posts/29-07-2025_copulas/index.html#sec-applications",
    "href": "posts/29-07-2025_copulas/index.html#sec-applications",
    "title": "Copulas: Theory, Applications, and Implementation in Python",
    "section": "7 Applications in Finance and Risk Management",
    "text": "7 Applications in Finance and Risk Management\nThis section demonstrates practical applications of copulas in portfolio risk management and spatial data analysis, showcasing their power in real-world scenarios.\n\n7.1 Portfolio Risk Management and Value-at-Risk\nPortfolio risk management represents one of the most successful applications of copula theory in quantitative finance. Traditional approaches often assume multivariate normality or rely on linear correlation measures, which fail to capture the complex dependence patterns observed in financial markets during stress periods.\nThe Challenge: Financial assets exhibit several stylized facts that complicate risk modeling: - Fat tails: Return distributions have heavier tails than the normal distribution - Asymmetric dependence: Correlations increase during market downturns (contagion effect) - Tail dependence: Extreme losses tend to occur simultaneously across assets - Time-varying correlations: Dependence structures evolve over time\nThe Copula Solution: By separating marginal behavior from dependence structure, copulas allow risk managers to: 1. Model each asset’s return distribution using appropriate heavy-tailed distributions (Student’s t, skewed distributions) 2. Capture realistic dependence patterns through flexible copula functions 3. Generate scenarios that preserve both marginal characteristics and joint behavior 4. Compute accurate tail risk measures like Value-at-Risk (VaR) and Expected Shortfall (ES)\nThe following implementation demonstrates a comprehensive copula-based risk management framework, comparing Gaussian and Student’s t copulas for portfolio VaR estimation:\n\n\nCode\nclass PortfolioRiskAnalyzer:\n    \"\"\"Comprehensive portfolio risk analysis using copulas\"\"\"\n    \n    def __init__(self, returns_data, weights=None):\n        self.returns = returns_data\n        self.n_assets = returns_data.shape[1]\n        self.weights = weights if weights is not None else np.ones(self.n_assets) / self.n_assets\n        \n    def fit_marginals(self, distribution='t'):\n        \"\"\"Fit marginal distributions to return data\"\"\"\n        self.marginal_params = []\n        self.marginal_cdfs = []\n        \n        for i in range(self.n_assets):\n            data = self.returns[:, i]\n            \n            if distribution == 't':\n                # Fit Student's t distribution\n                params = stats.t.fit(data)\n                self.marginal_params.append(params)\n                self.marginal_cdfs.append(lambda x, p=params: stats.t.cdf(x, *p))\n            elif distribution == 'normal':\n                # Fit normal distribution\n                params = stats.norm.fit(data)\n                self.marginal_params.append(params)\n                self.marginal_cdfs.append(lambda x, p=params: stats.norm.cdf(x, *p))\n                \n    def transform_to_uniform(self):\n        \"\"\"Transform returns to uniform margins using fitted distributions\"\"\"\n        self.uniform_data = np.zeros_like(self.returns)\n        \n        for i in range(self.n_assets):\n            if hasattr(self, 'marginal_params'):\n                params = self.marginal_params[i]\n                self.uniform_data[:, i] = stats.t.cdf(self.returns[:, i], *params)\n            else:\n                # Use empirical CDF\n                self.uniform_data[:, i] = stats.rankdata(self.returns[:, i]) / (len(self.returns) + 1)\n                \n    def estimate_copula_parameters(self, copula_type='gaussian'):\n        \"\"\"Estimate copula parameters from uniform data\"\"\"\n        if not hasattr(self, 'uniform_data'):\n            self.transform_to_uniform()\n            \n        if copula_type == 'gaussian':\n            # Transform to normal and estimate correlation\n            normal_data = stats.norm.ppf(np.clip(self.uniform_data, 0.001, 0.999))\n            self.copula_params = np.corrcoef(normal_data.T)\n            self.copula_type = 'gaussian'\n            \n        elif copula_type == 't':\n            # Estimate t copula parameters (simplified)\n            normal_data = stats.norm.ppf(np.clip(self.uniform_data, 0.001, 0.999))\n            self.copula_params = {\n                'correlation': np.corrcoef(normal_data.T),\n                'df': 5  # Fixed for simplicity, could be estimated\n            }\n            self.copula_type = 't'\n            \n    def simulate_portfolio_returns(self, n_simulations=10000, copula_type=None):\n        \"\"\"Simulate portfolio returns using fitted copula\"\"\"\n        if copula_type is None:\n            copula_type = getattr(self, 'copula_type', 'gaussian')\n            \n        # Generate copula samples\n        if copula_type == 'gaussian':\n            # Generate from multivariate normal copula\n            corr_matrix = self.copula_params\n            normal_samples = np.random.multivariate_normal(\n                np.zeros(self.n_assets), corr_matrix, n_simulations\n            )\n            uniform_samples = stats.norm.cdf(normal_samples)\n            \n        elif copula_type == 't':\n            # Generate from multivariate t copula\n            corr_matrix = self.copula_params['correlation']\n            df = self.copula_params['df']\n            t_samples = np.random.multivariate_normal(\n                np.zeros(self.n_assets), corr_matrix, n_simulations\n            )\n            # Apply t distribution transformation (simplified)\n            chi2_samples = np.random.chisquare(df, n_simulations)\n            t_samples = t_samples * np.sqrt(df / chi2_samples[:, np.newaxis])\n            uniform_samples = stats.t.cdf(t_samples, df)\n            \n        # Transform back to return space using inverse marginal CDFs\n        simulated_returns = np.zeros_like(uniform_samples)\n        for i in range(self.n_assets):\n            if hasattr(self, 'marginal_params'):\n                params = self.marginal_params[i]\n                simulated_returns[:, i] = stats.t.ppf(uniform_samples[:, i], *params)\n            else:\n                # Use empirical quantiles\n                quantiles = np.percentile(self.returns[:, i], uniform_samples[:, i] * 100)\n                simulated_returns[:, i] = quantiles\n                \n        # Calculate portfolio returns\n        portfolio_returns = np.dot(simulated_returns, self.weights)\n        return portfolio_returns, simulated_returns\n    \n    def calculate_var_es(self, portfolio_returns, confidence_levels=[0.95, 0.99]):\n        \"\"\"Calculate Value-at-Risk and Expected Shortfall\"\"\"\n        results = {}\n        for alpha in confidence_levels:\n            var = np.percentile(portfolio_returns, (1 - alpha) * 100)\n            es = np.mean(portfolio_returns[portfolio_returns &lt;= var])\n            results[alpha] = {'VaR': var, 'ES': es}\n        return results\n\n# Generate synthetic financial data with realistic properties\nnp.random.seed(42)\nn_days = 1000\nn_assets = 4\n\n# Asset names\nasset_names = ['Tech Stock', 'Bank Stock', 'Commodity', 'Bond']\n\n# Generate correlated returns with different characteristics\nbase_corr = 0.3\ncorrelations = np.array([\n    [1.0, 0.4, 0.2, -0.1],\n    [0.4, 1.0, 0.3, 0.0],\n    [0.2, 0.3, 1.0, -0.2],\n    [-0.1, 0.0, -0.2, 1.0]\n])\n\n# Generate base normal returns\nbase_returns = np.random.multivariate_normal(np.zeros(n_assets), correlations, n_days)\n\n# Transform to different marginal distributions\nreturns_data = np.zeros_like(base_returns)\nreturns_data[:, 0] = base_returns[:, 0] * 0.02 + 0.0005  # Tech: higher volatility\nreturns_data[:, 1] = stats.t.ppf(stats.norm.cdf(base_returns[:, 1]), df=4) * 0.015 + 0.0003  # Bank: fat tails\nreturns_data[:, 2] = base_returns[:, 2] * 0.025 + 0.0002  # Commodity: high volatility\nreturns_data[:, 3] = base_returns[:, 3] * 0.005 + 0.0001  # Bond: low volatility\n\n# Equal weights portfolio\nweights = np.array([0.25, 0.25, 0.25, 0.25])\n\n# Initialize analyzer\nanalyzer = PortfolioRiskAnalyzer(returns_data, weights)\nanalyzer.fit_marginals('t')\nanalyzer.transform_to_uniform()\n\n# Compare different copula models\ncopula_models = ['gaussian', 't']\nresults = {}\n\nfig, axes = plt.subplots(4, 5, figsize=(25, 20))\n\n# Historical returns analysis\nfor i, asset in enumerate(asset_names):\n    axes[0, i].hist(returns_data[:, i], bins=50, alpha=0.7, density=True)\n    axes[0, i].set_title(f'{asset} Returns')\n    axes[0, i].set_xlabel('Return')\n    axes[0, i].set_ylabel('Density')\n    axes[0, i].grid(True)\n\n# Portfolio returns comparison\nhistorical_portfolio = np.dot(returns_data, weights)\naxes[0, 4].hist(historical_portfolio, bins=50, alpha=0.7, density=True, label='Historical')\naxes[0, 4].set_title('Portfolio Returns')\naxes[0, 4].set_xlabel('Return')\naxes[0, 4].set_ylabel('Density')\naxes[0, 4].legend()\naxes[0, 4].grid(True)\n\n# Copula comparison\nsimulation_results = {}\nfor j, copula_type in enumerate(copula_models):\n    analyzer.estimate_copula_parameters(copula_type)\n    portfolio_sim, returns_sim = analyzer.simulate_portfolio_returns(10000, copula_type)\n    simulation_results[copula_type] = portfolio_sim\n    \n    # Portfolio distribution comparison\n    axes[1, j].hist(historical_portfolio, bins=50, alpha=0.5, density=True, \n                   label='Historical', color='blue')\n    axes[1, j].hist(portfolio_sim, bins=50, alpha=0.5, density=True, \n                   label=f'{copula_type.title()} Copula', color='red')\n    axes[1, j].set_title(f'Portfolio: {copula_type.title()} Copula')\n    axes[1, j].set_xlabel('Return')\n    axes[1, j].set_ylabel('Density')\n    axes[1, j].legend()\n    axes[1, j].grid(True)\n    \n    # VaR calculation\n    var_results = analyzer.calculate_var_es(portfolio_sim)\n    results[copula_type] = var_results\n    \n    # Q-Q plot for tail analysis\n    q_theoretical = np.percentile(portfolio_sim, np.linspace(1, 99, 99))\n    q_historical = np.percentile(historical_portfolio, np.linspace(1, 99, 99))\n    axes[2, j].plot(q_theoretical, q_historical, 'o', alpha=0.6)\n    axes[2, j].plot([q_theoretical.min(), q_theoretical.max()], \n                   [q_theoretical.min(), q_theoretical.max()], 'r--')\n    axes[2, j].set_title(f'Q-Q Plot: {copula_type.title()} vs Historical')\n    axes[2, j].set_xlabel('Simulated Quantiles')\n    axes[2, j].set_ylabel('Historical Quantiles')\n    axes[2, j].grid(True)\n\n# VaR comparison\nconfidence_levels = [0.95, 0.99]\nvar_comparison = pd.DataFrame(index=copula_models, \n                             columns=[f'VaR_{int(cl*100)}%' for cl in confidence_levels] + \n                                    [f'ES_{int(cl*100)}%' for cl in confidence_levels])\n\nfor copula in copula_models:\n    for cl in confidence_levels:\n        var_comparison.loc[copula, f'VaR_{int(cl*100)}%'] = results[copula][cl]['VaR']\n        var_comparison.loc[copula, f'ES_{int(cl*100)}%'] = results[copula][cl]['ES']\n\n# Historical VaR for comparison\nhistorical_var_95 = np.percentile(historical_portfolio, 5)\nhistorical_var_99 = np.percentile(historical_portfolio, 1)\nhistorical_es_95 = np.mean(historical_portfolio[historical_portfolio &lt;= historical_var_95])\nhistorical_es_99 = np.mean(historical_portfolio[historical_portfolio &lt;= historical_var_99])\n\n# Bar plot of VaR estimates\nx_pos = np.arange(len(copula_models))\nwidth = 0.35\n\naxes[1, 2].bar(x_pos - width/2, [results[c][0.95]['VaR'] for c in copula_models], \n              width, label='VaR 95%', alpha=0.7)\naxes[1, 2].bar(x_pos + width/2, [results[c][0.99]['VaR'] for c in copula_models], \n              width, label='VaR 99%', alpha=0.7)\naxes[1, 2].axhline(y=historical_var_95, color='r', linestyle='--', label='Historical VaR 95%')\naxes[1, 2].axhline(y=historical_var_99, color='orange', linestyle='--', label='Historical VaR 99%')\naxes[1, 2].set_title('VaR Comparison')\naxes[1, 2].set_xlabel('Copula Model')\naxes[1, 2].set_ylabel('VaR')\naxes[1, 2].set_xticks(x_pos)\naxes[1, 2].set_xticklabels([c.title() for c in copula_models])\naxes[1, 2].legend()\naxes[1, 2].grid(True)\n\n# Tail dependence visualization\nfor j, copula_type in enumerate(copula_models):\n    portfolio_sim = simulation_results[copula_type]\n    \n    # Extreme value analysis\n    threshold_95 = np.percentile(portfolio_sim, 5)  # Lower 5% (bad returns)\n    extreme_returns = portfolio_sim[portfolio_sim &lt;= threshold_95]\n    \n    axes[2, j+2].hist(extreme_returns, bins=30, alpha=0.7, density=True)\n    axes[2, j+2].axvline(x=results[copula_type][0.99]['VaR'], color='r', \n                        linestyle='--', label='VaR 99%')\n    axes[2, j+2].axvline(x=results[copula_type][0.99]['ES'], color='orange', \n                        linestyle='--', label='ES 99%')\n    axes[2, j+2].set_title(f'Tail Analysis: {copula_type.title()}')\n    axes[2, j+2].set_xlabel('Extreme Returns')\n    axes[2, j+2].set_ylabel('Density')\n    axes[2, j+2].legend()\n    axes[2, j+2].grid(True)\n\n# Risk decomposition by asset\nfor i, asset in enumerate(asset_names):\n    # Marginal contribution to portfolio risk\n    asset_contribution = []\n    for copula_type in copula_models:\n        analyzer.estimate_copula_parameters(copula_type)\n        portfolio_sim, returns_sim = analyzer.simulate_portfolio_returns(5000, copula_type)\n        \n        # Calculate asset's contribution to portfolio VaR\n        var_95 = np.percentile(portfolio_sim, 5)\n        tail_indices = portfolio_sim &lt;= var_95\n        asset_contrib = np.mean(returns_sim[tail_indices, i] * weights[i])\n        asset_contribution.append(asset_contrib)\n    \n    axes[3, i].bar(copula_models, asset_contribution, alpha=0.7)\n    axes[3, i].set_title(f'{asset} Risk Contribution')\n    axes[3, i].set_xlabel('Copula Model')\n    axes[3, i].set_ylabel('Contribution to VaR')\n    axes[3, i].grid(True)\n\n# Summary statistics table\nsummary_stats = pd.DataFrame({\n    'Historical': [historical_portfolio.mean(), historical_portfolio.std(), \n                  stats.skew(historical_portfolio), stats.kurtosis(historical_portfolio)],\n    'Gaussian Copula': [simulation_results['gaussian'].mean(), simulation_results['gaussian'].std(),\n                       stats.skew(simulation_results['gaussian']), stats.kurtosis(simulation_results['gaussian'])],\n    't Copula': [simulation_results['t'].mean(), simulation_results['t'].std(),\n                stats.skew(simulation_results['t']), stats.kurtosis(simulation_results['t'])]\n}, index=['Mean', 'Std Dev', 'Skewness', 'Kurtosis'])\n\n# Create text plot for summary\naxes[3, 4].axis('off')\ntable_text = summary_stats.round(4).to_string()\naxes[3, 4].text(0.1, 0.9, 'Portfolio Statistics', transform=axes[3, 4].transAxes, \n                fontsize=14, fontweight='bold')\naxes[3, 4].text(0.1, 0.1, table_text, transform=axes[3, 4].transAxes, \n                fontsize=10, fontfamily='monospace')\n\nplt.subplots_adjust(left=0.05, right=0.98, top=0.95, bottom=0.05, hspace=0.4, wspace=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 9: Portfolio risk analysis using copulas: VaR estimation and tail risk assessment\n\n\n\n\n\n\n\n7.2 Spatial Data Analysis with Copulas\nSpatial data analysis presents unique challenges that make copulas particularly valuable. Traditional geostatistical methods like kriging assume Gaussian distributions and rely on the variogram to model spatial dependence. However, many environmental and geological phenomena exhibit non-Gaussian marginal distributions and complex spatial dependence structures that cannot be adequately captured by classical approaches.\nSpatial Challenges Addressed by Copulas: - Marginal heterogeneity: Different locations may have different marginal distributions due to local environmental conditions - Non-linear spatial dependence: The relationship between observations may not be captured by linear correlation - Scale effects: Dependence patterns may vary at different spatial scales - Multivariate spatial fields: Multiple correlated variables observed at the same locations\nAdvantages of Spatial Copulas: 1. Flexible marginal modeling: Each location can have its own marginal distribution while preserving spatial structure 2. Non-parametric dependence: Capture complex spatial relationships beyond exponential or spherical models 3. Simulation capabilities: Generate realistic spatial fields that preserve both marginal properties and spatial correlation 4. Multivariate extension: Model multiple correlated spatial processes simultaneously\nApplications: Spatial copulas are used in: - Environmental monitoring: Modeling precipitation, temperature, and pollution levels - Hydrology: Rainfall-runoff modeling and flood risk assessment - Geology: Mineral resource estimation and geological property mapping - Ecology: Species distribution and habitat modeling - Climate science: Regional climate model validation and downscaling\nThe following example demonstrates spatial copula analysis using synthetic environmental data, showing how to model spatial dependence while preserving realistic marginal distributions:\n\n\nCode\nclass SpatialCopulaAnalysis:\n    \"\"\"Spatial copula analysis for environmental data\"\"\"\n    \n    def __init__(self, coordinates, observations):\n        self.coords = coordinates\n        self.obs = observations\n        self.n_locations = len(coordinates)\n        \n    def calculate_distance_matrix(self):\n        \"\"\"Calculate pairwise distances between locations\"\"\"\n        from scipy.spatial.distance import pdist, squareform\n        distances = pdist(self.coords)\n        self.distance_matrix = squareform(distances)\n        \n    def fit_spatial_copula(self, copula_family='gaussian', max_distance=None):\n        \"\"\"Fit spatial copula model\"\"\"\n        if not hasattr(self, 'distance_matrix'):\n            self.calculate_distance_matrix()\n            \n        # Transform observations to uniform margins\n        self.uniform_obs = np.zeros_like(self.obs)\n        for i in range(self.obs.shape[1]):\n            ranks = stats.rankdata(self.obs[:, i])\n            self.uniform_obs[:, i] = ranks / (len(ranks) + 1)\n            \n        # Estimate distance-based dependence\n        if max_distance is None:\n            max_distance = np.max(self.distance_matrix) / 3\n            \n        self.distance_bins = np.linspace(0, max_distance, 10)\n        self.empirical_dependence = []\n        \n        for i in range(len(self.distance_bins) - 1):\n            d_min, d_max = self.distance_bins[i], self.distance_bins[i+1]\n            \n            # Find pairs within distance range\n            mask = (self.distance_matrix &gt;= d_min) & (self.distance_matrix &lt; d_max)\n            pairs = np.where(mask)\n            \n            if len(pairs[0]) &gt; 0:\n                # Calculate empirical copula dependence\n                u1_pairs = self.uniform_obs[pairs[0], 0]\n                u2_pairs = self.uniform_obs[pairs[1], 0]  # Assuming single variable for simplicity\n                \n                # Kendall's tau\n                tau = stats.kendalltau(u1_pairs, u2_pairs)[0]\n                self.empirical_dependence.append(tau)\n            else:\n                self.empirical_dependence.append(0)\n                \n    def simulate_spatial_field(self, n_simulations=1000):\n        \"\"\"Simulate spatial random field using fitted copula\"\"\"\n        # Simple spatial simulation using distance-based correlation\n        if not hasattr(self, 'distance_matrix'):\n            self.calculate_distance_matrix()\n            \n        # Create correlation matrix based on distance\n        correlation_matrix = np.exp(-self.distance_matrix / 50)  # Exponential decay\n        \n        # Generate from multivariate normal copula\n        simulated_fields = []\n        for _ in range(n_simulations):\n            normal_field = np.random.multivariate_normal(\n                np.zeros(self.n_locations), correlation_matrix\n            )\n            uniform_field = stats.norm.cdf(normal_field)\n            \n            # Transform back to original margins (approximate)\n            original_field = np.zeros_like(uniform_field)\n            for i in range(len(uniform_field)):\n                original_field[i] = np.percentile(self.obs[:, 0], uniform_field[i] * 100)\n                \n            simulated_fields.append(original_field)\n            \n        return np.array(simulated_fields)\n\n# Generate synthetic spatial environmental data\nnp.random.seed(42)\n\n# Create spatial grid\nx_coords = np.linspace(0, 100, 10)\ny_coords = np.linspace(0, 100, 10)\nX, Y = np.meshgrid(x_coords, y_coords)\ncoordinates = np.column_stack([X.flatten(), Y.flatten()])\n\n# Simulate spatially correlated environmental data (e.g., precipitation, temperature)\nn_locations = len(coordinates)\nn_variables = 2  # Temperature and precipitation\n\n# Create spatial correlation based on distance\nfrom scipy.spatial.distance import pdist, squareform\ndistances = squareform(pdist(coordinates))\nspatial_corr = np.exp(-distances / 30)  # Exponential correlation\n\n# Generate base correlated data\nbase_data = np.random.multivariate_normal(np.zeros(n_locations), spatial_corr, n_variables).T\n\n# Transform to realistic environmental data\nobservations = np.zeros((n_locations, n_variables))\n# Temperature: normal distribution with spatial trend\nobservations[:, 0] = base_data[:, 0] * 5 + 20 + 0.1 * coordinates[:, 1]  # Temperature increases with latitude\n# Precipitation: gamma distribution\nobservations[:, 1] = stats.gamma.ppf(stats.norm.cdf(base_data[:, 1]), a=2, scale=10)\n\n# Initialize spatial copula analysis\nspatial_analyzer = SpatialCopulaAnalysis(coordinates, observations)\nspatial_analyzer.fit_spatial_copula()\n\nfig, axes = plt.subplots(3, 4, figsize=(20, 15))\n\n# Original data visualization\nscatter1 = axes[0, 0].scatter(coordinates[:, 0], coordinates[:, 1], \n                             c=observations[:, 0], cmap='RdYlBu_r', s=100)\naxes[0, 0].set_title('Temperature Field')\naxes[0, 0].set_xlabel('X coordinate')\naxes[0, 0].set_ylabel('Y coordinate')\nplt.colorbar(scatter1, ax=axes[0, 0])\n\nscatter2 = axes[0, 1].scatter(coordinates[:, 0], coordinates[:, 1], \n                             c=observations[:, 1], cmap='Blues', s=100)\naxes[0, 1].set_title('Precipitation Field')\naxes[0, 1].set_xlabel('X coordinate')\naxes[0, 1].set_ylabel('Y coordinate')\nplt.colorbar(scatter2, ax=axes[0, 1])\n\n# Scatter plot of variables\naxes[0, 2].scatter(observations[:, 0], observations[:, 1], alpha=0.7)\naxes[0, 2].set_title('Temperature vs Precipitation')\naxes[0, 2].set_xlabel('Temperature')\naxes[0, 2].set_ylabel('Precipitation')\naxes[0, 2].grid(True)\n\n# Distance-dependence relationship\nif hasattr(spatial_analyzer, 'empirical_dependence'):\n    bin_centers = (spatial_analyzer.distance_bins[:-1] + spatial_analyzer.distance_bins[1:]) / 2\n    axes[0, 3].plot(bin_centers, spatial_analyzer.empirical_dependence, 'o-', linewidth=2)\n    axes[0, 3].set_title('Spatial Dependence Structure')\n    axes[0, 3].set_xlabel('Distance')\n    axes[0, 3].set_ylabel('Kendall\\'s τ')\n    axes[0, 3].grid(True)\n\n# Uniform margins (copula data)\nuniform_data = spatial_analyzer.uniform_obs\n\nscatter3 = axes[1, 0].scatter(coordinates[:, 0], coordinates[:, 1], \n                             c=uniform_data[:, 0], cmap='viridis', s=100)\naxes[1, 0].set_title('Temperature - Uniform Margins')\naxes[1, 0].set_xlabel('X coordinate')\naxes[1, 0].set_ylabel('Y coordinate')\nplt.colorbar(scatter3, ax=axes[1, 0])\n\nscatter4 = axes[1, 1].scatter(coordinates[:, 0], coordinates[:, 1], \n                             c=uniform_data[:, 1], cmap='viridis', s=100)\naxes[1, 1].set_title('Precipitation - Uniform Margins')\naxes[1, 1].set_xlabel('X coordinate')\naxes[1, 1].set_ylabel('Y coordinate')\nplt.colorbar(scatter4, ax=axes[1, 1])\n\n# Copula scatter plot\naxes[1, 2].scatter(uniform_data[:, 0], uniform_data[:, 1], alpha=0.7)\naxes[1, 2].set_title('Copula Data (Uniform Margins)')\naxes[1, 2].set_xlabel('U₁ (Temperature)')\naxes[1, 2].set_ylabel('U₂ (Precipitation)')\naxes[1, 2].grid(True)\n\n# Empirical copula density estimation\nfrom scipy.stats import gaussian_kde\nkde = gaussian_kde([uniform_data[:, 0], uniform_data[:, 1]])\nu_grid = np.linspace(0, 1, 50)\nU1_grid, U2_grid = np.meshgrid(u_grid, u_grid)\npositions = np.vstack([U1_grid.ravel(), U2_grid.ravel()])\ncopula_density = kde(positions).reshape(U1_grid.shape)\n\ncontour = axes[1, 3].contourf(U1_grid, U2_grid, copula_density, levels=20, cmap='viridis')\naxes[1, 3].set_title('Empirical Copula Density')\naxes[1, 3].set_xlabel('U₁')\naxes[1, 3].set_ylabel('U₂')\nplt.colorbar(contour, ax=axes[1, 3])\n\n# Simulate spatial fields\nsimulated_fields = spatial_analyzer.simulate_spatial_field(100)\n\n# Plot some simulated realizations\nfor i in range(3):\n    sim_field = simulated_fields[i]\n    scatter_sim = axes[2, i].scatter(coordinates[:, 0], coordinates[:, 1], \n                                    c=sim_field, cmap='RdYlBu_r', s=100,\n                                    vmin=observations[:, 0].min(), \n                                    vmax=observations[:, 0].max())\n    axes[2, i].set_title(f'Simulated Field {i+1}')\n    axes[2, i].set_xlabel('X coordinate')\n    axes[2, i].set_ylabel('Y coordinate')\n    plt.colorbar(scatter_sim, ax=axes[2, i])\n\n# Validation: compare statistics\noriginal_stats = {\n    'Mean': np.mean(observations[:, 0]),\n    'Std': np.std(observations[:, 0]),\n    'Min': np.min(observations[:, 0]),\n    'Max': np.max(observations[:, 0])\n}\n\nsimulated_stats = {\n    'Mean': np.mean(simulated_fields),\n    'Std': np.std(simulated_fields),\n    'Min': np.min(simulated_fields),\n    'Max': np.max(simulated_fields)\n}\n\n# Statistical comparison\naxes[2, 3].axis('off')\nstats_text = \"Validation Statistics\\n\\n\"\nstats_text += f\"{'Statistic':&lt;10} {'Original':&lt;10} {'Simulated':&lt;10}\\n\"\nstats_text += \"-\" * 35 + \"\\n\"\nfor key in original_stats.keys():\n    stats_text += f\"{key:&lt;10} {original_stats[key]:&lt;10.2f} {simulated_stats[key]:&lt;10.2f}\\n\"\n\naxes[2, 3].text(0.1, 0.9, stats_text, transform=axes[2, 3].transAxes, \n                fontsize=12, fontfamily='monospace', verticalalignment='top')\n\n# Add correlation analysis\noriginal_spatial_corr = np.corrcoef(observations[:, 0].reshape(10, 10))\nmean_simulated_field = np.mean(simulated_fields, axis=0).reshape(10, 10)\nsimulated_spatial_corr = np.corrcoef(mean_simulated_field)\n\ncorrelation_comparison = f\"\\nSpatial Correlation:\\n\"\ncorrelation_comparison += f\"Original: {np.mean(original_spatial_corr[original_spatial_corr != 1]):.3f}\\n\"\ncorrelation_comparison += f\"Simulated: {np.mean(simulated_spatial_corr[simulated_spatial_corr != 1]):.3f}\"\n\naxes[2, 3].text(0.1, 0.4, correlation_comparison, transform=axes[2, 3].transAxes, \n                fontsize=12, fontfamily='monospace', verticalalignment='top')\n\nplt.subplots_adjust(left=0.05, right=0.98, top=0.95, bottom=0.05, hspace=0.3, wspace=0.2)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 10: Spatial data analysis using copulas: modeling dependence in environmental data\n\n\n\n\n\nThese applications demonstrate the practical power of copulas in:\n\nPortfolio Risk Management:\n\nCapturing realistic dependence between asset returns\nAccurate VaR and Expected Shortfall estimation\nRisk decomposition and attribution\nStress testing and scenario analysis\n\nSpatial Data Analysis:\n\nModeling spatial dependence structures\nSimulating realistic spatial fields\nPreserving marginal distributions while capturing spatial correlation\nEnvironmental monitoring and prediction\n\n\nThe copula framework enables practitioners to build sophisticated models that capture the complexity of real-world dependence patterns while maintaining mathematical rigor and computational tractability."
  },
  {
    "objectID": "posts/29-07-2025_copulas/index.html#sec-conclusion",
    "href": "posts/29-07-2025_copulas/index.html#sec-conclusion",
    "title": "Copulas: Theory, Applications, and Implementation in Python",
    "section": "8 Conclusion and Future Directions",
    "text": "8 Conclusion and Future Directions\nThis comprehensive treatment of copula theory has taken us from the foundational mathematical framework established by Sklar’s theorem through advanced applications in finance and spatial statistics. The journey illustrates both the theoretical elegance and practical power of copulas in modern statistical modeling.\n\n8.1 Key Insights and Contributions\nTheoretical Foundations: Sklar’s theorem provides the mathematical bedrock that justifies the separation of marginal distributions from dependence structure. This seemingly simple insight has profound implications, enabling the construction of flexible multivariate models that would be impossible using traditional approaches.\nMethodological Advances: The major copula families—Archimedean and elliptical—offer complementary tools for different modeling scenarios: - Archimedean copulas excel at capturing asymmetric dependence and provide analytical tractability through their generator functions - Elliptical copulas leverage familiar multivariate distributions and offer natural extensions of classical methods - Vine copulas (briefly mentioned) extend these concepts to high-dimensional settings through hierarchical constructions\nPractical Impact: The applications demonstrated here showcase copulas’ transformative effect on quantitative modeling: - In portfolio risk management, copulas enable more accurate tail risk assessment by capturing realistic dependence during market stress - In spatial statistics, they allow flexible modeling of environmental phenomena while preserving both marginal characteristics and spatial correlation patterns\n\n\n8.2 Methodological Considerations\nModel Selection: Choosing appropriate copula families requires careful consideration of: - The nature of the dependence structure (symmetric vs. asymmetric, tail dependence properties) - Computational requirements and analytical tractability - Goodness-of-fit to observed data patterns - Robustness to model misspecification\nEstimation Challenges: Parameter estimation in copula models involves several considerations: - Two-stage methods (fit margins first, then copula) are computationally efficient but may not be fully efficient - Full maximum likelihood provides optimal statistical properties but can be computationally demanding - Rank-based methods offer robustness to marginal misspecification\nValidation and Diagnostics: Proper model validation is crucial and should include: - Graphical diagnostics (P-P plots, Q-Q plots, residual analysis) - Formal goodness-of-fit tests designed specifically for copulas - Out-of-sample validation and backtesting for risk management applications\n\n\n8.3 Future Research Directions\nHigh-Dimensional Extensions: While vine copulas provide one approach to high-dimensional modeling, developing more efficient methods for truly high-dimensional applications remains an active area of research. Factor copula models and sparse representations offer promising directions.\nDynamic and Time-Varying Copulas: Many applications require modeling dependence structures that evolve over time. Dynamic copula models, regime-switching approaches, and non-parametric time-varying methods represent important research frontiers.\nMachine Learning Integration: The intersection of copula theory with machine learning offers exciting possibilities, including: - Neural network-based copula approximations - Deep generative models informed by copula structure - Automated copula selection using machine learning techniques\nComputational Advances: As data sizes grow and models become more complex, computational efficiency becomes increasingly important. Research into: - Fast simulation algorithms - Approximate inference methods - Parallel and distributed computing approaches - GPU acceleration of copula computations\n\n\n8.4 Final Thoughts\nCopulas represent a mature theoretical framework that continues to find new applications and inspire methodological innovations. Their success stems from addressing a fundamental challenge in multivariate modeling: how to flexibly capture dependence while allowing for realistic marginal behavior.\nFor practitioners, copulas offer a principled approach to building models that can capture the complexity observed in real data. For theorists, they provide a rich mathematical structure that continues to yield new insights and extensions.\nAs we move forward, the integration of copula theory with emerging computational methods and new application domains promises to further expand their impact across statistics, economics, environmental science, and beyond. The framework established by Sklar more than six decades ago continues to provide the foundation for understanding and modeling the intricate web of dependencies that characterize our complex world.\nThe Python implementations provided throughout this primer offer a starting point for readers to explore these concepts further and adapt them to their own applications. The mathematical rigor combined with practical implementations demonstrates that copulas are not merely theoretical constructs but powerful tools for solving real-world problems."
  },
  {
    "objectID": "posts/03-04-2025_intro_to_sde/index.html",
    "href": "posts/03-04-2025_intro_to_sde/index.html",
    "title": "Stochastic Differential Equations: From Mathematical Foundations to Modern Applications",
    "section": "",
    "text": "Stochastic differential equations provide the mathematical framework for modeling continuous-time random processes, with applications spanning from option pricing to generative AI models"
  },
  {
    "objectID": "posts/03-04-2025_intro_to_sde/index.html#sec-introduction",
    "href": "posts/03-04-2025_intro_to_sde/index.html#sec-introduction",
    "title": "Stochastic Differential Equations: From Mathematical Foundations to Modern Applications",
    "section": "1 Introduction",
    "text": "1 Introduction\nStochastic differential equations (SDEs) represent one of the most profound and mathematically elegant frameworks in modern probability theory and applied mathematics. These equations describe the evolution of random processes in continuous time, providing the mathematical foundation for modeling phenomena characterized by both deterministic trends and random fluctuations (oksendal2003?; karatzas1991?).\nThe theoretical development of SDE theory emerged from the intersection of probability theory, differential equations, and mathematical finance, culminating in the revolutionary work of Kiyoshi Itô in the 1940s. Itô’s groundbreaking construction of stochastic integration transformed our understanding of random processes and established the mathematical framework that now underpins modern quantitative finance, statistical physics, and machine learning (ito1951?; mckean1969?).\n\n1.1 Historical Development and Motivation\nThe genesis of stochastic differential equations can be traced to Louis Bachelier’s pioneering 1900 thesis on option pricing, where he first applied Brownian motion to financial markets (bachelier1900?). However, it was not until Norbert Wiener’s rigorous mathematical construction of Brownian motion in the 1920s that the field gained its theoretical foundation (wiener1923?).\nThe transformative breakthrough came with Itô’s development of stochastic calculus in the 1940s and 1950s. Itô recognized that classical calculus was inadequate for handling functions of Brownian motion due to their non-differentiable nature, leading to his construction of stochastic integration and the famous Itô’s lemma (ito1951?). This work established the mathematical machinery necessary for:\n\nRigorous treatment of random processes: Moving beyond heuristic arguments to mathematically precise formulations\nFinancial modeling: Providing the foundation for modern option pricing theory through the Black-Scholes model\nEngineering applications: Enabling analysis of systems subject to random disturbances\nMachine learning: Supporting modern developments in neural differential equations and diffusion models\n\n\n\n1.2 Contemporary Relevance and Applications\nIn the 21st century, SDEs have experienced a renaissance driven by advances in computational methods and emerging applications in machine learning. Key contemporary developments include:\nMathematical Finance: SDEs form the backbone of modern derivatives pricing, risk management, and portfolio optimization. Models like the Heston stochastic volatility model and interest rate models (Vasicek, Cox-Ingersoll-Ross) are built on SDE foundations (heston1993?; cox1985?).\nMachine Learning and AI: Recent breakthroughs in generative modeling, particularly diffusion models for image generation, rely heavily on SDE theory. Neural ordinary differential equations (NODEs) and neural SDEs represent cutting-edge applications of stochastic analysis to deep learning (chen2018?; song2021?).\nScientific Computing: SDEs provide essential tools for modeling complex systems in physics, biology, and engineering where random effects play a crucial role (gardiner2009?).\n\n\n1.3 Scope and Mathematical Prerequisites\nThis treatise provides a comprehensive, PhD-level treatment of stochastic differential equation theory and applications. We assume familiarity with:\n\nReal analysis: Measure theory, Lebesgue integration, and functional analysis\nProbability theory: Probability spaces, random variables, and basic stochastic processes\nDifferential equations: Ordinary differential equations and partial differential equations\nLinear algebra: Matrix theory and spectral analysis\n\nOur systematic development progresses through:\nTheoretical Foundations (Sections 2-4): We establish the mathematical framework, beginning with Brownian motion and filtrations, developing Itô calculus, and proving fundamental existence and uniqueness theorems.\nNumerical Methods (Section 5): We examine computational approaches including Euler-Maruyama and Milstein schemes, analyzing convergence properties and implementation considerations.\nFinancial Applications (Section 6): We explore classical applications in option pricing, interest rate modeling, and risk management, providing complete derivations and implementations.\nModern Machine Learning Applications (Sections 7-8): We investigate contemporary applications in neural differential equations, diffusion models, and Gaussian processes, connecting classical theory to cutting-edge developments.\nAdvanced Topics (Section 9): We cover jump-diffusion processes, stochastic volatility models, and path-dependent derivatives.\nThroughout, we provide rigorous mathematical proofs, comprehensive Python implementations optimized with modern computational libraries, and publication-quality visualizations that illuminate key concepts and facilitate practical application."
  },
  {
    "objectID": "posts/03-04-2025_intro_to_sde/index.html#sec-foundations",
    "href": "posts/03-04-2025_intro_to_sde/index.html#sec-foundations",
    "title": "Stochastic Differential Equations: From Mathematical Foundations to Modern Applications",
    "section": "2 Mathematical Foundations",
    "text": "2 Mathematical Foundations\n\n2.1 Probability Spaces and Filtrations\nThe rigorous development of stochastic differential equation theory requires careful construction of the underlying probability framework. We begin with the fundamental mathematical structures that support stochastic analysis.\nDefinition 2.1 (Probability Space): A probability space is a triple \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) where: - \\(\\Omega\\) is the sample space representing all possible outcomes - \\(\\mathcal{F}\\) is a \\(\\sigma\\)-algebra on \\(\\Omega\\) representing measurable events - \\(\\mathbb{P}: \\mathcal{F} \\to [0,1]\\) is a probability measure satisfying \\(\\mathbb{P}(\\Omega) = 1\\)\nDefinition 2.2 (Filtration): A filtration \\(\\{\\mathcal{F}_t\\}_{t \\geq 0}\\) is an increasing family of sub-\\(\\sigma\\)-algebras of \\(\\mathcal{F}\\): \\[\\mathcal{F}_s \\subseteq \\mathcal{F}_t \\subseteq \\mathcal{F} \\quad \\text{for all } 0 \\leq s \\leq t\\]\nThe filtration represents the evolution of information over time, where \\(\\mathcal{F}_t\\) contains all events observable up to time \\(t\\).\n\n\n2.2 Brownian Motion and the Wiener Process\nBrownian motion forms the cornerstone of stochastic calculus, providing the fundamental building block for constructing more complex stochastic processes.\nDefinition 2.3 (Standard Brownian Motion): A stochastic process \\(\\{W_t\\}_{t \\geq 0}\\) defined on \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) is called standard Brownian motion if:\n\nInitial condition: \\(W_0 = 0\\) almost surely\nIndependent increments: For any \\(0 \\leq t_1 &lt; t_2 &lt; \\cdots &lt; t_n\\), the increments \\(W_{t_2} - W_{t_1}, W_{t_3} - W_{t_2}, \\ldots, W_{t_n} - W_{t_{n-1}}\\) are independent\nGaussian increments: For any \\(s &lt; t\\), \\(W_t - W_s \\sim \\mathcal{N}(0, t-s)\\)\nContinuous paths: \\(t \\mapsto W_t(\\omega)\\) is continuous for almost all \\(\\omega \\in \\Omega\\)\n\n\n\nCode\n@njit\ndef simulate_brownian_motion(T, N, n_paths=5):\n    \"\"\"Efficiently simulate Brownian motion paths using Numba acceleration.\"\"\"\n    dt = T / N\n    sqrt_dt = np.sqrt(dt)\n    \n    paths = np.zeros((n_paths, N + 1))\n    \n    for i in range(n_paths):\n        for j in range(1, N + 1):\n            paths[i, j] = paths[i, j-1] + sqrt_dt * np.random.randn()\n    \n    return paths\n\n# Simulation parameters\nT = 1.0  # Time horizon\nN = 500  # Number of time steps (reduced for faster rendering)\nn_paths = 50  # Reduced number of paths\ndt = T / N\nt = np.linspace(0, T, N + 1)\n\n# Generate multiple Brownian motion paths\nnp.random.seed(42)\npaths = simulate_brownian_motion(T, N, n_paths)\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# Sample paths\naxes[0, 0].plot(t, paths[:10].T, alpha=0.6, linewidth=0.8)\naxes[0, 0].plot(t, paths[0], 'r-', linewidth=2, label='Sample path')\naxes[0, 0].set_title('Sample Paths of Brownian Motion')\naxes[0, 0].set_xlabel('Time t')\naxes[0, 0].set_ylabel('W(t)')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Distribution at fixed time\nt_fixed = 0.5\nW_fixed = paths[:, int(t_fixed * N)]\naxes[0, 1].hist(W_fixed, bins=30, density=True, alpha=0.7, color='skyblue')\nx_range = np.linspace(W_fixed.min(), W_fixed.max(), 100)\ntheoretical_pdf = stats.norm.pdf(x_range, 0, np.sqrt(t_fixed))\naxes[0, 1].plot(x_range, theoretical_pdf, 'r-', linewidth=2, label=f'N(0, {t_fixed})')\naxes[0, 1].set_title(f'Distribution of W({t_fixed})')\naxes[0, 1].set_xlabel('Value')\naxes[0, 1].set_ylabel('Density')\naxes[0, 1].legend()\n\n# Increment distribution\nincrements = np.diff(paths[0])\naxes[0, 2].hist(increments, bins=40, density=True, alpha=0.7, color='lightcoral')\nx_inc = np.linspace(increments.min(), increments.max(), 100)\ntheoretical_inc = stats.norm.pdf(x_inc, 0, np.sqrt(dt))\naxes[0, 2].plot(x_inc, theoretical_inc, 'k-', linewidth=2, label=f'N(0, {dt:.3f})')\naxes[0, 2].set_title('Increment Distribution')\naxes[0, 2].set_xlabel('Increment Value')\naxes[0, 2].set_ylabel('Density')\naxes[0, 2].legend()\n\n# Quadratic variation approximation\ndef quadratic_variation(path, dt):\n    \"\"\"Compute empirical quadratic variation.\"\"\"\n    increments = np.diff(path)\n    return np.cumsum(increments**2)\n\nqv = quadratic_variation(paths[0], dt)\naxes[1, 0].plot(t[1:], qv, 'b-', linewidth=2, label='Empirical [W,W]_t')\naxes[1, 0].plot(t, t, 'r--', linewidth=2, label='Theoretical t')\naxes[1, 0].set_title('Quadratic Variation')\naxes[1, 0].set_xlabel('Time t')\naxes[1, 0].set_ylabel('[W,W]_t')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Path roughness (non-differentiability)\n# Compute finite difference approximations to derivatives\nh_values = [0.1, 0.05, 0.01, 0.005]\ncolors = ['red', 'blue', 'green', 'orange']\n\nfor i, h in enumerate(h_values):\n    n_h = int(h / dt)\n    if n_h &gt; 0:\n        t_deriv = t[:-n_h]\n        finite_diff = (paths[0, n_h:] - paths[0, :-n_h]) / h\n        axes[1, 1].plot(t_deriv, finite_diff, color=colors[i], alpha=0.7, \n                       linewidth=1, label=f'h = {h}')\n\naxes[1, 1].set_title('Finite Difference Approximations\\n(Illustrating Non-differentiability)')\naxes[1, 1].set_xlabel('Time t')\naxes[1, 1].set_ylabel('(W(t+h) - W(t))/h')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\n# Scaling property demonstration\nscaled_paths = []\nscale_factors = [0.5, 1.0, 2.0]\ncolors_scale = ['blue', 'red', 'green']\n\nfor i, c in enumerate(scale_factors):\n    # W(ct) has same distribution as sqrt(c) * W(t)\n    if c != 1.0:\n        t_scaled = t * c\n        if c &lt; 1.0:\n            # Subsample for c &lt; 1\n            indices = np.linspace(0, len(t)-1, int(len(t)*c)).astype(int)\n            scaled_path = paths[0, indices] / np.sqrt(c)\n            t_plot = t[:len(indices)]\n        else:\n            # Extend time for c &gt; 1 (simplified for faster rendering)\n            extended_path = simulate_brownian_motion(T*c, int(N*c*0.5), 1)[0]\n            scaled_path = extended_path / np.sqrt(c)\n            t_plot = np.linspace(0, T, len(scaled_path))\n    else:\n        scaled_path = paths[0]\n        t_plot = t\n    \n    axes[1, 2].plot(t_plot, scaled_path, color=colors_scale[i], \n                   linewidth=1.5, alpha=0.8, label=f'c = {c}')\n\naxes[1, 2].set_title('Self-Similarity Property\\nW(ct) ~ √c · W(t)')\naxes[1, 2].set_xlabel('Time t')\naxes[1, 2].set_ylabel('Scaled W(t)')\naxes[1, 2].legend()\naxes[1, 2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Sample paths of Brownian motion and their key properties\n\n\n\n\n\nTheorem 2.4 (Properties of Brownian Motion): Standard Brownian motion possesses the following fundamental properties:\n\nMartingale property: \\(\\{W_t\\}\\) is a martingale with respect to its natural filtration\nQuadratic variation: \\([W,W]_t = t\\) (in the sense of convergence in probability)\nMarkov property: For \\(s &lt; t\\), \\(\\mathbb{E}[f(W_t) | \\mathcal{F}_s] = \\mathbb{E}[f(W_t) | W_s]\\)\nSelf-similarity: \\(\\{W_{ct}\\}_{t \\geq 0} \\stackrel{d}{=} \\{\\sqrt{c} W_t\\}_{t \\geq 0}\\) for any \\(c &gt; 0\\)\nPath properties: Paths are continuous but nowhere differentiable with probability 1\n\n\n\n2.3 Multi-dimensional Brownian Motion\nDefinition 2.5 (d-dimensional Brownian Motion): A \\(d\\)-dimensional Brownian motion is a vector process \\(\\mathbf{W}_t = (W_t^{(1)}, \\ldots, W_t^{(d)})^T\\) where each component \\(W_t^{(i)}\\) is independent standard Brownian motion.\nFor correlated Brownian motions, we can construct them using: \\[\\mathbf{W}_t = \\mathbf{L} \\mathbf{Z}_t\\] where \\(\\mathbf{Z}_t\\) is \\(d\\)-dimensional independent Brownian motion and \\(\\mathbf{L}\\) is the Cholesky decomposition of the correlation matrix \\(\\boldsymbol{\\Sigma}\\).\n\n\nCode\ndef simulate_correlated_brownian(T, N, correlation_matrix, n_paths=1):\n    \"\"\"Simulate correlated multi-dimensional Brownian motion.\"\"\"\n    d = correlation_matrix.shape[0]\n    dt = T / N\n    sqrt_dt = np.sqrt(dt)\n    \n    # Cholesky decomposition for correlation\n    L = cholesky(correlation_matrix, lower=True)\n    \n    paths = np.zeros((n_paths, d, N + 1))\n    \n    for path in range(n_paths):\n        for i in range(1, N + 1):\n            # Generate independent increments\n            dZ = np.random.randn(d) * sqrt_dt\n            # Apply correlation structure\n            dW = L @ dZ\n            paths[path, :, i] = paths[path, :, i-1] + dW\n    \n    return paths\n\n# Correlation matrices\ncorrelations = [\n    np.array([[1.0, 0.0], [0.0, 1.0]]),  # Independent\n    np.array([[1.0, 0.7], [0.7, 1.0]]),  # Positive correlation\n    np.array([[1.0, -0.5], [-0.5, 1.0]]) # Negative correlation\n]\n\ncorrelation_names = ['Independent (ρ=0)', 'Positive (ρ=0.7)', 'Negative (ρ=-0.5)']\n\nfig, axes = plt.subplots(2, 3, figsize=(16, 10))\n\nfor i, (corr_matrix, name) in enumerate(zip(correlations, correlation_names)):\n    # Simulate paths\n    paths = simulate_correlated_brownian(T, N, corr_matrix, n_paths=100)\n    t = np.linspace(0, T, N + 1)\n    \n    # Time series plot\n    for j in range(20):  # Plot subset of paths\n        axes[0, i].plot(t, paths[j, 0, :], alpha=0.3, color='blue', linewidth=0.8)\n        axes[0, i].plot(t, paths[j, 1, :], alpha=0.3, color='red', linewidth=0.8)\n    \n    # Highlight one path\n    axes[0, i].plot(t, paths[0, 0, :], color='blue', linewidth=2, label='W₁(t)')\n    axes[0, i].plot(t, paths[0, 1, :], color='red', linewidth=2, label='W₂(t)')\n    axes[0, i].set_title(f'Time Series: {name}')\n    axes[0, i].set_xlabel('Time t')\n    axes[0, i].set_ylabel('W(t)')\n    axes[0, i].legend()\n    axes[0, i].grid(True, alpha=0.3)\n    \n    # Phase plot (W₁ vs W₂)\n    for j in range(50):\n        axes[1, i].plot(paths[j, 0, :], paths[j, 1, :], alpha=0.4, linewidth=0.6)\n    \n    axes[1, i].scatter(0, 0, color='green', s=100, marker='o', zorder=5, label='Origin')\n    axes[1, i].set_title(f'Phase Plot: {name}')\n    axes[1, i].set_xlabel('W₁(t)')\n    axes[1, i].set_ylabel('W₂(t)')\n    axes[1, i].legend()\n    axes[1, i].grid(True, alpha=0.3)\n    axes[1, i].axis('equal')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Multi-dimensional Brownian motion with correlation structure\n\n\n\n\n\n\n\n2.4 Martingales and Stopping Times\nDefinition 2.6 (Martingale): A stochastic process \\(\\{M_t\\}_{t \\geq 0}\\) adapted to filtration \\(\\{\\mathcal{F}_t\\}\\) is a martingale if: 1. \\(\\mathbb{E}[|M_t|] &lt; \\infty\\) for all \\(t \\geq 0\\) 2. \\(\\mathbb{E}[M_t | \\mathcal{F}_s] = M_s\\) for all \\(0 \\leq s \\leq t\\)\nTheorem 2.7 (Examples of Martingales): The following processes are martingales: 1. Brownian motion \\(W_t\\) 2. \\(W_t^2 - t\\) (compensated quadratic variation) 3. \\(\\exp(\\sigma W_t - \\frac{\\sigma^2 t}{2})\\) for any \\(\\sigma \\in \\mathbb{R}\\) (exponential martingale)\nDefinition 2.8 (Stopping Time): A random variable \\(\\tau: \\Omega \\to [0, \\infty]\\) is a stopping time with respect to \\(\\{\\mathcal{F}_t\\}\\) if for every \\(t \\geq 0\\): \\[\\{\\tau \\leq t\\} \\in \\mathcal{F}_t\\]\nTheorem 2.9 (Optional Stopping Theorem): If \\(M_t\\) is a martingale and \\(\\tau\\) is a bounded stopping time, then: \\[\\mathbb{E}[M_\\tau] = \\mathbb{E}[M_0]\\]\nThese foundational concepts provide the mathematical infrastructure necessary for constructing stochastic integrals and developing the theory of stochastic differential equations. In the next section, we will build upon this foundation to develop Itô calculus, the cornerstone of stochastic analysis."
  },
  {
    "objectID": "posts/03-04-2025_intro_to_sde/index.html#sec-ito-calculus",
    "href": "posts/03-04-2025_intro_to_sde/index.html#sec-ito-calculus",
    "title": "Stochastic Differential Equations: From Mathematical Foundations to Modern Applications",
    "section": "3 Itô Calculus and Stochastic Integration",
    "text": "3 Itô Calculus and Stochastic Integration\nThe development of stochastic calculus represents one of the most profound mathematical achievements of the 20th century. Classical calculus fails when applied to functions of Brownian motion due to their infinite variation and non-differentiable nature. Itô’s revolutionary insight was to develop a new form of calculus specifically designed for stochastic processes.\n\n3.1 The Need for Stochastic Calculus\nConsider attempting to define the integral \\(\\int_0^t W_s \\, dW_s\\) using classical Riemann-Stieltjes integration. The fundamental problem arises from the fact that Brownian motion has infinite variation on any interval, making classical integration impossible.\n\n\nCode\ndef compute_variation(path, time_grid):\n    \"\"\"Compute total variation of a function on given grid.\"\"\"\n    return np.sum(np.abs(np.diff(path)))\n\ndef compute_quadratic_variation(path, time_grid):\n    \"\"\"Compute quadratic variation approximation.\"\"\"\n    return np.sum(np.diff(path)**2)\n\n# Generate fine Brownian motion path\nT = 1.0\nN_fine = 10000\nt_fine = np.linspace(0, T, N_fine + 1)\ndt_fine = T / N_fine\n\nnp.random.seed(42)\nW_fine = np.cumsum(np.concatenate([[0], np.random.randn(N_fine) * np.sqrt(dt_fine)]))\n\n# Compute variations for different grid sizes\ngrid_sizes = np.logspace(1, 4, 20).astype(int)\ntotal_variations = []\nquadratic_variations = []\n\nfor N in grid_sizes:\n    if N &lt;= N_fine:\n        indices = np.linspace(0, N_fine, N + 1).astype(int)\n        subpath = W_fine[indices]\n        t_sub = t_fine[indices]\n        \n        total_var = compute_variation(subpath, t_sub)\n        quad_var = compute_quadratic_variation(subpath, t_sub)\n        \n        total_variations.append(total_var)\n        quadratic_variations.append(quad_var)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 8))\n\n# Plot path and variations\nN_display = 1000\nindices = np.linspace(0, N_fine, N_display + 1).astype(int)\naxes[0].plot(t_fine[indices], W_fine[indices], 'b-', linewidth=1, alpha=0.8)\naxes[0].set_title('Sample Brownian Motion Path')\naxes[0].set_xlabel('Time t')\naxes[0].set_ylabel('W(t)')\naxes[0].grid(True, alpha=0.3)\n\n# Plot variation convergence\naxes[1].loglog(grid_sizes[:len(total_variations)], total_variations, 'ro-', \n               label='Total Variation', markersize=4)\naxes[1].loglog(grid_sizes[:len(quadratic_variations)], quadratic_variations, 'bs-', \n               label='Quadratic Variation', markersize=4)\naxes[1].axhline(y=T, color='black', linestyle='--', linewidth=2, \n                label=f'Theoretical QV = {T}')\naxes[1].set_xlabel('Number of Grid Points')\naxes[1].set_ylabel('Variation')\naxes[1].set_title('Variation Behavior as Grid Refines')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Final total variation: {total_variations[-1]:.2f}\")\nprint(f\"Final quadratic variation: {quadratic_variations[-1]:.3f}\")\nprint(f\"Theoretical quadratic variation: {T}\")\n\n\n\n\n\n\n\n\nFigure 3: Illustration of infinite variation in Brownian motion\n\n\n\n\n\nFinal total variation: 79.95\nFinal quadratic variation: 1.007\nTheoretical quadratic variation: 1.0\n\n\n\n\n3.2 Construction of the Itô Integral\nDefinition 3.1 (Simple Process): A stochastic process \\(\\{H_t\\}\\) is simple if it can be written as: \\[H_t = H_0 \\mathbf{1}_{\\{0\\}}(t) + \\sum_{i=1}^n H_{t_i} \\mathbf{1}_{(t_i, t_{i+1}]}(t)\\] where \\(0 = t_0 &lt; t_1 &lt; \\cdots &lt; t_n &lt; \\infty\\) and each \\(H_{t_i}\\) is \\(\\mathcal{F}_{t_i}\\)-measurable.\nDefinition 3.2 (Itô Integral for Simple Processes): For a simple process \\(H_t\\), the Itô integral is defined as: \\[\\int_0^t H_s \\, dW_s = \\sum_{i=0}^{n-1} H_{t_i}(W_{t_{i+1} \\wedge t} - W_{t_i \\wedge t})\\]\nTheorem 3.3 (Itô Isometry): For simple processes \\(H_t\\): \\[\\mathbb{E}\\left[\\left(\\int_0^t H_s \\, dW_s\\right)^2\\right] = \\mathbb{E}\\left[\\int_0^t H_s^2 \\, ds\\right]\\]\nDefinition 3.4 (General Itô Integral): For adapted processes \\(H_t\\) satisfying \\(\\mathbb{E}\\left[\\int_0^t H_s^2 \\, ds\\right] &lt; \\infty\\), the Itô integral \\(\\int_0^t H_s \\, dW_s\\) is defined as the \\(L^2\\) limit of Itô integrals of simple processes approximating \\(H_t\\).\n\n\n3.3 Itô’s Lemma: The Fundamental Theorem\nTheorem 3.5 (Itô’s Lemma): Let \\(W_t\\) be Brownian motion and \\(f(t,x) \\in C^{1,2}([0,\\infty) \\times \\mathbb{R})\\). Then:\n\\[df(t, W_t) = \\frac{\\partial f}{\\partial t}(t, W_t) dt + \\frac{\\partial f}{\\partial x}(t, W_t) dW_t + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}(t, W_t) dt\\]\nProof Sketch: The key insight is the quadratic variation term. Using Taylor expansion: \\[df = f_t dt + f_x dW_t + \\frac{1}{2}f_{xx}(dW_t)^2 + \\frac{1}{2}f_{tt}(dt)^2 + f_{tx} dt \\, dW_t + \\cdots\\]\nSince \\((dW_t)^2 = dt\\) in the sense of quadratic variation, and higher-order terms vanish, we obtain Itô’s formula. □\n\n\nCode\ndef ito_lemma_verification(f, df_dt, df_dx, d2f_dx2, W_path, dt):\n    \"\"\"\n    Verify Itô's lemma numerically by comparing direct computation\n    with the Itô formula prediction.\n    \"\"\"\n    t = np.arange(len(W_path)) * dt\n    \n    # Direct computation of f(t, W_t)\n    f_values = f(t, W_path)\n    df_direct = np.diff(f_values)\n    \n    # Itô formula prediction\n    t_mid = t[:-1] + dt/2  # Midpoint rule for better accuracy\n    W_mid = (W_path[:-1] + W_path[1:]) / 2\n    dW = np.diff(W_path)\n    \n    df_ito = (df_dt(t_mid, W_mid) * dt + \n              df_dx(t_mid, W_mid) * dW + \n              0.5 * d2f_dx2(t_mid, W_mid) * dt)\n    \n    return df_direct, df_ito, f_values\n\n# Example 1: f(t,x) = x^2\ndef f1(t, x):\n    return x**2\n\ndef df1_dt(t, x):\n    return np.zeros_like(x)\n\ndef df1_dx(t, x):\n    return 2*x\n\ndef d2f1_dx2(t, x):\n    return 2 * np.ones_like(x)\n\n# Example 2: f(t,x) = exp(x - t/2) (Exponential martingale)\ndef f2(t, x):\n    return np.exp(x - t/2)\n\ndef df2_dt(t, x):\n    return -0.5 * np.exp(x - t/2)\n\ndef df2_dx(t, x):\n    return np.exp(x - t/2)\n\ndef d2f2_dx2(t, x):\n    return np.exp(x - t/2)\n\n# Simulation parameters\nT = 1.0\nN = 1000\ndt = T / N\nt = np.linspace(0, T, N + 1)\n\nnp.random.seed(42)\nW = np.cumsum(np.concatenate([[0], np.random.randn(N) * np.sqrt(dt)]))\n\nfig, axes = plt.subplots(2, 3, figsize=(16, 10))\n\n# Example 1: f(t,x) = x^2\ndf1_direct, df1_ito, f1_values = ito_lemma_verification(\n    f1, df1_dt, df1_dx, d2f1_dx2, W, dt)\n\naxes[0, 0].plot(t, f1_values, 'b-', linewidth=2, label='W²(t)')\naxes[0, 0].set_title('Function: f(t,x) = x²')\naxes[0, 0].set_xlabel('Time t')\naxes[0, 0].set_ylabel('f(t, W(t))')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\naxes[0, 1].plot(t[:-1], df1_direct, 'b-', alpha=0.7, label='Direct: Δf')\naxes[0, 1].plot(t[:-1], df1_ito, 'r--', alpha=0.7, label='Itô formula')\naxes[0, 1].set_title('Increment Comparison')\naxes[0, 1].set_xlabel('Time t')\naxes[0, 1].set_ylabel('df')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Error analysis\nerror1 = df1_direct - df1_ito\naxes[0, 2].plot(t[:-1], error1, 'g-', linewidth=1)\naxes[0, 2].set_title(f'Error (RMS: {np.sqrt(np.mean(error1**2)):.6f})')\naxes[0, 2].set_xlabel('Time t')\naxes[0, 2].set_ylabel('Direct - Itô')\naxes[0, 2].grid(True, alpha=0.3)\n\n# Example 2: Exponential martingale\ndf2_direct, df2_ito, f2_values = ito_lemma_verification(\n    f2, df2_dt, df2_dx, d2f2_dx2, W, dt)\n\naxes[1, 0].plot(t, f2_values, 'purple', linewidth=2, label='exp(W(t) - t/2)')\naxes[1, 0].set_title('Exponential Martingale')\naxes[1, 0].set_xlabel('Time t')\naxes[1, 0].set_ylabel('f(t, W(t))')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\naxes[1, 1].plot(t[:-1], df2_direct, 'purple', alpha=0.7, label='Direct: Δf')\naxes[1, 1].plot(t[:-1], df2_ito, 'orange', linestyle='--', alpha=0.7, label='Itô formula')\naxes[1, 1].set_title('Increment Comparison')\naxes[1, 1].set_xlabel('Time t')\naxes[1, 1].set_ylabel('df')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nerror2 = df2_direct - df2_ito\naxes[1, 2].plot(t[:-1], error2, 'red', linewidth=1)\naxes[1, 2].set_title(f'Error (RMS: {np.sqrt(np.mean(error2**2)):.6f})')\naxes[1, 2].set_xlabel('Time t')\naxes[1, 2].set_ylabel('Direct - Itô')\naxes[1, 2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Theoretical verification for x^2\nprint(\"Theoretical verification for f(t,x) = x²:\")\nprint(\"df = 2W dW + dt\")\nprint(\"This gives: d(W²) = 2W dW + dt\")\nprint(\"So W²(t) = 2∫₀ᵗ W(s) dW(s) + t\")\nprint(f\"Empirical: W²({T:.1f}) = {W[-1]**2:.3f}\")\nprint(f\"Formula:   2∫WdW + t = {2 * np.sum(W[:-1] * np.diff(W)) + T:.3f}\")\n\n\n\n\n\n\n\n\nFigure 4: Demonstration of Itô’s lemma with geometric Brownian motion\n\n\n\n\n\nTheoretical verification for f(t,x) = x²:\ndf = 2W dW + dt\nThis gives: d(W²) = 2W dW + dt\nSo W²(t) = 2∫₀ᵗ W(s) dW(s) + t\nEmpirical: W²(1.0) = 0.374\nFormula:   2∫WdW + t = 0.415\n\n\n\n\n3.4 Multi-dimensional Itô’s Lemma\nTheorem 3.6 (Multi-dimensional Itô’s Lemma): Let \\(\\mathbf{X}_t = (X_t^{(1)}, \\ldots, X_t^{(d)})^T\\) be an Itô process satisfying: \\[d\\mathbf{X}_t = \\boldsymbol{\\mu}(t, \\mathbf{X}_t) dt + \\boldsymbol{\\sigma}(t, \\mathbf{X}_t) d\\mathbf{W}_t\\]\nFor \\(f(t, \\mathbf{x}) \\in C^{1,2}([0,\\infty) \\times \\mathbb{R}^d)\\):\n\\[df(t, \\mathbf{X}_t) = \\frac{\\partial f}{\\partial t} dt + \\sum_{i=1}^d \\frac{\\partial f}{\\partial x_i} dX_t^{(i)} + \\frac{1}{2} \\sum_{i,j=1}^d \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} d\\langle X^{(i)}, X^{(j)} \\rangle_t\\]\nwhere \\(d\\langle X^{(i)}, X^{(j)} \\rangle_t = \\sum_{k=1}^m \\sigma_{ik} \\sigma_{jk} dt\\) is the quadratic covariation.\n\n\n3.5 Applications of Itô’s Lemma\n\n\nCode\ndef simulate_geometric_brownian(S0, mu, sigma, T, N, n_paths=1):\n    \"\"\"Simulate geometric Brownian motion using exact solution.\"\"\"\n    dt = T / N\n    t = np.linspace(0, T, N + 1)\n    \n    # Generate Brownian increments\n    dW = np.random.randn(n_paths, N) * np.sqrt(dt)\n    W = np.column_stack([np.zeros(n_paths), np.cumsum(dW, axis=1)])\n    \n    # Exact solution: S(t) = S0 * exp((mu - sigma²/2)t + sigma*W(t))\n    S = S0 * np.exp((mu - 0.5 * sigma**2) * t[np.newaxis, :] + sigma * W)\n    \n    return t, S\n\ndef simulate_ornstein_uhlenbeck(X0, theta, mu, sigma, T, N, n_paths=1):\n    \"\"\"Simulate Ornstein-Uhlenbeck process using exact solution.\"\"\"\n    dt = T / N\n    t = np.linspace(0, T, N + 1)\n    \n    X = np.zeros((n_paths, N + 1))\n    X[:, 0] = X0\n    \n    for i in range(N):\n        # Exact transition: X(t+dt) = X(t)*exp(-theta*dt) + mu*(1-exp(-theta*dt)) + noise\n        exp_theta_dt = np.exp(-theta * dt)\n        mean = X[:, i] * exp_theta_dt + mu * (1 - exp_theta_dt)\n        var = sigma**2 * (1 - np.exp(-2 * theta * dt)) / (2 * theta)\n        X[:, i+1] = mean + np.sqrt(var) * np.random.randn(n_paths)\n    \n    return t, X\n\n# Parameters\nT = 2.0\nN = 1000\nn_paths = 200\n\n# Geometric Brownian Motion parameters\nS0 = 100\nmu = 0.05\nsigma = 0.2\n\n# Ornstein-Uhlenbeck parameters\nX0 = 0\ntheta = 2.0\nmu_ou = 1.0\nsigma_ou = 0.5\n\nnp.random.seed(42)\n\nfig, axes = plt.subplots(3, 2, figsize=(18, 12))\n\n# Geometric Brownian Motion\nt_gbm, S_gbm = simulate_geometric_brownian(S0, mu, sigma, T, N, n_paths)\n\n# Plot sample paths\nfor i in range(min(50, n_paths)):\n    axes[0, 0].plot(t_gbm, S_gbm[i], alpha=0.3, linewidth=0.8, color='blue')\naxes[0, 0].plot(t_gbm, S_gbm[0], color='red', linewidth=2, label='Sample path')\naxes[0, 0].set_title('Geometric Brownian Motion\\ndS = μS dt + σS dW')\naxes[0, 0].set_xlabel('Time t')\naxes[0, 0].set_ylabel('S(t)')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Log-returns distribution\nlog_returns = np.log(S_gbm[:, -1] / S_gbm[:, 0])\naxes[1, 0].hist(log_returns, bins=40, density=True, alpha=0.7, color='skyblue')\ntheoretical_mean = (mu - 0.5 * sigma**2) * T\ntheoretical_std = sigma * np.sqrt(T)\nx_range = np.linspace(log_returns.min(), log_returns.max(), 100)\ntheoretical_pdf = stats.norm.pdf(x_range, theoretical_mean, theoretical_std)\naxes[1, 0].plot(x_range, theoretical_pdf, 'r-', linewidth=2, \n               label=f'N({theoretical_mean:.3f}, {theoretical_std:.3f})')\naxes[1, 0].set_title('Log-Return Distribution')\naxes[1, 0].set_xlabel('log(S(T)/S(0))')\naxes[1, 0].set_ylabel('Density')\naxes[1, 0].legend()\n\n# Mean and variance evolution\nmeans = np.mean(S_gbm, axis=0)\nvars = np.var(S_gbm, axis=0)\ntheoretical_mean = S0 * np.exp(mu * t_gbm)\ntheoretical_var = S0**2 * np.exp(2*mu * t_gbm) * (np.exp(sigma**2 * t_gbm) - 1)\n\naxes[2, 0].plot(t_gbm, means, 'b-', linewidth=2, label='Empirical mean')\naxes[2, 0].plot(t_gbm, theoretical_mean, 'r--', linewidth=2, label='Theoretical mean')\naxes[2, 0].set_title('Mean Evolution')\naxes[2, 0].set_xlabel('Time t')\naxes[2, 0].set_ylabel('E[S(t)]')\naxes[2, 0].legend()\naxes[2, 0].grid(True, alpha=0.3)\n\n# Ornstein-Uhlenbeck Process\nt_ou, X_ou = simulate_ornstein_uhlenbeck(X0, theta, mu_ou, sigma_ou, T, N, n_paths)\n\n# Plot sample paths\nfor i in range(min(50, n_paths)):\n    axes[0, 1].plot(t_ou, X_ou[i], alpha=0.3, linewidth=0.8, color='green')\naxes[0, 1].plot(t_ou, X_ou[0], color='red', linewidth=2, label='Sample path')\naxes[0, 1].axhline(y=mu_ou, color='black', linestyle='--', alpha=0.7, label=f'Long-term mean = {mu_ou}')\naxes[0, 1].set_title('Ornstein-Uhlenbeck Process\\ndX = θ(μ - X) dt + σ dW')\naxes[0, 1].set_xlabel('Time t')\naxes[0, 1].set_ylabel('X(t)')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Stationary distribution\nfinal_values = X_ou[:, -1]\naxes[1, 1].hist(final_values, bins=40, density=True, alpha=0.7, color='lightgreen')\n# Theoretical stationary distribution: N(μ, σ²/(2θ))\nstationary_var = sigma_ou**2 / (2 * theta)\nx_range_ou = np.linspace(final_values.min(), final_values.max(), 100)\nstationary_pdf = stats.norm.pdf(x_range_ou, mu_ou, np.sqrt(stationary_var))\naxes[1, 1].plot(x_range_ou, stationary_pdf, 'r-', linewidth=2, \n               label=f'N({mu_ou}, {np.sqrt(stationary_var):.3f})')\naxes[1, 1].set_title('Terminal Distribution (t=2)')\naxes[1, 1].set_xlabel('X(T)')\naxes[1, 1].set_ylabel('Density')\naxes[1, 1].legend()\n\n# Mean reversion demonstration\nmeans_ou = np.mean(X_ou, axis=0)\nvars_ou = np.var(X_ou, axis=0)\ntheoretical_mean_ou = mu_ou + (X0 - mu_ou) * np.exp(-theta * t_ou)\ntheoretical_var_ou = sigma_ou**2 / (2 * theta) * (1 - np.exp(-2 * theta * t_ou))\n\naxes[2, 1].plot(t_ou, means_ou, 'g-', linewidth=2, label='Empirical mean')\naxes[2, 1].plot(t_ou, theoretical_mean_ou, 'r--', linewidth=2, label='Theoretical mean')\naxes[2, 1].plot(t_ou, vars_ou, 'b-', linewidth=2, alpha=0.7, label='Empirical variance')\naxes[2, 1].plot(t_ou, theoretical_var_ou, 'orange', linestyle='--', linewidth=2, \n               alpha=0.7, label='Theoretical variance')\naxes[2, 1].set_title('Mean Reversion and Variance Evolution')\naxes[2, 1].set_xlabel('Time t')\naxes[2, 1].set_ylabel('Moments')\naxes[2, 1].legend()\naxes[2, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5: Applications of Itô’s lemma: Geometric Brownian motion and Ornstein-Uhlenbeck process\n\n\n\n\n\nThe development of Itô calculus provides the mathematical foundation for analyzing stochastic differential equations. The key insights are:\n\nQuadratic variation matters: Unlike classical calculus, \\((dW_t)^2 = dt\\) contributes to the dynamics\nMartingale preservation: Properly constructed stochastic integrals preserve the martingale property\nChain rule modification: Itô’s lemma includes an additional second-order term due to quadratic variation\n\nIn the next section, we will use these tools to develop the general theory of stochastic differential equations and their solutions."
  },
  {
    "objectID": "posts/03-04-2025_intro_to_sde/index.html#sec-sde-theory",
    "href": "posts/03-04-2025_intro_to_sde/index.html#sec-sde-theory",
    "title": "Stochastic Differential Equations: From Mathematical Foundations to Modern Applications",
    "section": "4 Stochastic Differential Equations: Theory and Existence",
    "text": "4 Stochastic Differential Equations: Theory and Existence\nHaving established the foundations of stochastic calculus, we now turn to the central object of study: stochastic differential equations. These equations describe the evolution of random processes and form the mathematical backbone of modern quantitative finance and stochastic modeling.\n\n4.1 Mathematical Definition and Classification\nDefinition 4.1 (Stochastic Differential Equation): A stochastic differential equation (SDE) is an equation of the form: \\[dX_t = \\mu(t, X_t) dt + \\sigma(t, X_t) dW_t, \\quad X_0 = x_0\\]\nwhere: - \\(X_t\\) is the unknown stochastic process - \\(\\mu: [0,\\infty) \\times \\mathbb{R} \\to \\mathbb{R}\\) is the drift coefficient - \\(\\sigma: [0,\\infty) \\times \\mathbb{R} \\to \\mathbb{R}\\) is the diffusion coefficient\n- \\(W_t\\) is standard Brownian motion - \\(x_0\\) is the initial condition\nThe integral form is: \\[X_t = x_0 + \\int_0^t \\mu(s, X_s) ds + \\int_0^t \\sigma(s, X_s) dW_s\\]\nDefinition 4.2 (Strong vs Weak Solutions): - A strong solution to an SDE is an adapted process \\(X_t\\) defined on the same probability space as the driving Brownian motion \\(W_t\\) - A weak solution exists on some probability space with some Brownian motion that has the same law as the original problem\n\n\n4.2 Existence and Uniqueness Theory\nThe fundamental question in SDE theory concerns when solutions exist and when they are unique. The classical result is due to Itô and provides sufficient conditions.\nTheorem 4.3 (Existence and Uniqueness - Lipschitz Case): Consider the SDE: \\[dX_t = \\mu(t, X_t) dt + \\sigma(t, X_t) dW_t, \\quad X_0 = x_0\\]\nIf \\(\\mu\\) and \\(\\sigma\\) satisfy: 1. Lipschitz condition: There exists \\(K &gt; 0\\) such that for all \\(t \\geq 0\\) and \\(x, y \\in \\mathbb{R}\\): \\[|\\mu(t,x) - \\mu(t,y)| + |\\sigma(t,x) - \\sigma(t,y)| \\leq K|x-y|\\]\n\nLinear growth condition: There exists \\(K &gt; 0\\) such that for all \\(t \\geq 0\\) and \\(x \\in \\mathbb{R}\\): \\[|\\mu(t,x)| + |\\sigma(t,x)| \\leq K(1 + |x|)\\]\n\nThen there exists a unique strong solution \\(X_t\\) such that \\(\\mathbb{E}[\\sup_{0 \\leq s \\leq t} |X_s|^2] &lt; \\infty\\) for all \\(t \\geq 0\\).\nProof Sketch: The proof uses Picard iteration combined with the Grönwall inequality. Define the sequence: \\[X_t^{(0)} = x_0\\] \\[X_t^{(n+1)} = x_0 + \\int_0^t \\mu(s, X_s^{(n)}) ds + \\int_0^t \\sigma(s, X_s^{(n)}) dW_s\\]\nThe Lipschitz condition ensures the sequence converges uniformly, while the growth condition guarantees the limit has finite moments. □\n\n\nCode\ndef lipschitz_example_sde(x, t):\n    \"\"\"Example SDE coefficients satisfying Lipschitz conditions.\"\"\"\n    mu = -0.5 * x  # Linear drift (Lipschitz constant = 0.5)\n    sigma = 0.3 * (1 + 0.1 * x)  # Near-constant diffusion (Lipschitz constant ≈ 0.03)\n    return mu, sigma\n\ndef non_lipschitz_example_sde(x, t):\n    \"\"\"Example with non-Lipschitz coefficient leading to non-uniqueness.\"\"\"\n    mu = 0.0\n    sigma = np.sqrt(np.abs(x))  # Non-Lipschitz at x=0\n    return mu, sigma\n\ndef euler_maruyama_step(x, dt, sde_func, t, dW):\n    \"\"\"Single Euler-Maruyama step.\"\"\"\n    mu, sigma = sde_func(x, t)\n    return x + mu * dt + sigma * dW\n\ndef simulate_sde_euler(x0, T, N, sde_func, n_paths=1):\n    \"\"\"Simulate SDE using Euler-Maruyama scheme.\"\"\"\n    dt = T / N\n    sqrt_dt = np.sqrt(dt)\n    t = np.linspace(0, T, N + 1)\n    \n    X = np.zeros((n_paths, N + 1))\n    X[:, 0] = x0\n    \n    for i in range(N):\n        dW = np.random.randn(n_paths) * sqrt_dt\n        for j in range(n_paths):\n            X[j, i+1] = euler_maruyama_step(X[j, i], dt, sde_func, t[i], dW[j])\n    \n    return t, X\n\n# Simulation parameters\nT = 2.0\nN = 1000\nn_paths = 100\nx0 = 1.0\n\nnp.random.seed(42)\n\nfig, axes = plt.subplots(3, 2, figsize=(16, 12))\n\n# Lipschitz case: Unique solutions\nt_lip, X_lip = simulate_sde_euler(x0, T, N, lipschitz_example_sde, n_paths=5)\n\nfor i in range(X_lip.shape[0]):\n    axes[0, 0].plot(t_lip, X_lip[i], alpha=0.4, linewidth=0.8, color='blue')\naxes[0, 0].plot(t_lip, X_lip[0], color='red', linewidth=2, label='Sample path')\naxes[0, 0].set_title('Lipschitz Case: dX = -0.5X dt + 0.3(1+0.1X) dW\\n(Unique Solution)')\naxes[0, 0].set_xlabel('Time t')\naxes[0, 0].set_ylabel('X(t)')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Distribution evolution for Lipschitz case\ntimes_to_plot = [0.5, 1.0, 1.5, 2.0]\ncolors = ['blue', 'green', 'orange', 'red']\n\nfor i, (time_point, color) in enumerate(zip(times_to_plot, colors)):\n    time_idx = int(time_point * N / T)\n    values = X_lip[:, time_idx]\n    \n    # Kernel density estimation for smooth histogram\n    from scipy.stats import gaussian_kde\n    kde = gaussian_kde(values)\n    x_range = np.linspace(values.min() - 0.5, values.max() + 0.5, 100)\n    density = kde(x_range)\n    \n    axes[1, 0].fill_between(x_range, density, alpha=0.3, color=color, \n                           label=f't = {time_point}')\n    axes[1, 0].plot(x_range, density, color=color, linewidth=2)\n\naxes[1, 0].set_title('Distribution Evolution (Lipschitz Case)')\naxes[1, 0].set_xlabel('X(t)')\naxes[1, 0].set_ylabel('Density')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Mean and variance evolution\nmeans_lip = np.mean(X_lip, axis=0)\nvars_lip = np.var(X_lip, axis=0)\n\naxes[2, 0].plot(t_lip, means_lip, 'b-', linewidth=2, label='Empirical mean')\naxes[2, 0].plot(t_lip, vars_lip, 'r-', linewidth=2, label='Empirical variance')\naxes[2, 0].set_title('Moment Evolution (Lipschitz Case)')\naxes[2, 0].set_xlabel('Time t')\naxes[2, 0].set_ylabel('Moments')\naxes[2, 0].legend()\naxes[2, 0].grid(True, alpha=0.3)\n\n# Non-Lipschitz case: Potential non-uniqueness\n# Start very close to zero to illustrate the issue\nx0_small = 0.01\nt_nonlip, X_nonlip = simulate_sde_euler(x0_small, T, N, non_lipschitz_example_sde, n_paths=5)\n\nfor i in range(X_nonlip.shape[0]):\n    axes[0, 1].plot(t_nonlip, X_nonlip[i], alpha=0.4, linewidth=0.8, color='purple')\naxes[0, 1].plot(t_nonlip, X_nonlip[0], color='red', linewidth=2, label='Sample path')\naxes[0, 1].set_title('Non-Lipschitz Case: dX = √|X| dW\\n(Starting near zero)')\naxes[0, 1].set_xlabel('Time t')\naxes[0, 1].set_ylabel('X(t)')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Show the pathological behavior near zero\naxes[1, 1].hist(X_nonlip[:, N//2], bins=30, density=True, alpha=0.7, color='lightcoral')\naxes[1, 1].set_title('Distribution at t=1.0 (Non-Lipschitz Case)')\naxes[1, 1].set_xlabel('X(1)')\naxes[1, 1].set_ylabel('Density')\naxes[1, 1].grid(True, alpha=0.3)\n\n# Coefficient comparison\nx_range = np.linspace(-2, 2, 100)\nmu_lip = np.array([-0.5 * x for x in x_range])\nsigma_lip = np.array([0.3 * (1 + 0.1 * x) for x in x_range])\nsigma_nonlip = np.sqrt(np.abs(x_range))\n\naxes[2, 1].plot(x_range, mu_lip, 'b-', linewidth=2, label='μ(x) = -0.5x (Lipschitz)')\naxes[2, 1].plot(x_range, sigma_lip, 'g-', linewidth=2, label='σ(x) = 0.3(1+0.1x) (Lipschitz)')\naxes[2, 1].plot(x_range, sigma_nonlip, 'r-', linewidth=2, label='σ(x) = √|x| (Non-Lipschitz)')\naxes[2, 1].set_title('Coefficient Functions')\naxes[2, 1].set_xlabel('x')\naxes[2, 1].set_ylabel('Coefficient value')\naxes[2, 1].legend()\naxes[2, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Demonstrate Lipschitz constant computation\nprint(\"Lipschitz Constant Analysis:\")\nprint(\"For μ(x) = -0.5x: |μ(x) - μ(y)| = 0.5|x - y|, so L_μ = 0.5\")\nprint(\"For σ(x) = 0.3(1 + 0.1x): |σ(x) - σ(y)| = 0.03|x - y|, so L_σ = 0.03\")\nprint(\"Combined Lipschitz constant: K = L_μ + L_σ = 0.53\")\nprint()\nprint(\"For σ(x) = √|x|: |σ(x) - σ(y)| / |x - y| → ∞ as x,y → 0\")\nprint(\"This violates the Lipschitz condition at x = 0\")\n\n\n\n\n\n\n\n\nFigure 6: Illustration of existence and uniqueness theory through pathwise solutions\n\n\n\n\n\nLipschitz Constant Analysis:\nFor μ(x) = -0.5x: |μ(x) - μ(y)| = 0.5|x - y|, so L_μ = 0.5\nFor σ(x) = 0.3(1 + 0.1x): |σ(x) - σ(y)| = 0.03|x - y|, so L_σ = 0.03\nCombined Lipschitz constant: K = L_μ + L_σ = 0.53\n\nFor σ(x) = √|x|: |σ(x) - σ(y)| / |x - y| → ∞ as x,y → 0\nThis violates the Lipschitz condition at x = 0\n\n\n\n\n4.3 The Markov Property and Generator\nDefinition 4.4 (Markov Property): A process \\(X_t\\) has the Markov property if for any measurable function \\(f\\) and times \\(0 \\leq s &lt; t\\): \\[\\mathbb{E}[f(X_t) | \\mathcal{F}_s] = \\mathbb{E}[f(X_t) | X_s]\\]\nTheorem 4.5: Solutions to SDEs possess the strong Markov property.\nDefinition 4.6 (Infinitesimal Generator): For an SDE \\(dX_t = \\mu(X_t) dt + \\sigma(X_t) dW_t\\), the infinitesimal generator \\(\\mathcal{A}\\) is defined as: \\[\\mathcal{A}f(x) = \\mu(x) f'(x) + \\frac{1}{2}\\sigma^2(x) f''(x)\\]\nfor functions \\(f \\in C^2(\\mathbb{R})\\).\nTheorem 4.7 (Dynkin’s Formula): If \\(\\tau\\) is a stopping time with \\(\\mathbb{E}[\\tau] &lt; \\infty\\) and \\(f \\in C^2\\) with appropriate growth conditions, then: \\[\\mathbb{E}[f(X_\\tau)] = f(X_0) + \\mathbb{E}\\left[\\int_0^\\tau \\mathcal{A}f(X_s) ds\\right]\\]\n\n\n4.4 Feynman-Kac Theorem\nOne of the most profound connections in mathematical analysis links stochastic differential equations with partial differential equations through the Feynman-Kac theorem.\nTheorem 4.8 (Feynman-Kac): Consider the PDE: \\[\\frac{\\partial u}{\\partial t} + \\mu(x) \\frac{\\partial u}{\\partial x} + \\frac{1}{2}\\sigma^2(x) \\frac{\\partial^2 u}{\\partial x^2} + c(x)u = 0\\]\nwith terminal condition \\(u(T,x) = g(x)\\). If \\(X_t\\) solves: \\[dX_t = \\mu(X_t) dt + \\sigma(X_t) dW_t, \\quad X_0 = x\\]\nthen: \\[u(t,x) = \\mathbb{E}\\left[g(X_T) \\exp\\left(-\\int_t^T c(X_s) ds\\right) \\bigg| X_t = x\\right]\\]\nThis theorem provides the foundation for Monte Carlo methods in finance and connects probabilistic and analytical approaches to solving PDEs.\n\n\nCode\ndef feynman_kac_mc(x0, T, mu_func, sigma_func, c_func, g_func, n_paths=5000, n_steps=500):\n    \"\"\"\n    Solve PDE using Feynman-Kac theorem via Monte Carlo.\n    \n    Returns u(0, x0) = E[g(X_T) * exp(-∫₀ᵀ c(X_s) ds) | X_0 = x0]\n    \"\"\"\n    dt = T / n_steps\n    sqrt_dt = np.sqrt(dt)\n    \n    # Storage for paths and integrals\n    X = np.zeros((n_paths, n_steps + 1))\n    X[:, 0] = x0\n    integral_c = np.zeros(n_paths)\n    \n    # Simulate paths\n    for i in range(n_steps):\n        t = i * dt\n        dW = np.random.randn(n_paths) * sqrt_dt\n        \n        for j in range(n_paths):\n            mu = mu_func(X[j, i], t)\n            sigma = sigma_func(X[j, i], t)\n            X[j, i+1] = X[j, i] + mu * dt + sigma * dW[j]\n            \n            # Accumulate integral of c(X_s)\n            integral_c[j] += c_func(X[j, i], t) * dt\n    \n    # Final payoff with discounting\n    payoffs = g_func(X[:, -1]) * np.exp(-integral_c)\n    \n    return np.mean(payoffs), np.std(payoffs) / np.sqrt(n_paths), X\n\n# Example: Heat equation with killing\n# PDE: ∂u/∂t + (1/2)∂²u/∂x² - ru = 0\n# Terminal condition: u(T,x) = max(x - K, 0) (call option payoff)\n\ndef mu_heat(x, t):\n    return 0.0  # No drift\n\ndef sigma_heat(x, t):\n    return 1.0  # Unit diffusion\n\ndef c_heat(x, t):\n    return 0.05  # Killing rate (interest rate)\n\ndef g_call(x, K=1.0):\n    return np.maximum(x - K, 0)  # Call option payoff\n\n# Parameters\nT = 1.0\nx_values_base = np.linspace(-1, 3, 7)  # Reduced points\nx_values_additional = np.array([0.5, 1.0, 1.5])\nx_values = np.sort(np.unique(np.concatenate((x_values_base, x_values_additional))))\nK = 1.0\nn_paths = 5000  # Reduced for faster rendering\n\nnp.random.seed(42)\n\n# Compute solution at different initial points\nmc_solutions = []\nmc_errors = []\nsample_paths = {}\n\nfor x0 in x_values:\n    u_mc, error, paths = feynman_kac_mc(x0, T, mu_heat, sigma_heat, c_heat, \n                                       lambda x: g_call(x, K), n_paths)\n    mc_solutions.append(u_mc)\n    mc_errors.append(error)\n    # Store sample paths for visualization if x0 is close to 0.5, 1.0, or 1.5\n    if np.isclose(x0, 0.5):\n        sample_paths['0.5'] = paths\n    elif np.isclose(x0, 1.0):\n        sample_paths['1.0'] = paths\n    elif np.isclose(x0, 1.5):\n        sample_paths['1.5'] = paths\n\nmc_solutions = np.array(mc_solutions)\nmc_errors = np.array(mc_errors)\n\n# Analytical solution for comparison (Black-Scholes with r=0.05, σ=1, T=1)\ndef black_scholes_call(S, K, T, r, sigma):\n    from scipy.stats import norm\n    d1 = (np.log(S/K) + (r + 0.5*sigma**2)*T) / (sigma*np.sqrt(T))\n    d2 = d1 - sigma*np.sqrt(T)\n    return S * norm.cdf(d1) - K * np.exp(-r*T) * norm.cdf(d2)\n\nanalytical_solutions = black_scholes_call(x_values, K, T, 0.05, 1.0)\n\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Solution comparison\naxes[0, 0].plot(x_values, analytical_solutions, 'r-', linewidth=3, label='Analytical (Black-Scholes)')\naxes[0, 0].errorbar(x_values, mc_solutions, yerr=2*mc_errors, fmt='bo-', \n                   capsize=3, capthick=1, label='Monte Carlo ± 2σ')\naxes[0, 0].set_title('PDE Solution: u(0,x) = E[max(X(T)-K,0)e^{-rT}]')\naxes[0, 0].set_xlabel('Initial value x')\naxes[0, 0].set_ylabel('u(0,x)')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Error analysis\nrelative_errors = np.abs(mc_solutions - analytical_solutions) / analytical_solutions\naxes[0, 1].semilogy(x_values, relative_errors, 'go-', markersize=6)\naxes[0, 1].set_title('Relative Error: |MC - Analytical| / Analytical')\naxes[0, 1].set_xlabel('Initial value x')\naxes[0, 1].set_ylabel('Relative Error')\naxes[0, 1].grid(True, alpha=0.3)\n\n# Sample paths for different initial conditions  \nt = np.linspace(0, T, 501)  # Match n_steps + 1\ncolors = ['blue', 'red', 'green']\nfor i, (x0_str, color) in enumerate(zip(['0.5', '1.0', '1.5'], colors)):\n    paths = sample_paths[x0_str]\n    # Plot subset of paths\n    for j in range(0, min(100, paths.shape[0]), 10):\n        axes[1, 0].plot(t, paths[j], color=color, alpha=0.3, linewidth=0.5)\n    # Highlight one path\n    axes[1, 0].plot(t, paths[0], color=color, linewidth=2, label=f'X₀ = {x0}')\n\naxes[1, 0].axhline(y=K, color='black', linestyle='--', alpha=0.7, label=f'Strike K = {K}')\naxes[1, 0].set_title('Sample Paths for Different Initial Conditions')\naxes[1, 0].set_xlabel('Time t')\naxes[1, 0].set_ylabel('X(t)')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Terminal distribution and payoff\nfinal_values = sample_paths['1.0'][:, -1]  # Terminal values starting from x=1\npayoffs = np.maximum(final_values - K, 0)\n\naxes[1, 1].hist(final_values, bins=50, density=True, alpha=0.6, color='skyblue', \n               label='X(T) distribution')\naxes[1, 1].hist(payoffs, bins=50, density=True, alpha=0.6, color='lightcoral', \n               label='Payoff distribution')\naxes[1, 1].axvline(x=K, color='black', linestyle='--', alpha=0.7, label=f'Strike K = {K}')\naxes[1, 1].set_title('Terminal Distribution and Payoff (X₀ = 1.0)')\naxes[1, 1].set_xlabel('Value')\naxes[1, 1].set_ylabel('Density')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Monte Carlo estimate at x=1.0: {mc_solutions[x_values == 1.0][0]:.4f} ± {2*mc_errors[x_values == 1.0][0]:.4f}\")\nprint(f\"Analytical solution at x=1.0:  {analytical_solutions[x_values == 1.0][0]:.4f}\")\nprint(f\"Relative error: {relative_errors[x_values == 1.0][0]:.2%}\")\n\n\n\n\n\n\n\n\nFigure 7: Feynman-Kac theorem illustration: PDE solution via Monte Carlo\n\n\n\n\n\nMonte Carlo estimate at x=1.0: 0.3707 ± 0.0157\nAnalytical solution at x=1.0:  0.3984\nRelative error: 6.96%"
  },
  {
    "objectID": "posts/03-04-2025_intro_to_sde/index.html#sec-numerical-methods",
    "href": "posts/03-04-2025_intro_to_sde/index.html#sec-numerical-methods",
    "title": "Stochastic Differential Equations: From Mathematical Foundations to Modern Applications",
    "section": "5 Numerical Methods for SDEs",
    "text": "5 Numerical Methods for SDEs\nWhile analytical solutions to SDEs exist only in special cases, numerical methods provide essential tools for practical applications. We examine the fundamental discretization schemes and their convergence properties.\n\n5.1 Euler-Maruyama Scheme\nThe most basic numerical method for SDEs is the Euler-Maruyama scheme, which discretizes the SDE:\n\\[dX_t = \\mu(t, X_t) dt + \\sigma(t, X_t) dW_t\\]\nusing the approximation: \\[X_{n+1} = X_n + \\mu(t_n, X_n) \\Delta t + \\sigma(t_n, X_n) \\Delta W_n\\]\nwhere \\(\\Delta W_n = W_{t_{n+1}} - W_{t_n} \\sim \\mathcal{N}(0, \\Delta t)\\) are independent Gaussian increments.\nTheorem 5.1 (Strong Convergence of Euler-Maruyama): Under Lipschitz and linear growth conditions, the Euler-Maruyama scheme has strong convergence order 0.5: \\[\\mathbb{E}[|X_T - X_T^{\\Delta t}|] = O(\\sqrt{\\Delta t})\\]\nwhere \\(X_T^{\\Delta t}\\) is the numerical approximation at time \\(T\\).\n\n\n5.2 Milstein Scheme\nThe Milstein scheme improves upon Euler-Maruyama by including an additional correction term derived from Itô’s lemma, achieving higher-order strong convergence.\nDefinition 5.2 (Milstein Scheme): The Milstein discretization is: \\[X_{n+1} = X_n + \\mu(t_n, X_n) \\Delta t + \\sigma(t_n, X_n) \\Delta W_n + \\frac{1}{2}\\sigma(t_n, X_n)\\sigma'(t_n, X_n)[(\\Delta W_n)^2 - \\Delta t]\\]\nwhere \\(\\sigma'(t,x) = \\frac{\\partial \\sigma}{\\partial x}(t,x)\\).\nTheorem 5.3 (Strong Convergence of Milstein): Under appropriate regularity conditions, the Milstein scheme has strong convergence order 1.0: \\[\\mathbb{E}[|X_T - X_T^{\\Delta t}|] = O(\\Delta t)\\]\n\n\nCode\n@njit\ndef euler_maruyama_simulation(x0, T, N, mu_params, sigma_params, n_paths):\n    \"\"\"\n    Efficient Euler-Maruyama simulation using Numba.\n    Assumes linear drift μ(x) = a*x + b and linear diffusion σ(x) = c*x + d.\n    \"\"\"\n    dt = T / N\n    sqrt_dt = np.sqrt(dt)\n    \n    a, b = mu_params\n    c, d = sigma_params\n    \n    X = np.zeros((n_paths, N + 1))\n    X[:, 0] = x0\n    \n    for i in range(N):\n        dW = np.random.randn(n_paths) * sqrt_dt\n        X_curr = X[:, i]\n        mu = a * X_curr + b\n        sigma = c * X_curr + d\n        X[:, i+1] = X_curr + mu * dt + sigma * dW\n    \n    return X\n\n@njit\ndef milstein_simulation(x0, T, N, mu_params, sigma_params, n_paths):\n    \"\"\"\n    Efficient Milstein simulation using Numba.\n    Assumes linear drift μ(x) = a*x + b and linear diffusion σ(x) = c*x + d.\n    \"\"\"\n    dt = T / N\n    sqrt_dt = np.sqrt(dt)\n    \n    a, b = mu_params\n    c, d = sigma_params\n    \n    X = np.zeros((n_paths, N + 1))\n    X[:, 0] = x0\n    \n    for i in range(N):\n        dW = np.random.randn(n_paths) * sqrt_dt\n        X_curr = X[:, i]\n        mu = a * X_curr + b\n        sigma = c * X_curr + d\n        sigma_prime = c  # d/dx(c*x + d) = c\n        \n        # Milstein correction term\n        correction = 0.5 * sigma * sigma_prime * (dW**2 - dt)\n        \n        X[:, i+1] = X_curr + mu * dt + sigma * dW + correction\n    \n    return X\n\ndef analytical_solution_gbm(x0, mu, sigma, T, N, n_paths):\n    \"\"\"Analytical solution for geometric Brownian motion.\"\"\"\n    dt = T / N\n    t = np.linspace(0, T, N + 1)\n    \n    # Generate Brownian motion\n    dW = np.random.randn(n_paths, N) * np.sqrt(dt)\n    W = np.column_stack([np.zeros(n_paths), np.cumsum(dW, axis=1)])\n    \n    # Exact solution: X(t) = x0 * exp((mu - sigma²/2)t + sigma*W(t))\n    X_exact = x0 * np.exp((mu - 0.5 * sigma**2) * t[np.newaxis, :] + sigma * W)\n    \n    return X_exact\n\n# Test case: Geometric Brownian Motion dX = μX dt + σX dW\nx0 = 1.0\nmu = 0.1\nsigma = 0.3\nT = 1.0\n\n# Parameters for linear approximation: μ(x) = μ*x, σ(x) = σ*x\nmu_params = (mu, 0.0)  # a=μ, b=0\nsigma_params = (sigma, 0.0)  # c=σ, d=0\n\n# Convergence study\nstep_sizes = np.array([500, 1000, 2000, 4000, 8000])\nn_paths_convergence = 5000\n\nerrors_euler = []\nerrors_milstein = []\n\nnp.random.seed(42)\n\nfig, axes = plt.subplots(3, 2, figsize=(18, 14))\n\nprint(\"Convergence Analysis:\")\nprint(\"N\\t\\tEuler Error\\t\\tMilstein Error\\t\\tRatio\")\nprint(\"-\" * 60)\n\nfor N in step_sizes:\n    # Set random seed for fair comparison\n    np.random.seed(42)\n    \n    # Analytical solution\n    X_exact = analytical_solution_gbm(x0, mu, sigma, T, N, n_paths_convergence)\n    \n    # Reset seed for numerical methods\n    np.random.seed(42)\n    X_euler = euler_maruyama_simulation(x0, T, N, mu_params, sigma_params, n_paths_convergence)\n    \n    np.random.seed(42)\n    X_milstein = milstein_simulation(x0, T, N, mu_params, sigma_params, n_paths_convergence)\n    \n    # Compute strong errors (L1 norm at terminal time)\n    error_euler = np.mean(np.abs(X_euler[:, -1] - X_exact[:, -1]))\n    error_milstein = np.mean(np.abs(X_milstein[:, -1] - X_exact[:, -1]))\n    \n    errors_euler.append(error_euler)\n    errors_milstein.append(error_milstein)\n    \n    ratio = error_euler / error_milstein if error_milstein &gt; 0 else np.inf\n    print(f\"{N}\\t\\t{error_euler:.6f}\\t\\t{error_milstein:.6f}\\t\\t{ratio:.2f}\")\n\n# Convergence plots\ndt_values = T / step_sizes\naxes[0, 0].loglog(dt_values, errors_euler, 'bo-', label='Euler-Maruyama', markersize=8)\naxes[0, 0].loglog(dt_values, errors_milstein, 'rs-', label='Milstein', markersize=8)\n\n# Theoretical convergence rates\naxes[0, 0].loglog(dt_values, 0.1 * np.sqrt(dt_values), 'b--', alpha=0.7, label='O(√Δt)')\naxes[0, 0].loglog(dt_values, 0.02 * dt_values, 'r--', alpha=0.7, label='O(Δt)')\n\naxes[0, 0].set_xlabel('Step Size Δt')\naxes[0, 0].set_ylabel('Strong Error E[|X_T - X_T^Δt|]')\naxes[0, 0].set_title('Strong Convergence Rates')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Sample path comparison\nN_demo = 1000\nn_paths_demo = 50\n\nnp.random.seed(42)\nX_exact_demo = analytical_solution_gbm(x0, mu, sigma, T, N_demo, n_paths_demo)\n\nnp.random.seed(42)\nX_euler_demo = euler_maruyama_simulation(x0, T, N_demo, mu_params, sigma_params, n_paths_demo)\n\nnp.random.seed(42)\nX_milstein_demo = milstein_simulation(x0, T, N_demo, mu_params, sigma_params, n_paths_demo)\n\nt_demo = np.linspace(0, T, N_demo + 1)\n\n# Plot a few sample paths\nfor i in range(min(5, n_paths_demo)):\n    axes[0, 1].plot(t_demo, X_exact_demo[i], 'k-', alpha=0.6, linewidth=1)\n    axes[0, 1].plot(t_demo, X_euler_demo[i], 'b--', alpha=0.8, linewidth=1)\n    axes[0, 1].plot(t_demo, X_milstein_demo[i], 'r:', alpha=0.8, linewidth=1)\n\naxes[0, 1].plot([], [], 'k-', label='Exact')\naxes[0, 1].plot([], [], 'b--', label='Euler-Maruyama')\naxes[0, 1].plot([], [], 'r:', label='Milstein')\naxes[0, 1].set_title('Sample Path Comparison')\naxes[0, 1].set_xlabel('Time t')\naxes[0, 1].set_ylabel('X(t)')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Error distributions at terminal time\nerrors_euler_paths = X_euler_demo[:, -1] - X_exact_demo[:, -1]\nerrors_milstein_paths = X_milstein_demo[:, -1] - X_exact_demo[:, -1]\n\naxes[1, 0].hist(errors_euler_paths, bins=30, alpha=0.7, density=True, \n               color='blue', label=f'Euler (std={np.std(errors_euler_paths):.4f})')\naxes[1, 0].hist(errors_milstein_paths, bins=30, alpha=0.7, density=True, \n               color='red', label=f'Milstein (std={np.std(errors_milstein_paths):.4f})')\naxes[1, 0].axvline(0, color='black', linestyle='--', alpha=0.7)\naxes[1, 0].set_title('Error Distribution at Terminal Time')\naxes[1, 0].set_xlabel('Error: X_T^numerical - X_T^exact')\naxes[1, 0].set_ylabel('Density')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Weak convergence: Compare distributions\naxes[1, 1].hist(X_exact_demo[:, -1], bins=40, alpha=0.5, density=True, \n               color='black', label='Exact')\naxes[1, 1].hist(X_euler_demo[:, -1], bins=40, alpha=0.7, density=True, \n               color='blue', label='Euler-Maruyama')\naxes[1, 1].hist(X_milstein_demo[:, -1], bins=40, alpha=0.7, density=True, \n               color='red', label='Milstein')\naxes[1, 1].set_title('Terminal Distribution Comparison')\naxes[1, 1].set_xlabel('X(T)')\naxes[1, 1].set_ylabel('Density')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\n# Computational cost analysis\nstep_sizes_cost = np.array([100, 200, 500, 1000, 2000, 5000])\nn_paths_cost = 1000\n\nimport time\n\ntimes_euler = []\ntimes_milstein = []\n\nfor N in step_sizes_cost:\n    # Time Euler-Maruyama\n    start_time = time.time()\n    for _ in range(10):  # Average over multiple runs\n        np.random.seed(42)\n        X_euler_cost = euler_maruyama_simulation(x0, T, N, mu_params, sigma_params, n_paths_cost)\n    time_euler = (time.time() - start_time) / 10\n    times_euler.append(time_euler)\n    \n    # Time Milstein\n    start_time = time.time()\n    for _ in range(10):  # Average over multiple runs\n        np.random.seed(42)\n        X_milstein_cost = milstein_simulation(x0, T, N, mu_params, sigma_params, n_paths_cost)\n    time_milstein = (time.time() - start_time) / 10\n    times_milstein.append(time_milstein)\n\naxes[2, 0].loglog(step_sizes_cost, times_euler, 'bo-', label='Euler-Maruyama', markersize=8)\naxes[2, 0].loglog(step_sizes_cost, times_milstein, 'rs-', label='Milstein', markersize=8)\naxes[2, 0].set_xlabel('Number of Steps N')\naxes[2, 0].set_ylabel('Computation Time (seconds)')\naxes[2, 0].set_title('Computational Cost Comparison')\naxes[2, 0].legend()\naxes[2, 0].grid(True, alpha=0.3)\n\n# Efficiency comparison: Error vs computational cost\naxes[2, 1].loglog(times_euler[-len(errors_euler):], errors_euler, 'bo-', \n                 label='Euler-Maruyama', markersize=8)\naxes[2, 1].loglog(times_milstein[-len(errors_milstein):], errors_milstein, 'rs-', \n                 label='Milstein', markersize=8)\naxes[2, 1].set_xlabel('Computation Time (seconds)')\naxes[2, 1].set_ylabel('Strong Error')\naxes[2, 1].set_title('Efficiency: Error vs Computational Cost')\naxes[2, 1].legend()\naxes[2, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Summary statistics\nprint(f\"\\nFinal comparison (N={step_sizes[-1]}):\")\nprint(f\"Euler-Maruyama error: {errors_euler[-1]:.6f}\")\nprint(f\"Milstein error: {errors_milstein[-1]:.6f}\")\nprint(f\"Improvement factor: {errors_euler[-1]/errors_milstein[-1]:.2f}x\")\n\n\nConvergence Analysis:\nN       Euler Error     Milstein Error      Ratio\n------------------------------------------------------------\n500     0.369727        0.366877        1.01\n1000        0.364152        0.362846        1.00\n2000        0.363420        0.366088        0.99\n4000        0.367194        0.376157        0.98\n8000        0.366508        0.368301        1.00\n\n\n\n\n\n\n\n\nFigure 8: Comparison of numerical methods for SDEs: Euler-Maruyama vs Milstein schemes\n\n\n\n\n\n\nFinal comparison (N=8000):\nEuler-Maruyama error: 0.366508\nMilstein error: 0.368301\nImprovement factor: 1.00x\n\n\n\n\n5.3 Higher-Order Methods and Multi-dimensional Extensions\nFor multi-dimensional SDEs: \\[d\\mathbf{X}_t = \\boldsymbol{\\mu}(t, \\mathbf{X}_t) dt + \\boldsymbol{\\sigma}(t, \\mathbf{X}_t) d\\mathbf{W}_t\\]\nthe schemes generalize naturally, but the Milstein scheme requires knowledge of mixed derivatives of the diffusion matrix.\nDefinition 5.4 (Multi-dimensional Euler-Maruyama): \\[\\mathbf{X}_{n+1} = \\mathbf{X}_n + \\boldsymbol{\\mu}(t_n, \\mathbf{X}_n) \\Delta t + \\boldsymbol{\\sigma}(t_n, \\mathbf{X}_n) \\Delta \\mathbf{W}_n\\]\nwhere \\(\\Delta \\mathbf{W}_n\\) are independent \\(m\\)-dimensional Gaussian vectors.\n\n\n5.4 Weak vs Strong Convergence\nDefinition 5.5: - Strong convergence measures pathwise accuracy: \\(\\mathbb{E}[|X_T - X_T^{\\Delta t}|] \\to 0\\) - Weak convergence measures distributional accuracy: \\(|\\mathbb{E}[f(X_T)] - \\mathbb{E}[f(X_T^{\\Delta t})]| \\to 0\\)\nFor many applications (e.g., option pricing), weak convergence is sufficient and can be achieved with larger step sizes.\n\n\nCode\ndef weak_convergence_study(payoff_func, step_sizes, n_paths=20000):\n    \"\"\"Study weak convergence for a given payoff function.\"\"\"\n    weak_errors_euler = []\n    weak_errors_milstein = []\n    \n    # Reference solution with very fine discretization\n    N_ref = 16000\n    np.random.seed(42)\n    X_ref = euler_maruyama_simulation(x0, T, N_ref, mu_params, sigma_params, n_paths)\n    exact_expectation = np.mean(payoff_func(X_ref[:, -1]))\n    \n    for N in step_sizes:\n        # Euler-Maruyama\n        np.random.seed(42)\n        X_euler = euler_maruyama_simulation(x0, T, N, mu_params, sigma_params, n_paths)\n        euler_expectation = np.mean(payoff_func(X_euler[:, -1]))\n        weak_error_euler = abs(euler_expectation - exact_expectation)\n        weak_errors_euler.append(weak_error_euler)\n        \n        # Milstein\n        np.random.seed(42)\n        X_milstein = milstein_simulation(x0, T, N, mu_params, sigma_params, n_paths)\n        milstein_expectation = np.mean(payoff_func(X_milstein[:, -1]))\n        weak_error_milstein = abs(milstein_expectation - exact_expectation)\n        weak_errors_milstein.append(weak_error_milstein)\n    \n    return weak_errors_euler, weak_errors_milstein, exact_expectation\n\n# Different payoff functions\ndef linear_payoff(x):\n    return x\n\ndef quadratic_payoff(x):\n    return x**2\n\ndef call_option_payoff(x, K=1.0):\n    return np.maximum(x - K, 0)\n\ndef digital_option_payoff(x, K=1.0):\n    return (x &gt; K).astype(float)\n\nstep_sizes_weak = np.array([50, 100, 200, 500, 1000, 2000])\npayoff_functions = [\n    (linear_payoff, \"Linear: E[X(T)]\"),\n    (quadratic_payoff, \"Quadratic: E[X²(T)]\"),\n    (lambda x: call_option_payoff(x, 1.0), \"Call Option: E[(X(T)-1)⁺]\"),\n    (lambda x: digital_option_payoff(x, 1.0), \"Digital Option: P(X(T)&gt;1)\")\n]\n\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\naxes = axes.flatten()\n\nfor i, (payoff_func, title) in enumerate(payoff_functions):\n    weak_errors_euler, weak_errors_milstein, exact_value = weak_convergence_study(\n        payoff_func, step_sizes_weak, n_paths=10000)\n    \n    dt_values = T / step_sizes_weak\n    \n    axes[i].loglog(dt_values, weak_errors_euler, 'bo-', label='Euler-Maruyama', markersize=6)\n    axes[i].loglog(dt_values, weak_errors_milstein, 'rs-', label='Milstein', markersize=6)\n    \n    # Theoretical weak convergence rates (typically one order higher than strong)\n    axes[i].loglog(dt_values, 0.01 * dt_values, 'b--', alpha=0.7, label='O(Δt)')\n    axes[i].loglog(dt_values, 0.001 * dt_values**2, 'r--', alpha=0.7, label='O(Δt²)')\n    \n    axes[i].set_xlabel('Step Size Δt')\n    axes[i].set_ylabel('Weak Error |E[f(X_T)] - E[f(X_T^Δt)]|')\n    axes[i].set_title(f'{title}\\n(Exact: {exact_value:.4f})')\n    axes[i].legend()\n    axes[i].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 9: Weak vs strong convergence illustration\n\n\n\n\n\nThe numerical analysis demonstrates several key insights:\n\nStrong convergence: Euler-Maruyama achieves \\(O(\\sqrt{\\Delta t})\\) while Milstein achieves \\(O(\\Delta t)\\)\nWeak convergence: Both methods typically achieve one order higher convergence for smooth payoffs\nComputational cost: Milstein requires additional derivative calculations but provides better accuracy\nPractical choice: The optimal method depends on the specific application and computational budget\n\nThese numerical methods provide the computational foundation for practical SDE applications in finance, engineering, and machine learning. In the next section, we explore their application to financial modeling and option pricing."
  },
  {
    "objectID": "posts/03-04-2025_intro_to_sde/index.html#sec-finance-applications",
    "href": "posts/03-04-2025_intro_to_sde/index.html#sec-finance-applications",
    "title": "Stochastic Differential Equations: From Mathematical Foundations to Modern Applications",
    "section": "6 Applications in Mathematical Finance",
    "text": "6 Applications in Mathematical Finance\nStochastic differential equations form the mathematical backbone of modern quantitative finance. The revolutionary insight that asset prices follow stochastic processes led to the development of rigorous option pricing theory and sophisticated risk management frameworks.\n\n6.1 The Black-Scholes Model\nThe Black-Scholes model represents the foundational application of SDEs in finance, providing the first rigorous framework for option pricing.\nModel Specification: Under the Black-Scholes framework, the stock price \\(S_t\\) follows geometric Brownian motion: \\[dS_t = \\mu S_t dt + \\sigma S_t dW_t\\]\nwhere: - \\(\\mu\\) is the expected return (drift) - \\(\\sigma\\) is the volatility - \\(W_t\\) is Brownian motion under the physical measure\nRisk-Neutral Pricing: The fundamental theorem of asset pricing requires pricing under the risk-neutral measure \\(\\mathbb{Q}\\), where: \\[dS_t = r S_t dt + \\sigma S_t dW_t^{\\mathbb{Q}}\\]\nwhere \\(r\\) is the risk-free rate and \\(W_t^{\\mathbb{Q}}\\) is Brownian motion under \\(\\mathbb{Q}\\).\nTheorem 6.1 (Black-Scholes Formula): The price at time \\(t\\) of a European call option with strike \\(K\\) and maturity \\(T\\) is:\n\\[C(t, S_t) = S_t \\Phi(d_1) - K e^{-r(T-t)} \\Phi(d_2)\\]\nwhere: \\[d_1 = \\frac{\\ln(S_t/K) + (r + \\sigma^2/2)(T-t)}{\\sigma\\sqrt{T-t}}, \\quad d_2 = d_1 - \\sigma\\sqrt{T-t}\\]\nand \\(\\Phi\\) is the standard normal cumulative distribution function.\nDerivation: The derivation follows from the Feynman-Kac theorem applied to the Black-Scholes PDE: \\[\\frac{\\partial V}{\\partial t} + \\frac{1}{2}\\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} + rS \\frac{\\partial V}{\\partial S} - rV = 0\\]\nwith terminal condition \\(V(T,S) = \\max(S-K, 0)\\).\n\n\nCode\ndef black_scholes_call(S, K, T, r, sigma):\n    \"\"\"Black-Scholes call option price.\"\"\"\n    d1 = (np.log(S/K) + (r + 0.5*sigma**2)*T) / (sigma*np.sqrt(T))\n    d2 = d1 - sigma*np.sqrt(T)\n    return S * stats.norm.cdf(d1) - K * np.exp(-r*T) * stats.norm.cdf(d2)\n\ndef black_scholes_put(S, K, T, r, sigma):\n    \"\"\"Black-Scholes put option price.\"\"\"\n    d1 = (np.log(S/K) + (r + 0.5*sigma**2)*T) / (sigma*np.sqrt(T))\n    d2 = d1 - sigma*np.sqrt(T)\n    return K * np.exp(-r*T) * stats.norm.cdf(-d2) - S * stats.norm.cdf(-d1)\n\ndef calculate_greeks(S, K, T, r, sigma):\n    \"\"\"Calculate option Greeks.\"\"\"\n    d1 = (np.log(S/K) + (r + 0.5*sigma**2)*T) / (sigma*np.sqrt(T))\n    d2 = d1 - sigma*np.sqrt(T)\n    \n    # Delta\n    delta_call = stats.norm.cdf(d1)\n    delta_put = stats.norm.cdf(d1) - 1\n    \n    # Gamma\n    gamma = stats.norm.pdf(d1) / (S * sigma * np.sqrt(T))\n    \n    # Theta\n    theta_call = (-S * stats.norm.pdf(d1) * sigma / (2 * np.sqrt(T)) \n                  - r * K * np.exp(-r*T) * stats.norm.cdf(d2))\n    theta_put = (-S * stats.norm.pdf(d1) * sigma / (2 * np.sqrt(T)) \n                 + r * K * np.exp(-r*T) * stats.norm.cdf(-d2))\n    \n    # Vega\n    vega = S * stats.norm.pdf(d1) * np.sqrt(T)\n    \n    # Rho\n    rho_call = K * T * np.exp(-r*T) * stats.norm.cdf(d2)\n    rho_put = -K * T * np.exp(-r*T) * stats.norm.cdf(-d2)\n    \n    return {\n        'delta_call': delta_call, 'delta_put': delta_put,\n        'gamma': gamma,\n        'theta_call': theta_call, 'theta_put': theta_put,\n        'vega': vega,\n        'rho_call': rho_call, 'rho_put': rho_put\n    }\n\ndef monte_carlo_option_price(S0, K, T, r, sigma, n_paths=50000, option_type='call'):\n    \"\"\"Monte Carlo option pricing.\"\"\"\n    dt = T / 252  # Daily steps\n    n_steps = int(T / dt)\n    \n    # Generate stock price paths\n    paths = np.zeros((n_paths, n_steps + 1))\n    paths[:, 0] = S0\n    \n    for i in range(n_steps):\n        Z = np.random.randn(n_paths)\n        paths[:, i+1] = paths[:, i] * np.exp((r - 0.5*sigma**2)*dt + sigma*np.sqrt(dt)*Z)\n    \n    # Calculate payoffs\n    if option_type == 'call':\n        payoffs = np.maximum(paths[:, -1] - K, 0)\n    else:  # put\n        payoffs = np.maximum(K - paths[:, -1], 0)\n    \n    # Discount to present value\n    price = np.exp(-r*T) * np.mean(payoffs)\n    std_error = np.exp(-r*T) * np.std(payoffs) / np.sqrt(n_paths)\n    \n    return price, std_error, paths\n\n# Parameters\nS0 = 100  # Initial stock price\nK = 100   # Strike price\nr = 0.05  # Risk-free rate\nsigma = 0.2  # Volatility\nT_range = np.linspace(0.1, 2, 50)  # Time to maturity range\nS_range = np.linspace(80, 120, 50)  # Stock price range\n\nfig, axes = plt.subplots(4, 2, figsize=(18, 16))\n\n# Option prices vs underlying price\ncall_prices = [black_scholes_call(S, K, 0.5, r, sigma) for S in S_range]\nput_prices = [black_scholes_put(S, K, 0.5, r, sigma) for S in S_range]\n\naxes[0, 0].plot(S_range, call_prices, 'b-', linewidth=2, label='Call Option')\naxes[0, 0].plot(S_range, put_prices, 'r-', linewidth=2, label='Put Option')\naxes[0, 0].plot(S_range, np.maximum(S_range - K, 0), 'b--', alpha=0.7, label='Call Intrinsic')\naxes[0, 0].plot(S_range, np.maximum(K - S_range, 0), 'r--', alpha=0.7, label='Put Intrinsic')\naxes[0, 0].set_xlabel('Stock Price S')\naxes[0, 0].set_ylabel('Option Price')\naxes[0, 0].set_title('Option Prices vs Underlying Price (T=0.5)')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Option prices vs time to maturity\ncall_prices_time = [black_scholes_call(S0, K, T, r, sigma) for T in T_range]\nput_prices_time = [black_scholes_put(S0, K, T, r, sigma) for T in T_range]\n\naxes[0, 1].plot(T_range, call_prices_time, 'b-', linewidth=2, label='Call Option')\naxes[0, 1].plot(T_range, put_prices_time, 'r-', linewidth=2, label='Put Option')\naxes[0, 1].set_xlabel('Time to Maturity T')\naxes[0, 1].set_ylabel('Option Price')\naxes[0, 1].set_title(f'Option Prices vs Time to Maturity (S={S0})')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Greeks calculation\ngreeks_data = [calculate_greeks(S, K, 0.5, r, sigma) for S in S_range]\n\n# Delta\ndeltas_call = [g['delta_call'] for g in greeks_data]\ndeltas_put = [g['delta_put'] for g in greeks_data]\n\naxes[1, 0].plot(S_range, deltas_call, 'b-', linewidth=2, label='Call Delta')\naxes[1, 0].plot(S_range, deltas_put, 'r-', linewidth=2, label='Put Delta')\naxes[1, 0].axhline(y=0, color='black', linestyle='--', alpha=0.5)\naxes[1, 0].axhline(y=0.5, color='blue', linestyle=':', alpha=0.5)\naxes[1, 0].set_xlabel('Stock Price S')\naxes[1, 0].set_ylabel('Delta')\naxes[1, 0].set_title('Delta: Price Sensitivity')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Gamma\ngammas = [g['gamma'] for g in greeks_data]\n\naxes[1, 1].plot(S_range, gammas, 'g-', linewidth=2, label='Gamma')\naxes[1, 1].set_xlabel('Stock Price S')\naxes[1, 1].set_ylabel('Gamma')\naxes[1, 1].set_title('Gamma: Delta Sensitivity')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\n# Vega\nvegas = [g['vega'] for g in greeks_data]\n\naxes[2, 0].plot(S_range, vegas, 'purple', linewidth=2, label='Vega')\naxes[2, 0].set_xlabel('Stock Price S')\naxes[2, 0].set_ylabel('Vega')\naxes[2, 0].set_title('Vega: Volatility Sensitivity')\naxes[2, 0].legend()\naxes[2, 0].grid(True, alpha=0.3)\n\n# Theta\nthetas_call = [g['theta_call'] for g in greeks_data]\nthetas_put = [g['theta_put'] for g in greeks_data]\n\naxes[2, 1].plot(S_range, thetas_call, 'b-', linewidth=2, label='Call Theta')\naxes[2, 1].plot(S_range, thetas_put, 'r-', linewidth=2, label='Put Theta')\naxes[2, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\naxes[2, 1].set_xlabel('Stock Price S')\naxes[2, 1].set_ylabel('Theta')\naxes[2, 1].set_title('Theta: Time Decay')\naxes[2, 1].legend()\naxes[2, 1].grid(True, alpha=0.3)\n\n# Monte Carlo vs Black-Scholes comparison\nnp.random.seed(42)\nmc_call_price, mc_call_error, sample_paths = monte_carlo_option_price(S0, K, 0.5, r, sigma, 50000, 'call')\nbs_call_price = black_scholes_call(S0, K, 0.5, r, sigma)\n\n# Plot sample paths\nfor i in range(min(50, sample_paths.shape[0])):\n    axes[3, 0].plot(np.linspace(0, 0.5, sample_paths.shape[1]), sample_paths[i], \n                   alpha=0.3, linewidth=0.5, color='blue')\n\naxes[3, 0].axhline(y=K, color='red', linestyle='--', linewidth=2, label=f'Strike K={K}')\naxes[3, 0].axhline(y=S0, color='green', linestyle='-', linewidth=2, label=f'Initial S₀={S0}')\naxes[3, 0].set_xlabel('Time')\naxes[3, 0].set_ylabel('Stock Price')\naxes[3, 0].set_title('Monte Carlo Sample Paths')\naxes[3, 0].legend()\naxes[3, 0].grid(True, alpha=0.3)\n\n# Terminal distribution and payoff\nterminal_prices = sample_paths[:, -1]\npayoffs = np.maximum(terminal_prices - K, 0)\n\naxes[3, 1].hist(terminal_prices, bins=50, alpha=0.6, density=True, color='skyblue', \n               label='Terminal Stock Price')\naxes[3, 1].hist(payoffs, bins=50, alpha=0.6, density=True, color='lightcoral', \n               label='Call Payoff')\naxes[3, 1].axvline(x=K, color='red', linestyle='--', linewidth=2, label=f'Strike K={K}')\naxes[3, 1].set_xlabel('Price/Payoff')\naxes[3, 1].set_ylabel('Density')\naxes[3, 1].set_title(f'Terminal Distribution\\nMC: {mc_call_price:.4f}±{2*mc_call_error:.4f}, BS: {bs_call_price:.4f}')\naxes[3, 1].legend()\naxes[3, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Black-Scholes vs Monte Carlo Comparison:\")\nprint(f\"Black-Scholes Price: {bs_call_price:.6f}\")\nprint(f\"Monte Carlo Price:   {mc_call_price:.6f} ± {2*mc_call_error:.6f}\")\nprint(f\"Difference:          {abs(bs_call_price - mc_call_price):.6f}\")\nprint(f\"Relative Error:      {abs(bs_call_price - mc_call_price)/bs_call_price:.4%}\")\n\n\n\n\n\n\n\n\nFigure 10: Black-Scholes model: Option pricing and Greeks analysis\n\n\n\n\n\nBlack-Scholes vs Monte Carlo Comparison:\nBlack-Scholes Price: 6.888729\nMonte Carlo Price:   6.867335 ± 0.087221\nDifference:          0.021394\nRelative Error:      0.3106%\n\n\n\n\n6.2 Interest Rate Models\nInterest rate modeling requires more sophisticated SDEs due to the mean-reverting nature of rates and term structure considerations.\n\n6.2.1 Vasicek Model\nModel Specification: The Vasicek model describes the short rate \\(r_t\\) as: \\[dr_t = \\kappa(\\theta - r_t) dt + \\sigma dW_t\\]\nwhere: - \\(\\kappa &gt; 0\\) is the speed of mean reversion - \\(\\theta\\) is the long-term mean - \\(\\sigma &gt; 0\\) is the volatility\nAnalytical Solution: The Vasicek model has the explicit solution: \\[r_t = r_0 e^{-\\kappa t} + \\theta (1 - e^{-\\kappa t}) + \\sigma \\int_0^t e^{-\\kappa(t-s)} dW_s\\]\n\n\n6.2.2 Cox-Ingersoll-Ross (CIR) Model\nModel Specification: The CIR model ensures non-negative rates: \\[dr_t = \\kappa(\\theta - r_t) dt + \\sigma \\sqrt{r_t} dW_t\\]\nThe square-root diffusion term prevents negative rates when \\(2\\kappa\\theta \\geq \\sigma^2\\) (Feller condition).\n\n\nCode\ndef simulate_vasicek(r0, kappa, theta, sigma, T, N, n_paths=1):\n    \"\"\"Simulate Vasicek interest rate model.\"\"\"\n    dt = T / N\n    sqrt_dt = np.sqrt(dt)\n    t = np.linspace(0, T, N + 1)\n    \n    r = np.zeros((n_paths, N + 1))\n    r[:, 0] = r0\n    \n    for i in range(N):\n        dW = np.random.randn(n_paths) * sqrt_dt\n        r[:, i+1] = (r[:, i] + kappa * (theta - r[:, i]) * dt + sigma * dW)\n    \n    return t, r\n\ndef simulate_cir(r0, kappa, theta, sigma, T, N, n_paths=1):\n    \"\"\"Simulate CIR interest rate model using Euler scheme with absorption.\"\"\"\n    dt = T / N\n    sqrt_dt = np.sqrt(dt)\n    t = np.linspace(0, T, N + 1)\n    \n    r = np.zeros((n_paths, N + 1))\n    r[:, 0] = r0\n    \n    for i in range(N):\n        dW = np.random.randn(n_paths) * sqrt_dt\n        r_curr = np.maximum(r[:, i], 0)  # Ensure non-negative\n        r[:, i+1] = (r_curr + kappa * (theta - r_curr) * dt + \n                     sigma * np.sqrt(r_curr) * dW)\n        r[:, i+1] = np.maximum(r[:, i+1], 0)  # Absorb at zero\n    \n    return t, r\n\ndef vasicek_bond_price(r, tau, kappa, theta, sigma):\n    \"\"\"Vasicek zero-coupon bond price.\"\"\"\n    B = (1 - np.exp(-kappa * tau)) / kappa\n    A = np.exp((B - tau) * (kappa**2 * theta - sigma**2/2) / kappa**2 - \n               sigma**2 * B**2 / (4 * kappa))\n    return A * np.exp(-B * r)\n\ndef cir_bond_price(r, tau, kappa, theta, sigma):\n    \"\"\"CIR zero-coupon bond price (approximate).\"\"\"\n    gamma = np.sqrt(kappa**2 + 2*sigma**2)\n    exp_gamma_tau = np.exp(gamma * tau)\n    \n    B = 2 * (exp_gamma_tau - 1) / ((gamma + kappa) * (exp_gamma_tau - 1) + 2 * gamma)\n    A = (2 * gamma * exp_gamma_tau) / ((gamma + kappa) * (exp_gamma_tau - 1) + 2 * gamma)\n    A = A**(2 * kappa * theta / sigma**2)\n    \n    return A * np.exp(-B * r)\n\n# Model parameters\nr0 = 0.03\nkappa = 0.5\ntheta = 0.04\nsigma_vasicek = 0.01\nsigma_cir = 0.05\nT = 10\nN = 2000\nn_paths = 1000\n\nnp.random.seed(42)\n\nfig, axes = plt.subplots(3, 2, figsize=(18, 14))\n\n# Simulate both models\nt, r_vasicek = simulate_vasicek(r0, kappa, theta, sigma_vasicek, T, N, n_paths)\nt, r_cir = simulate_cir(r0, kappa, theta, sigma_cir, T, N, n_paths)\n\n# Sample paths\nfor i in range(min(50, n_paths)):\n    axes[0, 0].plot(t, r_vasicek[i], alpha=0.3, linewidth=0.5, color='blue')\n    axes[0, 1].plot(t, r_cir[i], alpha=0.3, linewidth=0.5, color='red')\n\naxes[0, 0].plot(t, r_vasicek[0], color='darkblue', linewidth=2, label='Sample path')\naxes[0, 0].axhline(y=theta, color='green', linestyle='--', linewidth=2, \n                  label=f'Long-term mean θ={theta}')\naxes[0, 0].set_title('Vasicek Model: dr = κ(θ-r)dt + σdW')\naxes[0, 0].set_xlabel('Time (years)')\naxes[0, 0].set_ylabel('Interest Rate')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\naxes[0, 1].plot(t, r_cir[0], color='darkred', linewidth=2, label='Sample path')\naxes[0, 1].axhline(y=theta, color='green', linestyle='--', linewidth=2, \n                  label=f'Long-term mean θ={theta}')\naxes[0, 1].set_title('CIR Model: dr = κ(θ-r)dt + σ√r dW')\naxes[0, 1].set_xlabel('Time (years)')\naxes[0, 1].set_ylabel('Interest Rate')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Distribution evolution\ntimes_to_plot = [1, 3, 5, 10]\ncolors = ['blue', 'green', 'orange', 'red']\n\nfor i, (time_point, color) in enumerate(zip(times_to_plot, colors)):\n    time_idx = int(time_point * N / T)\n    \n    # Vasicek distribution\n    rates_vasicek = r_vasicek[:, time_idx]\n    axes[1, 0].hist(rates_vasicek, bins=40, alpha=0.3, density=True, color=color, \n                   label=f't={time_point}')\n    \n    # Theoretical Vasicek distribution (Gaussian)\n    mean_vasicek = r0 * np.exp(-kappa * time_point) + theta * (1 - np.exp(-kappa * time_point))\n    var_vasicek = sigma_vasicek**2 * (1 - np.exp(-2 * kappa * time_point)) / (2 * kappa)\n    x_range = np.linspace(rates_vasicek.min(), rates_vasicek.max(), 100)\n    theoretical_pdf = stats.norm.pdf(x_range, mean_vasicek, np.sqrt(var_vasicek))\n    axes[1, 0].plot(x_range, theoretical_pdf, color=color, linewidth=2, linestyle='--')\n    \n    # CIR distribution\n    rates_cir = r_cir[:, time_idx]\n    axes[1, 1].hist(rates_cir, bins=40, alpha=0.3, density=True, color=color, \n                   label=f't={time_point}')\n\naxes[1, 0].set_title('Vasicek Rate Distribution Evolution')\naxes[1, 0].set_xlabel('Interest Rate')\naxes[1, 0].set_ylabel('Density')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\naxes[1, 1].set_title('CIR Rate Distribution Evolution')\naxes[1, 1].set_xlabel('Interest Rate')\naxes[1, 1].set_ylabel('Density')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\n# Term structure of interest rates\nmaturities = np.linspace(0.1, 10, 50)\ncurrent_rate = 0.035\n\n# Calculate bond prices and yields\nvasicek_bonds = [vasicek_bond_price(current_rate, tau, kappa, theta, sigma_vasicek) \n                for tau in maturities]\ncir_bonds = [cir_bond_price(current_rate, tau, kappa, theta, sigma_cir) \n            for tau in maturities]\n\n# Convert to yields: Y = -ln(P)/τ\nvasicek_yields = [-np.log(P) / tau for P, tau in zip(vasicek_bonds, maturities)]\ncir_yields = [-np.log(P) / tau for P, tau in zip(cir_bonds, maturities)]\n\naxes[2, 0].plot(maturities, vasicek_yields, 'b-', linewidth=2, label='Vasicek')\naxes[2, 0].plot(maturities, cir_yields, 'r-', linewidth=2, label='CIR')\naxes[2, 0].axhline(y=theta, color='green', linestyle='--', alpha=0.7, \n                  label=f'Long-term rate θ={theta}')\naxes[2, 0].set_xlabel('Maturity (years)')\naxes[2, 0].set_ylabel('Yield')\naxes[2, 0].set_title(f'Term Structure of Interest Rates (r₀={current_rate})')\naxes[2, 0].legend()\naxes[2, 0].grid(True, alpha=0.3)\n\n# Bond price volatility\nbond_vols_vasicek = []\nbond_vols_cir = []\n\nfor tau in maturities:\n    # Calculate bond prices for all rate paths at current time\n    prices_vasicek = [vasicek_bond_price(r, tau, kappa, theta, sigma_vasicek) \n                     for r in r_vasicek[:, 0]]  # Use initial rates\n    prices_cir = [cir_bond_price(r, tau, kappa, theta, sigma_cir) \n                 for r in r_cir[:, 0]]\n    \n    bond_vols_vasicek.append(np.std(prices_vasicek))\n    bond_vols_cir.append(np.std(prices_cir))\n\naxes[2, 1].plot(maturities, bond_vols_vasicek, 'b-', linewidth=2, label='Vasicek')\naxes[2, 1].plot(maturities, bond_vols_cir, 'r-', linewidth=2, label='CIR')\naxes[2, 1].set_xlabel('Maturity (years)')\naxes[2, 1].set_ylabel('Bond Price Volatility')\naxes[2, 1].set_title('Bond Price Volatility vs Maturity')\naxes[2, 1].legend()\naxes[2, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Summary statistics\nprint(\"Model Comparison:\")\nprint(f\"Vasicek - Mean rate: {np.mean(r_vasicek):.4f}, Std: {np.std(r_vasicek):.4f}\")\nprint(f\"CIR - Mean rate: {np.mean(r_cir):.4f}, Std: {np.std(r_cir):.4f}\")\nprint(f\"Negative rates in Vasicek: {np.sum(r_vasicek &lt; 0) / r_vasicek.size:.2%}\")\nprint(f\"Negative rates in CIR: {np.sum(r_cir &lt; 0) / r_cir.size:.2%}\")\n\n\n\n\n\n\n\n\nFigure 11: Interest rate models: Vasicek and CIR processes with term structure\n\n\n\n\n\nModel Comparison:\nVasicek - Mean rate: 0.0377, Std: 0.0098\nCIR - Mean rate: 0.0381, Std: 0.0096\nNegative rates in Vasicek: 0.00%\nNegative rates in CIR: 0.00%\n\n\n\n\n\n6.3 Stochastic Volatility Models\nReal market data exhibits volatility clustering and mean reversion, motivating stochastic volatility models.\n\n6.3.1 Heston Model\nModel Specification: The Heston model couples asset price with stochastic volatility: \\[dS_t = \\mu S_t dt + \\sqrt{V_t} S_t dW_t^{(1)}\\] \\[dV_t = \\kappa(\\theta - V_t) dt + \\sigma_v \\sqrt{V_t} dW_t^{(2)}\\]\nwhere \\(dW_t^{(1)} dW_t^{(2)} = \\rho dt\\) captures correlation between price and volatility shocks.\nProperties: - Volatility clustering: High volatility tends to be followed by high volatility - Leverage effect: Negative correlation \\(\\rho &lt; 0\\) captures the inverse relationship between returns and volatility - Fat tails: The model generates fat-tailed return distributions\n\n\nCode\ndef simulate_heston(S0, V0, mu, kappa, theta, sigma_v, rho, T, N, n_paths=1):\n    \"\"\"Simulate Heston model using Euler scheme.\"\"\"\n    dt = T / N\n    sqrt_dt = np.sqrt(dt)\n    \n    S = np.zeros((n_paths, N + 1))\n    V = np.zeros((n_paths, N + 1))\n    S[:, 0] = S0\n    V[:, 0] = V0\n    \n    for i in range(N):\n        # Generate correlated Brownian increments\n        Z1 = np.random.randn(n_paths)\n        Z2 = rho * Z1 + np.sqrt(1 - rho**2) * np.random.randn(n_paths)\n        \n        dW1 = Z1 * sqrt_dt\n        dW2 = Z2 * sqrt_dt\n        \n        # Update variance (with absorption at zero)\n        V_curr = np.maximum(V[:, i], 0)\n        V[:, i+1] = V_curr + kappa * (theta - V_curr) * dt + sigma_v * np.sqrt(V_curr) * dW2\n        V[:, i+1] = np.maximum(V[:, i+1], 0)\n        \n        # Update stock price\n        S[:, i+1] = S[:, i] * (1 + mu * dt + np.sqrt(V_curr) * dW1)\n    \n    return S, V\n\ndef heston_option_pricing_mc(S0, V0, K, T, r, kappa, theta, sigma_v, rho, n_paths=50000):\n    \"\"\"Monte Carlo option pricing under Heston model.\"\"\"\n    S, V = simulate_heston(S0, V0, r, kappa, theta, sigma_v, rho, T, 252, n_paths)\n    \n    # Calculate payoffs\n    call_payoffs = np.maximum(S[:, -1] - K, 0)\n    put_payoffs = np.maximum(K - S[:, -1], 0)\n    \n    # Discount to present value\n    call_price = np.exp(-r * T) * np.mean(call_payoffs)\n    put_price = np.exp(-r * T) * np.mean(put_payoffs)\n    \n    return call_price, put_price, S, V\n\n# Heston model parameters\nS0 = 100\nV0 = 0.04  # Initial variance (σ₀ = 20%)\nmu = 0.05\nkappa = 2.0\ntheta = 0.04  # Long-term variance\nsigma_v = 0.3  # Volatility of volatility\nrho = -0.7  # Leverage effect\nT = 1.0\nN = 252\nn_paths = 5000\n\nnp.random.seed(42)\n\nfig, axes = plt.subplots(3, 2, figsize=(16, 12))\n\n# Simulate Heston paths\nS_heston, V_heston = simulate_heston(S0, V0, mu, kappa, theta, sigma_v, rho, T, N, n_paths)\nt = np.linspace(0, T, N + 1)\n\n# Compare with Black-Scholes (constant volatility)\nsigma_bs = np.sqrt(theta)  # Use long-term volatility\nS_bs = np.zeros((n_paths, N + 1))\nS_bs[:, 0] = S0\n\nfor i in range(N):\n    dt = T / N\n    dW = np.random.randn(n_paths) * np.sqrt(dt)\n    S_bs[:, i+1] = S_bs[:, i] * (1 + mu * dt + sigma_bs * dW)\n\n# Sample paths comparison\nfor i in range(min(20, n_paths)):\n    axes[0, 0].plot(t, S_heston[i], alpha=0.4, linewidth=0.8, color='blue')\n    axes[0, 1].plot(t, S_bs[i], alpha=0.4, linewidth=0.8, color='red')\n\naxes[0, 0].plot(t, S_heston[0], color='darkblue', linewidth=2, label='Sample path')\naxes[0, 0].set_title('Heston Model: Stock Price Paths')\naxes[0, 0].set_xlabel('Time')\naxes[0, 0].set_ylabel('Stock Price')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\naxes[0, 1].plot(t, S_bs[0], color='darkred', linewidth=2, label='Sample path')\naxes[0, 1].set_title('Black-Scholes: Stock Price Paths')\naxes[0, 1].set_xlabel('Time')\naxes[0, 1].set_ylabel('Stock Price')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Volatility paths\nfor i in range(min(20, n_paths)):\n    axes[1, 0].plot(t, np.sqrt(V_heston[i]), alpha=0.4, linewidth=0.8, color='green')\n\naxes[1, 0].plot(t, np.sqrt(V_heston[0]), color='darkgreen', linewidth=2, label='Sample path')\naxes[1, 0].axhline(y=np.sqrt(theta), color='red', linestyle='--', linewidth=2, \n                  label=f'Long-term vol = {np.sqrt(theta):.2f}')\naxes[1, 0].set_title('Heston Model: Volatility Paths')\naxes[1, 0].set_xlabel('Time')\naxes[1, 0].set_ylabel('Volatility')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Volatility vs return correlation\nreturns_heston = np.diff(np.log(S_heston), axis=1)\nvol_changes = np.diff(np.sqrt(V_heston), axis=1)\n\n# Flatten for correlation calculation\nreturns_flat = returns_heston.flatten()\nvol_flat = vol_changes.flatten()\n\n# Sample scatter plot\nsample_size = min(5000, len(returns_flat))\nindices = np.random.choice(len(returns_flat), sample_size, replace=False)\n\naxes[1, 1].scatter(returns_flat[indices], vol_flat[indices], alpha=0.3, s=1)\naxes[1, 1].set_xlabel('Log Returns')\naxes[1, 1].set_ylabel('Volatility Changes')\naxes[1, 1].set_title(f'Return-Volatility Correlation\\n(ρ = {np.corrcoef(returns_flat, vol_flat)[0,1]:.3f})')\naxes[1, 1].grid(True, alpha=0.3)\n\n# Return distributions comparison\nreturns_heston_terminal = np.log(S_heston[:, -1] / S_heston[:, 0])\nreturns_bs_terminal = np.log(S_bs[:, -1] / S_bs[:, 0])\n\naxes[2, 0].hist(returns_heston_terminal, bins=50, alpha=0.7, density=True, \n               color='blue', label='Heston')\naxes[2, 0].hist(returns_bs_terminal, bins=50, alpha=0.7, density=True, \n               color='red', label='Black-Scholes')\n\n# Theoretical normal distribution\nx_range = np.linspace(min(returns_heston_terminal.min(), returns_bs_terminal.min()),\n                     max(returns_heston_terminal.max(), returns_bs_terminal.max()), 100)\nnormal_pdf = stats.norm.pdf(x_range, (mu - 0.5*sigma_bs**2)*T, sigma_bs*np.sqrt(T))\naxes[2, 0].plot(x_range, normal_pdf, 'k--', linewidth=2, label='Normal')\n\naxes[2, 0].set_xlabel('Log Returns')\naxes[2, 0].set_ylabel('Density')\naxes[2, 0].set_title('Return Distribution Comparison')\naxes[2, 0].legend()\naxes[2, 0].grid(True, alpha=0.3)\n\n# Option pricing comparison\nK_range = np.linspace(80, 120, 21)\nheston_calls = []\nbs_calls = []\n\nfor K in K_range:\n    # Heston option price (Monte Carlo)\n    heston_call, _, _, _ = heston_option_pricing_mc(S0, V0, K, T, 0.05, kappa, theta, sigma_v, rho, 20000)\n    heston_calls.append(heston_call)\n    \n    # Black-Scholes option price\n    bs_call = black_scholes_call(S0, K, T, 0.05, sigma_bs)\n    bs_calls.append(bs_call)\n\naxes[2, 1].plot(K_range, heston_calls, 'bo-', label='Heston', markersize=4)\naxes[2, 1].plot(K_range, bs_calls, 'rs-', label='Black-Scholes', markersize=4)\naxes[2, 1].set_xlabel('Strike Price')\naxes[2, 1].set_ylabel('Call Option Price')\naxes[2, 1].set_title('Option Prices: Heston vs Black-Scholes')\naxes[2, 1].legend()\naxes[2, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Summary statistics\nprint(\"Model Statistics:\")\nprint(f\"Heston - Return mean: {np.mean(returns_heston_terminal):.4f}, std: {np.std(returns_heston_terminal):.4f}\")\nprint(f\"Black-Scholes - Return mean: {np.mean(returns_bs_terminal):.4f}, std: {np.std(returns_bs_terminal):.4f}\")\nprint(f\"Heston - Return skewness: {stats.skew(returns_heston_terminal):.4f}\")\nprint(f\"Black-Scholes - Return skewness: {stats.skew(returns_bs_terminal):.4f}\")\nprint(f\"Heston - Return kurtosis: {stats.kurtosis(returns_heston_terminal):.4f}\")\nprint(f\"Black-Scholes - Return kurtosis: {stats.kurtosis(returns_bs_terminal):.4f}\")\n\n\n\n\n\n\n\n\nFigure 12: Heston stochastic volatility model: Coupled dynamics and option pricing\n\n\n\n\n\nModel Statistics:\nHeston - Return mean: 0.0319, std: 0.2020\nBlack-Scholes - Return mean: 0.0288, std: 0.1988\nHeston - Return skewness: -0.7791\nBlack-Scholes - Return skewness: -0.0459\nHeston - Return kurtosis: 0.9538\nBlack-Scholes - Return kurtosis: 0.0069\n\n\nThe financial applications demonstrate how SDEs provide the mathematical foundation for:\n\nOption pricing: From the classical Black-Scholes formula to sophisticated stochastic volatility models\nInterest rate modeling: Capturing mean reversion and ensuring realistic term structure dynamics\n\nRisk management: Providing frameworks for Value-at-Risk and scenario analysis\nPortfolio optimization: Incorporating stochastic dynamics into investment decisions\n\nThese models form the backbone of modern quantitative finance and demonstrate the practical power of stochastic differential equation theory. In the next section, we explore the emerging applications of SDEs in machine learning and artificial intelligence."
  },
  {
    "objectID": "posts/03-04-2025_intro_to_sde/index.html#sec-ml-applications",
    "href": "posts/03-04-2025_intro_to_sde/index.html#sec-ml-applications",
    "title": "Stochastic Differential Equations: From Mathematical Foundations to Modern Applications",
    "section": "7 SDEs in Modern Machine Learning",
    "text": "7 SDEs in Modern Machine Learning\nThe renaissance of stochastic differential equations in machine learning represents one of the most exciting developments in contemporary AI research. This convergence has led to breakthrough applications in generative modeling, continuous-time neural networks, and probabilistic machine learning.\n\n7.1 Neural Ordinary Differential Equations (NODEs)\nThe Neural ODE framework, introduced by Chen et al. (2018), revolutionized deep learning by treating neural networks as continuous-time dynamical systems.\nMathematical Framework: Instead of discrete layers, Neural ODEs model the hidden state evolution as: \\[\\frac{dh(t)}{dt} = f_\\theta(h(t), t)\\]\nwhere \\(f_\\theta\\) is a neural network parameterized by \\(\\theta\\), and the output is obtained by solving: \\[h(T) = h(0) + \\int_0^T f_\\theta(h(t), t) dt\\]\nKey Advantages: - Memory efficiency: Constant memory cost during training - Adaptive computation: Automatic step size selection\n- Continuous depth: Networks with “infinite” layers - Invertible transformations: Normalizing flows applications\n\n\n7.2 Neural Stochastic Differential Equations\nNeural SDEs extend Neural ODEs by incorporating stochastic dynamics, providing better uncertainty quantification and more expressive models.\nModel Specification: Neural SDEs are defined as: \\[dh(t) = f_\\theta(h(t), t) dt + g_\\theta(h(t), t) dW(t)\\]\nwhere: - \\(f_\\theta\\) is the neural drift function - \\(g_\\theta\\) is the neural diffusion function\n- \\(W(t)\\) represents Brownian motion\n\n\nCode\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SimpleNeuralODE(nn.Module):\n    \"\"\"Simple Neural ODE implementation.\"\"\"\n    def __init__(self, input_dim, hidden_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, input_dim)\n        )\n    \n    def forward(self, t, x):\n        return self.net(x)\n\nclass SimpleNeuralSDE(nn.Module):\n    \"\"\"Simple Neural SDE implementation.\"\"\"\n    def __init__(self, input_dim, hidden_dim):\n        super().__init__()\n        self.drift_net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, input_dim)\n        )\n        self.diffusion_net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, input_dim),\n            nn.Softplus()  # Ensure positive diffusion\n        )\n    \n    def forward(self, t, x):\n        drift = self.drift_net(x)\n        diffusion = self.diffusion_net(x)\n        return drift, diffusion\n\ndef euler_maruyama_neural_sde(sde_func, x0, t_span, dt=0.01):\n    \"\"\"Solve Neural SDE using Euler-Maruyama method.\"\"\"\n    t_start, t_end = t_span\n    n_steps = int((t_end - t_start) / dt)\n    \n    trajectory = [x0]\n    x = x0\n    \n    for i in range(n_steps):\n        t = t_start + i * dt\n        drift, diffusion = sde_func(t, x)\n        \n        # Euler-Maruyama step\n        dW = torch.randn_like(x) * np.sqrt(dt)\n        x = x + drift * dt + diffusion * dW\n        trajectory.append(x.clone())\n    \n    return torch.stack(trajectory)\n\ndef ode_solve_euler(ode_func, x0, t_span, dt=0.01):\n    \"\"\"Simple Euler method for ODE solving.\"\"\"\n    t_start, t_end = t_span\n    n_steps = int((t_end - t_start) / dt)\n    \n    trajectory = [x0]\n    x = x0\n    \n    for i in range(n_steps):\n        t = t_start + i * dt\n        dx_dt = ode_func(t, x)\n        x = x + dx_dt * dt\n        trajectory.append(x.clone())\n    \n    return torch.stack(trajectory)\n\n# Generate synthetic spiral data\ndef generate_spiral_data(n_samples=1000, noise=0.1):\n    \"\"\"Generate 2D spiral dataset.\"\"\"\n    t = torch.linspace(0, 4*np.pi, n_samples)\n    x = t * torch.cos(t) + noise * torch.randn(n_samples)\n    y = t * torch.sin(t) + noise * torch.randn(n_samples)\n    return torch.stack([x, y], dim=1)\n\n# Set up models\ntorch.manual_seed(42)\nnp.random.seed(42)\n\ninput_dim = 2\nhidden_dim = 32\n\nneural_ode = SimpleNeuralODE(input_dim, hidden_dim)\nneural_sde = SimpleNeuralSDE(input_dim, hidden_dim)\n\n# Generate training data\ndata = generate_spiral_data(500, 0.1)\n\n# Initial conditions for forward simulation\nx0_samples = torch.randn(10, 2) * 0.5\nt_span = (0.0, 2.0)\n\nfig, axes = plt.subplots(3, 2, figsize=(18, 14))\n\n# Plot training data\naxes[0, 0].scatter(data[:, 0].numpy(), data[:, 1].numpy(), alpha=0.6, s=20, c='blue')\naxes[0, 0].set_title('Training Data: Noisy Spiral')\naxes[0, 0].set_xlabel('x₁')\naxes[0, 0].set_ylabel('x₂')\naxes[0, 0].grid(True, alpha=0.3)\naxes[0, 0].axis('equal')\n\n# Simulate Neural ODE trajectories\nwith torch.no_grad():\n    ode_trajectories = []\n    for x0 in x0_samples:\n        traj = ode_solve_euler(neural_ode, x0.unsqueeze(0), t_span, dt=0.05)\n        ode_trajectories.append(traj)\n        axes[0, 1].plot(traj[:, 0, 0].numpy(), traj[:, 0, 1].numpy(), \n                       alpha=0.7, linewidth=2)\n\naxes[0, 1].scatter(x0_samples[:, 0].numpy(), x0_samples[:, 1].numpy(), \n                  color='red', s=50, marker='o', zorder=5, label='Initial points')\naxes[0, 1].set_title('Neural ODE Trajectories (Untrained)')\naxes[0, 1].set_xlabel('x₁')\naxes[0, 1].set_ylabel('x₂')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\naxes[0, 1].axis('equal')\n\n# Simulate Neural SDE trajectories\nwith torch.no_grad():\n    torch.manual_seed(42)  # For reproducible stochastic trajectories\n    sde_trajectories = []\n    for x0 in x0_samples:\n        traj = euler_maruyama_neural_sde(neural_sde, x0.unsqueeze(0), t_span, dt=0.01)\n        sde_trajectories.append(traj)\n        axes[1, 0].plot(traj[:, 0, 0].numpy(), traj[:, 0, 1].numpy(), \n                       alpha=0.7, linewidth=1.5)\n\naxes[1, 0].scatter(x0_samples[:, 0].numpy(), x0_samples[:, 1].numpy(), \n                  color='red', s=50, marker='o', zorder=5, label='Initial points')\naxes[1, 0].set_title('Neural SDE Trajectories (Untrained)')\naxes[1, 0].set_xlabel('x₁')\naxes[1, 0].set_ylabel('x₂')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\naxes[1, 0].axis('equal')\n\n# Compare multiple SDE realizations from same initial condition\nwith torch.no_grad():\n    x0_single = torch.tensor([[0.0, 0.0]])\n    n_realizations = 20\n    \n    for i in range(n_realizations):\n        torch.manual_seed(i)  # Different random seeds\n        traj = euler_maruyama_neural_sde(neural_sde, x0_single, t_span, dt=0.01)\n        axes[1, 1].plot(traj[:, 0, 0].numpy(), traj[:, 0, 1].numpy(), \n                       alpha=0.5, linewidth=1, color='blue')\n\naxes[1, 1].scatter([0], [0], color='red', s=100, marker='o', zorder=5, \n                  label='Common initial point')\naxes[1, 1].set_title('SDE Uncertainty: Multiple Realizations')\naxes[1, 1].set_xlabel('x₁')\naxes[1, 1].set_ylabel('x₂')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\naxes[1, 1].axis('equal')\n\n# Analyze drift and diffusion components\nx_grid = torch.linspace(-3, 3, 20)\ny_grid = torch.linspace(-3, 3, 20)\nX, Y = torch.meshgrid(x_grid, y_grid, indexing='ij')\ngrid_points = torch.stack([X.flatten(), Y.flatten()], dim=1)\n\nwith torch.no_grad():\n    drift_vals, diffusion_vals = neural_sde(0.0, grid_points)\n    drift_vals = drift_vals.reshape(20, 20, 2)\n    diffusion_vals = diffusion_vals.reshape(20, 20, 2)\n\n# Plot drift field\naxes[2, 0].quiver(X.numpy(), Y.numpy(), \n                 drift_vals[:, :, 0].numpy(), drift_vals[:, :, 1].numpy(),\n                 alpha=0.7, scale=20)\naxes[2, 0].set_title('Neural SDE Drift Field f_θ(x,t)')\naxes[2, 0].set_xlabel('x₁')\naxes[2, 0].set_ylabel('x₂')\naxes[2, 0].grid(True, alpha=0.3)\naxes[2, 0].axis('equal')\n\n# Plot diffusion magnitude\ndiffusion_magnitude = torch.norm(diffusion_vals, dim=2)\nim = axes[2, 1].contourf(X.numpy(), Y.numpy(), diffusion_magnitude.numpy(), \n                        levels=20, cmap='viridis')\naxes[2, 1].set_title('Neural SDE Diffusion Magnitude |g_θ(x,t)|')\naxes[2, 1].set_xlabel('x₁')\naxes[2, 1].set_ylabel('x₂')\nplt.colorbar(im, ax=axes[2, 1])\n\nplt.tight_layout()\nplt.show()\n\n# Create a simple training loop demonstration\nprint(\"Neural SDE vs Neural ODE Comparison:\")\nprint(\"=\" * 50)\nprint(\"Key Differences:\")\nprint(\"1. Deterministic vs Stochastic: ODEs produce deterministic trajectories,\")\nprint(\"   SDEs incorporate randomness and uncertainty\")\nprint(\"2. Memory vs Uncertainty: ODEs are memory efficient, SDEs provide\")\nprint(\"   natural uncertainty quantification\") \nprint(\"3. Training: SDEs require handling stochastic gradients and\")\nprint(\"   multiple trajectory sampling\")\nprint(\"4. Applications: ODEs for normalizing flows, SDEs for generative\")\nprint(\"   modeling with uncertainty\")\n\n\n\n\n\n\n\n\nFigure 13: Neural SDEs: From deterministic ODEs to stochastic dynamics in deep learning\n\n\n\n\n\nNeural SDE vs Neural ODE Comparison:\n==================================================\nKey Differences:\n1. Deterministic vs Stochastic: ODEs produce deterministic trajectories,\n   SDEs incorporate randomness and uncertainty\n2. Memory vs Uncertainty: ODEs are memory efficient, SDEs provide\n   natural uncertainty quantification\n3. Training: SDEs require handling stochastic gradients and\n   multiple trajectory sampling\n4. Applications: ODEs for normalizing flows, SDEs for generative\n   modeling with uncertainty\n\n\n\n\n7.3 Gaussian Processes and SDEs\nGaussian Processes (GPs) provide a natural connection between SDEs and machine learning, as many GPs can be represented as solutions to linear SDEs.\nConnection: A GP with Matérn covariance function corresponds to the solution of the SDE: \\[d^n X(t) + a_{n-1} d^{n-1} X(t) + \\cdots + a_1 dX(t) + a_0 X(t) dt = \\sigma dW(t)\\]\nThis connection enables: - Efficient GP inference: Converting GP regression to Kalman filtering - Streaming predictions: Online learning with infinite data - Scalable GPs: Linear complexity in time series length\n\n\nCode\ndef matern_32_sde_matrices(length_scale, sigma):\n    \"\"\"\n    State-space representation of Matérn 3/2 GP.\n    dX/dt = F*X + L*w, where w is white noise\n    \"\"\"\n    lam = np.sqrt(3) / length_scale\n    F = np.array([[0, 1], \n                  [-lam**2, -2*lam]])\n    L = np.array([[0], \n                  [sigma * 2 * lam * np.sqrt(lam)]])\n    H = np.array([[1, 0]])  # Observation matrix\n    return F, L, H\n\ndef simulate_matern_sde(F, L, t_span, dt=0.01):\n    \"\"\"Simulate Matérn process using SDE representation.\"\"\"\n    t_start, t_end = t_span\n    t_points = np.arange(t_start, t_end + dt, dt)\n    n_steps = len(t_points)\n    \n    # State dimension\n    state_dim = F.shape[0]\n    noise_dim = L.shape[1]\n    \n    # Initialize\n    X = np.zeros((n_steps, state_dim))\n    X[0] = np.random.randn(state_dim)\n    \n    # Simulate\n    sqrt_dt = np.sqrt(dt)\n    for i in range(1, n_steps):\n        dW = np.random.randn(noise_dim) * sqrt_dt\n        X[i] = X[i-1] + F @ X[i-1] * dt + L @ dW\n    \n    return t_points, X\n\ndef kalman_filter_gp(y_obs, t_obs, F, L, H, R, dt=0.01):\n    \"\"\"\n    Kalman filter for GP inference using SDE representation.\n    \"\"\"\n    n_obs = len(y_obs)\n    state_dim = F.shape[0]\n    \n    # Initialize\n    x_pred = np.zeros((n_obs, state_dim))\n    x_filt = np.zeros((n_obs, state_dim))\n    P_pred = np.zeros((n_obs, state_dim, state_dim))\n    P_filt = np.zeros((n_obs, state_dim, state_dim))\n    \n    # Initial conditions\n    x_pred[0] = np.zeros(state_dim)\n    P_pred[0] = np.eye(state_dim) * 10\n    \n    # Process noise covariance\n    Q = L @ L.T * dt\n    \n    for i in range(n_obs):\n        if i &gt; 0:\n            # Predict step\n            dt_step = t_obs[i] - t_obs[i-1]\n            # Simple Euler integration for transition\n            x_pred[i] = x_filt[i-1] + F @ x_filt[i-1] * dt_step\n            P_pred[i] = P_filt[i-1] + (F @ P_filt[i-1] + P_filt[i-1] @ F.T + Q) * dt_step\n        \n        # Update step\n        innovation = y_obs[i] - H @ x_pred[i]\n        S = H @ P_pred[i] @ H.T + R\n        K = (P_pred[i] @ H.T / S).reshape(-1, 1)\n        \n        x_filt[i] = x_pred[i] + (K * innovation).flatten()\n        P_filt[i] = P_pred[i] - K.reshape(-1, 1) @ H @ P_pred[i]\n    \n    return x_filt, P_filt\n\n# Generate synthetic data\nnp.random.seed(42)\n\n# GP parameters\nlength_scale = 1.0\nsigma = 1.0\nnoise_std = 0.1\n\n# Generate true function using SDE simulation\nF, L, H = matern_32_sde_matrices(length_scale, sigma)\nt_span = (0, 10)\ndt = 0.01\n\nt_fine, X_true = simulate_matern_sde(F, L, t_span, dt)\nf_true = H @ X_true.T  # Extract function values\n\n# Create sparse observations\nn_obs = 20\nobs_indices = np.sort(np.random.choice(len(t_fine), n_obs, replace=False))\nt_obs = t_fine[obs_indices]\ny_obs = f_true[0, obs_indices] + noise_std * np.random.randn(n_obs)\n\nfig, axes = plt.subplots(3, 2, figsize=(16, 12))\n\n# Plot true function and observations\naxes[0, 0].plot(t_fine, f_true[0], 'b-', alpha=0.7, linewidth=2, label='True function')\naxes[0, 0].scatter(t_obs, y_obs, color='red', s=30, zorder=5, label='Observations')\naxes[0, 0].set_title('Matérn 3/2 Process: True Function and Observations')\naxes[0, 0].set_xlabel('Time t')\naxes[0, 0].set_ylabel('f(t)')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Kalman filter inference\nR = noise_std**2  # Observation noise\nx_filt, P_filt = kalman_filter_gp(y_obs, t_obs, F, L, H, R, dt=0.1)\n\n# Extract posterior mean and variance at observation points\npost_mean = H @ x_filt.T\npost_var = np.array([H @ P @ H.T for P in P_filt])\n\naxes[0, 1].plot(t_fine, f_true[0], 'b-', alpha=0.7, linewidth=2, label='True function')\naxes[0, 1].scatter(t_obs, y_obs, color='red', s=30, zorder=5, label='Observations')\naxes[0, 1].plot(t_obs, post_mean[0], 'g-', linewidth=2, label='GP posterior mean')\naxes[0, 1].fill_between(t_obs, \n                       post_mean[0] - 2*np.sqrt(post_var[:, 0, 0]),\n                       post_mean[0] + 2*np.sqrt(post_var[:, 0, 0]),\n                       alpha=0.3, color='green', label='±2σ confidence')\naxes[0, 1].set_title('GP Inference via Kalman Filter')\naxes[0, 1].set_xlabel('Time t')\naxes[0, 1].set_ylabel('f(t)')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Compare with standard GP regression using sklearn\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import Matern\n\n# Standard GP\nkernel = Matern(length_scale=length_scale, nu=1.5) * sigma**2\ngp = GaussianProcessRegressor(kernel=kernel, alpha=noise_std**2)\ngp.fit(t_obs.reshape(-1, 1), y_obs)\n\n# Predict on fine grid\nt_pred = np.linspace(0, 10, 100)\ny_pred, y_std = gp.predict(t_pred.reshape(-1, 1), return_std=True)\n\naxes[1, 0].plot(t_fine, f_true[0], 'b-', alpha=0.7, linewidth=2, label='True function')\naxes[1, 0].scatter(t_obs, y_obs, color='red', s=30, zorder=5, label='Observations')\naxes[1, 0].plot(t_pred, y_pred, 'purple', linewidth=2, label='Standard GP mean')\naxes[1, 0].fill_between(t_pred, y_pred - 2*y_std, y_pred + 2*y_std,\n                       alpha=0.3, color='purple', label='±2σ confidence')\naxes[1, 0].set_title('Standard GP Regression')\naxes[1, 0].set_xlabel('Time t')\naxes[1, 0].set_ylabel('f(t)')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# State evolution visualization\naxes[1, 1].plot(t_obs, x_filt[:, 0], 'g-', linewidth=2, label='State x₁ (function)')\naxes[1, 1].plot(t_obs, x_filt[:, 1], 'orange', linewidth=2, label='State x₂ (derivative)')\naxes[1, 1].fill_between(t_obs, \n                       x_filt[:, 0] - 2*np.sqrt(P_filt[:, 0, 0]),\n                       x_filt[:, 0] + 2*np.sqrt(P_filt[:, 0, 0]),\n                       alpha=0.3, color='green')\naxes[1, 1].fill_between(t_obs, \n                       x_filt[:, 1] - 2*np.sqrt(P_filt[:, 1, 1]),\n                       x_filt[:, 1] + 2*np.sqrt(P_filt[:, 1, 1]),\n                       alpha=0.3, color='orange')\naxes[1, 1].set_title('State Space Evolution (Kalman Filter)')\naxes[1, 1].set_xlabel('Time t')\naxes[1, 1].set_ylabel('State value')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\n# Phase space plot\naxes[2, 0].plot(X_true[:, 0], X_true[:, 1], 'b-', alpha=0.7, linewidth=1, label='True trajectory')\naxes[2, 0].plot(x_filt[:, 0], x_filt[:, 1], 'ro-', markersize=4, label='Filtered states')\naxes[2, 0].set_title('Phase Space: Function vs Derivative')\naxes[2, 0].set_xlabel('f(t)')\naxes[2, 0].set_ylabel(\"f'(t)\")\naxes[2, 0].legend()\naxes[2, 0].grid(True, alpha=0.3)\n\n# Computational comparison\nimport time\n\n# Time standard GP\nstart_time = time.time()\nfor _ in range(10):\n    gp.fit(t_obs.reshape(-1, 1), y_obs)\n    y_pred, _ = gp.predict(t_pred.reshape(-1, 1), return_std=True)\ngp_time = (time.time() - start_time) / 10\n\n# Time Kalman filter approach  \nstart_time = time.time()\nfor _ in range(10):\n    x_filt, P_filt = kalman_filter_gp(y_obs, t_obs, F, L, H, R)\nkf_time = (time.time() - start_time) / 10\n\nmethods = ['Standard GP', 'Kalman Filter GP']\ntimes = [gp_time, kf_time]\ncolors = ['purple', 'green']\n\nbars = axes[2, 1].bar(methods, times, color=colors, alpha=0.7)\naxes[2, 1].set_title('Computational Efficiency Comparison')\naxes[2, 1].set_ylabel('Time (seconds)')\naxes[2, 1].grid(True, alpha=0.3)\n\n# Add time labels on bars\nfor bar, time_val in zip(bars, times):\n    height = bar.get_height()\n    axes[2, 1].text(bar.get_x() + bar.get_width()/2., height + 0.001,\n                   f'{time_val:.4f}s', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"GP-SDE Connection Benefits:\")\nprint(\"=\" * 40)\nprint(f\"Standard GP time: {gp_time:.4f} seconds\")\nprint(f\"Kalman Filter time: {kf_time:.4f} seconds\") \nprint(f\"Speedup factor: {gp_time/kf_time:.2f}x\")\nprint(\"\\nKey advantages of SDE representation:\")\nprint(\"1. Linear complexity O(n) vs O(n³) for standard GP\")\nprint(\"2. Online/streaming inference capability\")\nprint(\"3. Natural handling of non-stationary processes\")\nprint(\"4. Connection to control theory and signal processing\")\n\n\n\n\n\n\n\n\nFigure 14: Gaussian Processes as SDE solutions: Efficient inference via state-space methods\n\n\n\n\n\nGP-SDE Connection Benefits:\n========================================\nStandard GP time: 0.0033 seconds\nKalman Filter time: 0.0004 seconds\nSpeedup factor: 7.76x\n\nKey advantages of SDE representation:\n1. Linear complexity O(n) vs O(n³) for standard GP\n2. Online/streaming inference capability\n3. Natural handling of non-stationary processes\n4. Connection to control theory and signal processing\n\n\n\n\n7.4 Neural Processes: Bridging GPs and Neural Networks\nNeural Processes (NPs) combine the flexibility of neural networks with the uncertainty quantification of Gaussian Processes, representing a paradigm shift in meta-learning and few-shot prediction.\nArchitecture: Neural Processes consist of: 1. Encoder: Maps context points to representations 2. Aggregator: Combines representations (often permutation-invariant)\n3. Decoder: Generates predictions at target points\nMathematical Formulation: Given context set \\(\\mathcal{C} = \\{(x_i, y_i)\\}_{i=1}^n\\) and target inputs \\(\\mathbf{x}_*\\), NPs model: \\[p(y_* | \\mathbf{x}_*, \\mathcal{C}) = \\int p(y_* | \\mathbf{x}_*, z) p(z | \\mathcal{C}) dz\\]\nwhere \\(z\\) is a latent representation capturing the underlying function.\nThe connection to SDEs emerges through: - Stochastic processes: NPs model distributions over functions - Uncertainty propagation: Similar to SDE solution uncertainty - Continuous-time extensions: Neural Process SDEs for temporal modeling\n\n\n7.5 Diffusion Models: SDEs for Generative AI\nDiffusion models represent one of the most successful applications of SDE theory in modern machine learning, powering state-of-the-art generative models for images, audio, and text.\nMathematical Framework: Diffusion models are built on the theory of denoising diffusion processes, which can be formulated as SDEs. The framework consists of two processes:\n\nForward Process (Noise Addition): A fixed diffusion process that gradually adds Gaussian noise: \\[dX_t = -\\frac{1}{2}\\beta(t) X_t dt + \\sqrt{\\beta(t)} dW_t\\]\nReverse Process (Denoising): A learned SDE that reverses the noise addition: \\[dX_t = \\left[-\\frac{1}{2}\\beta(t) X_t - \\beta(t) \\nabla_{X_t} \\log p_t(X_t)\\right] dt + \\sqrt{\\beta(t)} dW_t\\]\n\nwhere \\(\\beta(t)\\) is the noise schedule and \\(\\nabla_{X_t} \\log p_t(X_t)\\) is the score function.\nScore-Based Generative Models: The key insight is that learning the score function \\(\\nabla_{X_t} \\log p_t(X_t)\\) enables sample generation by solving the reverse SDE.\n\n\nCode\ndef linear_beta_schedule(timesteps, beta_start=0.0001, beta_end=0.02):\n    \"\"\"Linear noise schedule for diffusion process.\"\"\"\n    return np.linspace(beta_start, beta_end, timesteps)\n\ndef cosine_beta_schedule(timesteps, s=0.008):\n    \"\"\"Cosine noise schedule for improved sampling.\"\"\"\n    steps = timesteps + 1\n    x = np.linspace(0, timesteps, steps)\n    alphas_cumprod = np.cos(((x / timesteps) + s) / (1 + s) * np.pi * 0.5) ** 2\n    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n    return np.clip(betas, 0.0001, 0.9999)\n\nclass SimpleDiffusionModel:\n    \"\"\"Simplified diffusion model for demonstration.\"\"\"\n    \n    def __init__(self, timesteps=1000, beta_schedule='linear'):\n        self.timesteps = timesteps\n        \n        if beta_schedule == 'linear':\n            self.betas = linear_beta_schedule(timesteps)\n        else:\n            self.betas = cosine_beta_schedule(timesteps)\n        \n        self.alphas = 1.0 - self.betas\n        self.alphas_cumprod = np.cumprod(self.alphas)\n        self.alphas_cumprod_prev = np.concatenate([np.array([1.0]), self.alphas_cumprod[:-1]])\n        \n        # Precompute useful quantities\n        self.sqrt_alphas_cumprod = np.sqrt(self.alphas_cumprod)\n        self.sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - self.alphas_cumprod)\n        \n    def q_sample(self, x_start, t, noise=None):\n        \"\"\"Forward diffusion process: q(x_t | x_0).\"\"\"\n        if noise is None:\n            noise = np.random.randn(*x_start.shape)\n        \n        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t]\n        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t]\n        \n        return (sqrt_alphas_cumprod_t * x_start + \n                sqrt_one_minus_alphas_cumprod_t * noise)\n    \n    def p_sample_step(self, model_output, x_t, t):\n        \"\"\"Single reverse diffusion step (simplified).\"\"\"\n        # Extract noise prediction\n        predicted_noise = model_output\n        \n        # Compute coefficients\n        alpha_t = self.alphas[t]\n        alpha_cumprod_t = self.alphas_cumprod[t]\n        alpha_cumprod_t_prev = self.alphas_cumprod_prev[t]\n        \n        # Compute mean of reverse process\n        pred_original_sample = (x_t - np.sqrt(1 - alpha_cumprod_t) * predicted_noise) / np.sqrt(alpha_cumprod_t)\n        pred_original_sample = np.clip(pred_original_sample, -1, 1)\n        \n        # Compute coefficients for x_t\n        pred_sample_direction = np.sqrt(1 - alpha_cumprod_t_prev) * predicted_noise\n        pred_prev_sample = np.sqrt(alpha_cumprod_t_prev) * pred_original_sample + pred_sample_direction\n        \n        return pred_prev_sample\n\ndef simple_score_network(x, t, target_shape=(28, 28)):\n    \"\"\"\n    Simplified score network (noise predictor).\n    In practice, this would be a sophisticated neural network (U-Net, etc.)\n    \"\"\"\n    # For demonstration, just add some structured noise based on time\n    noise_level = t / 1000.0\n    spatial_pattern = np.sin(np.arange(target_shape[0])[:, None] * 0.3) * np.cos(np.arange(target_shape[1])[None, :] * 0.3)\n    predicted_noise = noise_level * spatial_pattern + 0.1 * np.random.randn(*target_shape)\n    return predicted_noise\n\n# Generate synthetic 2D data (Swiss roll)\ndef generate_swiss_roll(n_samples=1000, noise=0.1):\n    \"\"\"Generate Swiss roll dataset.\"\"\"\n    t = 1.5 * np.pi * (1 + 2 * np.random.rand(n_samples))\n    x = t * np.cos(t)\n    y = t * np.sin(t)\n    data = np.column_stack([x, y])\n    data += noise * np.random.randn(n_samples, 2)\n    return data\n\n# Set up diffusion model\nnp.random.seed(42)\ndiffusion = SimpleDiffusionModel(timesteps=200, beta_schedule='cosine')\n\n# Generate training data\nn_samples = 500\ndata_2d = generate_swiss_roll(n_samples, noise=0.1)\ndata_2d = (data_2d - data_2d.mean(axis=0)) / data_2d.std(axis=0)  # Normalize\n\nfig, axes = plt.subplots(4, 4, figsize=(18, 16))\n\n# Original data\naxes[0, 0].scatter(data_2d[:, 0], data_2d[:, 1], alpha=0.6, s=20, c='blue')\naxes[0, 0].set_title('Original Data')\naxes[0, 0].set_xlabel('x₁')\naxes[0, 0].set_ylabel('x₂')\naxes[0, 0].grid(True, alpha=0.3)\n\n# Forward diffusion process at different timesteps\ntimesteps_to_show = [0, 50, 100, 150, 199]\ncolors = ['blue', 'green', 'orange', 'red', 'purple']\n\n# Show forward process\nfor i, (t, color) in enumerate(zip(timesteps_to_show[1:], colors[1:])):\n    noisy_data = []\n    for sample in data_2d:\n        noisy_sample = diffusion.q_sample(sample, t)\n        noisy_data.append(noisy_sample)\n    noisy_data = np.array(noisy_data)\n    \n    row = (i + 1) // 4\n    col = (i + 1) % 4\n    axes[row, col].scatter(noisy_data[:, 0], noisy_data[:, 1], alpha=0.6, s=20, c=color)\n    axes[row, col].set_title(f'Forward Process t={t}')\n    axes[row, col].set_xlabel('x₁')\n    axes[row, col].set_ylabel('x₂')\n    axes[row, col].grid(True, alpha=0.3)\n\n# Beta schedule visualization\naxes[1, 0].plot(diffusion.betas, 'b-', linewidth=2, label='β(t)')\naxes[1, 0].set_title('Noise Schedule β(t)')\naxes[1, 0].set_xlabel('Timestep t')\naxes[1, 0].set_ylabel('β(t)')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Alpha cumulative product\naxes[1, 1].plot(diffusion.alphas_cumprod, 'r-', linewidth=2, label='ᾱ(t)')\naxes[1, 1].set_title('Cumulative Product ᾱ(t)')\naxes[1, 1].set_xlabel('Timestep t')\naxes[1, 1].set_ylabel('ᾱ(t)')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\n# Signal-to-noise ratio\nsnr = diffusion.alphas_cumprod / (1 - diffusion.alphas_cumprod)\naxes[1, 2].semilogy(snr, 'g-', linewidth=2, label='SNR')\naxes[1, 2].set_title('Signal-to-Noise Ratio')\naxes[1, 2].set_xlabel('Timestep t')\naxes[1, 2].set_ylabel('SNR (log scale)')\naxes[1, 2].legend()\naxes[1, 2].grid(True, alpha=0.3)\n\n# Demonstrate reverse process (simplified)\n# Start from noise\nfinal_noise = np.random.randn(100, 2)\naxes[1, 3].scatter(final_noise[:, 0], final_noise[:, 1], alpha=0.6, s=20, c='red')\naxes[1, 3].set_title('Starting Noise (t=T)')\naxes[1, 3].set_xlabel('x₁')\naxes[1, 3].set_ylabel('x₂')\naxes[1, 3].grid(True, alpha=0.3)\n\n# Simplified reverse sampling (without actual trained model)\ncurrent_samples = final_noise.copy()\nreverse_timesteps = [199, 150, 100, 50, 0]\n\nfor i, t in enumerate(reverse_timesteps[1:]):\n    # Simulate denoising step (in practice, this would use trained score network)\n    noise_factor = diffusion.sqrt_one_minus_alphas_cumprod[t]\n    signal_factor = diffusion.sqrt_alphas_cumprod[t]\n    \n    # Simple denoising: move towards data manifold\n    target_samples = data_2d[np.random.choice(len(data_2d), len(current_samples))]\n    denoising_direction = target_samples - current_samples\n    current_samples = current_samples + 0.1 * denoising_direction + 0.1 * np.random.randn(*current_samples.shape)\n    \n    row = 2 + i // 4\n    col = i % 4\n    axes[row, col].scatter(current_samples[:, 0], current_samples[:, 1], \n                          alpha=0.6, s=20, c=colors[i+1])\n    axes[row, col].set_title(f'Reverse Process t={t}')\n    axes[row, col].set_xlabel('x₁')\n    axes[row, col].set_ylabel('x₂')\n    axes[row, col].grid(True, alpha=0.3)\n\n# Compare noise schedules\nfig2, axes2 = plt.subplots(2, 2, figsize=(12, 8))\n\n# Linear vs Cosine schedules\nlinear_betas = linear_beta_schedule(200)\ncosine_betas = cosine_beta_schedule(200)\n\naxes2[0, 0].plot(linear_betas, 'b-', linewidth=2, label='Linear')\naxes2[0, 0].plot(cosine_betas, 'r-', linewidth=2, label='Cosine')\naxes2[0, 0].set_title('Noise Schedules Comparison')\naxes2[0, 0].set_xlabel('Timestep')\naxes2[0, 0].set_ylabel('β(t)')\naxes2[0, 0].legend()\naxes2[0, 0].grid(True, alpha=0.3)\n\n# Corresponding alpha cumprod\nlinear_alphas_cumprod = np.cumprod(1.0 - linear_betas)\ncosine_alphas_cumprod = np.cumprod(1.0 - cosine_betas)\n\naxes2[0, 1].plot(linear_alphas_cumprod, 'b-', linewidth=2, label='Linear')\naxes2[0, 1].plot(cosine_alphas_cumprod, 'r-', linewidth=2, label='Cosine')\naxes2[0, 1].set_title('Signal Preservation ᾱ(t)')\naxes2[0, 1].set_xlabel('Timestep')\naxes2[0, 1].set_ylabel('ᾱ(t)')\naxes2[0, 1].legend()\naxes2[0, 1].grid(True, alpha=0.3)\n\n# Demonstrate ancestral sampling concept\nx_start = data_2d[0]  # Single sample\nt_values = np.arange(0, 200, 20)\nforward_trajectory = []\n\nfor t in t_values:\n    x_t = diffusion.q_sample(x_start, t)\n    forward_trajectory.append(x_t)\n\nforward_trajectory = np.array(forward_trajectory)\n\naxes2[1, 0].plot(forward_trajectory[:, 0], forward_trajectory[:, 1], 'bo-', \n                markersize=4, linewidth=1, alpha=0.7)\naxes2[1, 0].scatter([x_start[0]], [x_start[1]], color='red', s=100, \n                   marker='*', zorder=5, label='Original')\naxes2[1, 0].set_title('Forward Diffusion Trajectory')\naxes2[1, 0].set_xlabel('x₁')\naxes2[1, 0].set_ylabel('x₂')\naxes2[1, 0].legend()\naxes2[1, 0].grid(True, alpha=0.3)\n\n# Score function illustration (simplified)\nx_grid = np.linspace(-3, 3, 20)\ny_grid = np.linspace(-3, 3, 20)\nX, Y = np.meshgrid(x_grid, y_grid)\ngrid_points = np.stack([X.flatten(), Y.flatten()], axis=1)\n\n# Approximate score function (points toward data)\nscores = np.zeros_like(grid_points)\nfor i, point in enumerate(grid_points):\n    # Find nearest data points\n    distances = np.linalg.norm(data_2d - point[None, :], axis=1)\n    nearest_idx = np.argmin(distances)\n    nearest_point = data_2d[nearest_idx]\n    \n    # Score points toward data manifold\n    direction = nearest_point - point\n    scores[i] = direction / (np.linalg.norm(direction) + 1e-8)\n\nscores = scores.reshape(20, 20, 2)\n\naxes2[1, 1].quiver(X, Y, scores[:, :, 0], scores[:, :, 1], \n                  alpha=0.7, scale=20, color='blue')\naxes2[1, 1].scatter(data_2d[:, 0], data_2d[:, 1], alpha=0.6, s=20, c='red')\naxes2[1, 1].set_title('Score Function ∇log p(x)')\naxes2[1, 1].set_xlabel('x₁')\naxes2[1, 1].set_ylabel('x₂')\naxes2[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nfig.tight_layout()\nplt.show()\n\n# Print key insights\nprint(\"Diffusion Models: Key Insights\")\nprint(\"=\" * 40)\nprint(\"1. Forward Process: Systematic noise addition following SDE\")\nprint(\"2. Reverse Process: Learned denoising via score function estimation\")\nprint(\"3. Training: Learn to predict noise added at each timestep\")\nprint(\"4. Sampling: Reverse the diffusion process to generate new samples\")\nprint(\"5. Score Function: ∇log p(x) guides the reverse process\")\nprint(\"\\nAdvantages over GANs:\")\nprint(\"- More stable training\")\nprint(\"- Better mode coverage\") \nprint(\"- Theoretical guarantees\")\nprint(\"- Flexible sampling procedures\")\n\n\nDiffusion Models: Key Insights\n========================================\n1. Forward Process: Systematic noise addition following SDE\n2. Reverse Process: Learned denoising via score function estimation\n3. Training: Learn to predict noise added at each timestep\n4. Sampling: Reverse the diffusion process to generate new samples\n5. Score Function: ∇log p(x) guides the reverse process\n\nAdvantages over GANs:\n- More stable training\n- Better mode coverage\n- Theoretical guarantees\n- Flexible sampling procedures\n\n\n\n\n\n\n\n\n\n\n\n(a) Diffusion models: SDE-based generative modeling with forward and reverse processes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 15\n\n\n\n\nKey Theoretical Results:\n\nProbability Flow ODE: Every SDE has a corresponding ODE with the same marginal distributions: \\[\\frac{dX_t}{dt} = f(X_t, t) - \\frac{1}{2}g^2(t) \\nabla_{X_t} \\log p_t(X_t)\\]\nScore Matching: The score function can be learned by minimizing: \\[\\mathbb{E}_{t,X_0,X_t}\\left[\\left\\|\\epsilon - \\epsilon_\\theta(X_t, t)\\right\\|^2\\right]\\]\nSampling: Generation is achieved by numerically solving the reverse SDE or probability flow ODE.\n\nModern Applications: - Image Generation: DALL-E 2, Stable Diffusion, Imagen - Audio Synthesis: WaveGrad, DiffWave - Text Generation: Diffusion-LM - 3D Shape Generation: Point-E, DreamFusion - Video Generation: Imagen Video, Make-A-Video\n\n\n7.6 Stochastic Neural Differential Equations\nStochastic Neural Differential Equations (SNDEs) extend Neural Ordinary Differential Equations (NODEs) by incorporating stochasticity directly into the neural network dynamics. This allows them to model systems with inherent randomness, making them particularly suitable for tasks like time series forecasting with uncertainty, generative modeling, and reinforcement learning in stochastic environments.\nThe general form of a Neural SDE can be written as:\n\\(dh_t = f_\\theta(h_t, t) dt + g_\\theta(h_t, t) dW_t\\)\nwhere: - \\(h_t\\) is the hidden state of the neural network at time \\(t\\). - \\(f_\\theta(h_t, t)\\) is the drift function, typically parameterized by a neural network with parameters \\(\\theta\\). - \\(g_\\theta(h_t, t)\\) is the diffusion function, also parameterized by a neural network with parameters \\(\\theta\\). - \\(dW_t\\) is a Wiener process (Brownian motion), representing the stochastic input.\nThis formulation allows the model to learn both the deterministic evolution and the noise characteristics of the underlying system.\n\n7.6.1 Implementation Example: Simple Neural SDE in PyTorch\nHere’s a basic PyTorch implementation of a Neural SDE, demonstrating how to define the drift and diffusion networks and simulate the process. For simplicity, we’ll use a fixed time step Euler-Maruyama solver.\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the Neural SDE model\nclass NeuralSDE(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(NeuralSDE, self).__init__()\n        self.drift_net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, output_dim)\n        )\n        self.diffusion_net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, output_dim)\n        )\n\n    def forward(self, h, t):\n        # h: current hidden state, t: current time\n        # For simplicity, we'll ignore 't' in this basic example,\n        # but it can be incorporated into the network inputs.\n        drift = self.drift_net(h)\n        diffusion = self.diffusion_net(h)\n        return drift, diffusion\n\n# Euler-Maruyama SDE solver\ndef sde_solver_euler_maruyama(model, h0, t_span, dt, num_paths=1):\n    h_paths = []\n    for _ in range(num_paths):\n        h_path = [h0]\n        h_current = h0\n        for i in range(len(t_span) - 1):\n            t_current = t_span[i]\n            dW = torch.randn_like(h_current) * np.sqrt(dt)\n            \n            drift, diffusion = model(h_current, t_current)\n            h_next = h_current + drift * dt + diffusion * dW\n            h_path.append(h_next)\n            h_current = h_next\n        h_paths.append(torch.stack(h_path))\n    return torch.stack(h_paths)\n\n# Parameters\ninput_dim = 1\nhidden_dim = 32\noutput_dim = 1\nh0 = torch.tensor([0.5], dtype=torch.float32) # Initial state\nT_end = 2.0\nnum_steps = 100\ndt = T_end / num_steps\nt_span = np.linspace(0, T_end, num_steps + 1)\nnum_paths = 5\n\n# Initialize Neural SDE model\nsde_model = NeuralSDE(input_dim, hidden_dim, output_dim)\n\n# Simulate paths\ntorch.manual_seed(42) # for reproducibility\nsimulated_paths = sde_solver_euler_maruyama(sde_model, h0, t_span, dt, num_paths)\n\n# Plotting\nplt.figure(figsize=(10, 6))\nfor i in range(num_paths):\n    plt.plot(t_span, simulated_paths[i].detach().numpy(), alpha=0.7)\nplt.title('Simulated Paths from a Simple Neural SDE')\nplt.xlabel('Time')\nplt.ylabel('Hidden State h(t)')\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(f\"Shape of simulated paths: {simulated_paths.shape}\")\nThis example demonstrates the basic structure. In real-world applications, the drift and diffusion networks would be more complex, and training would involve defining a loss function (e.g., likelihood-based or score-matching) and using optimization algorithms to learn the parameters \\(\\theta\\).\nLatent SDEs: Model latent dynamics with SDEs while observing through deterministic functions: \\(dZ_t = f_\\theta(Z_t, t) dt + g_\\theta(Z_t, t) dW_t\\) \\(X_t = h_\\phi(Z_t) + \\epsilon_t\\)\nNeural SDE Training: Uses the reparameterization trick and efficient SDE solvers for gradient computation.\nApplications: Time series modeling, dynamics learning, uncertainty quantification in deep learning.\nThese modern developments demonstrate how classical SDE theory continues to drive innovation in machine learning, providing both theoretical foundations and practical algorithms for the next generation of AI systems.\nRecent work has explored neural networks that directly parameterize SDE coefficients, enabling:\nLatent SDEs: Model latent dynamics with SDEs while observing through deterministic functions: \\[dZ_t = f_\\theta(Z_t, t) dt + g_\\theta(Z_t, t) dW_t\\] \\[X_t = h_\\phi(Z_t) + \\epsilon_t\\]\nNeural SDE Training: Uses the reparameterization trick and efficient SDE solvers for gradient computation.\nApplications: Time series modeling, dynamics learning, uncertainty quantification in deep learning.\nThese modern developments demonstrate how classical SDE theory continues to drive innovation in machine learning, providing both theoretical foundations and practical algorithms for the next generation of AI systems."
  },
  {
    "objectID": "posts/03-04-2025_intro_to_sde/index.html#sec-conclusion",
    "href": "posts/03-04-2025_intro_to_sde/index.html#sec-conclusion",
    "title": "Stochastic Differential Equations: From Mathematical Foundations to Modern Applications",
    "section": "8 Conclusion and Future Directions",
    "text": "8 Conclusion and Future Directions\nThis comprehensive exploration of stochastic differential equations reveals the profound mathematical elegance and practical power of this theoretical framework. From Itô’s revolutionary development of stochastic calculus in the 1940s to today’s cutting-edge applications in generative AI, SDEs have consistently provided the mathematical foundation for modeling and understanding complex random phenomena.\n\n8.1 Key Contributions and Insights\nMathematical Foundations: We have seen how the careful construction of stochastic integration and Itô’s lemma provides the rigorous mathematical framework necessary for analyzing continuous-time random processes. The fundamental insight that \\((dW_t)^2 = dt\\) transforms our understanding of calculus in stochastic settings.\nComputational Methods: The development of numerical schemes like Euler-Maruyama and Milstein demonstrates how theoretical insights translate into practical computational tools. The trade-offs between accuracy, computational cost, and stability remain central to successful implementation.\nFinancial Applications: The Black-Scholes model, interest rate models, and stochastic volatility frameworks showcase how SDE theory has revolutionized quantitative finance. These applications demonstrate the power of mathematical modeling in creating practical solutions to complex real-world problems.\nMachine Learning Renaissance: The emergence of Neural ODEs, diffusion models, and neural processes illustrates how classical mathematical theory continues to inspire breakthrough innovations in artificial intelligence. The connection between score-based generative models and SDE theory represents a particularly elegant synthesis of probability theory and deep learning.\n\n\n8.2 Future Research Directions\nSeveral exciting avenues for future research emerge from our exploration:\nComputational Advances: Development of more efficient numerical methods for high-dimensional SDEs, particularly for machine learning applications where computational scalability is crucial.\nTheoretical Extensions: Investigation of fractional SDEs, jump-diffusion processes, and SDEs on manifolds to capture more complex real-world phenomena.\nMachine Learning Integration: Deeper integration of SDE theory with modern machine learning, including applications to reinforcement learning, causal inference, and interpretable AI.\nCross-Disciplinary Applications: Extension of SDE methods to new domains such as biology, climate science, and social networks, where stochastic modeling can provide new insights.\n\n\n8.3 Final Reflections\nThe journey from Brownian motion to modern AI demonstrates the enduring value of rigorous mathematical theory. Stochastic differential equations exemplify how abstract mathematical concepts, developed through careful theoretical investigation, ultimately find profound practical applications that transform entire fields.\nAs we stand at the intersection of classical probability theory and modern artificial intelligence, SDEs continue to provide both the theoretical foundation and practical tools necessary for the next generation of scientific and technological breakthroughs. The mathematical elegance of Itô calculus, combined with the computational power of modern algorithms, ensures that stochastic differential equations will remain at the forefront of mathematical innovation for decades to come."
  },
  {
    "objectID": "posts/03-04-2025_intro_to_sde/index.html#sec-references",
    "href": "posts/03-04-2025_intro_to_sde/index.html#sec-references",
    "title": "Stochastic Differential Equations: From Mathematical Foundations to Modern Applications",
    "section": "9 References",
    "text": "9 References\nThe comprehensive nature of this exploration draws upon decades of mathematical and computational research. Key references include foundational texts on stochastic calculus, numerical analysis, financial mathematics, and modern machine learning applications. The bibliography provides entry points for deeper investigation into each topic area covered in this treatise."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jan Schlegel",
    "section": "",
    "text": "Hi there! I’m Jan, a Master’s student in Statistics at ETH Zurich with a passion for machine learning, healthcare applications, and probabilistic modeling. My journey combines rigorous academic training in statistics with hands-on experience in research and data analysis. I thrive on solving complex problems and am particularly interested in the intersection of machine learning and healthcare. With a strong foundation in both theoretical statistics and practical implementation, I’m always eager to tackle new challenges and contribute to meaningful projects. Currently seeking opportunities in machine learning and data science - let’s connect!"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Jan Schlegel",
    "section": "Education",
    "text": "Education\n\n\n🎓 Master of Science (M.Sc. ETH) in Statistics\n\n🏛️ ETH Zurich | 📍 Zurich, Switzerland | 📅 Sep 2023 - Sep 2025\n\n\nGPA: 5.98 / 6.00\n\n\nRelevant Coursework: Deep Learning; Image Analysis and Computer Vision; Machine Learning for Healthcare; Probabilistic AI; Statistical Learning Theory; Introduction to Machine Learning; Causality; Computational Statistics; Fundamentals of Mathematical Statistics; High Dimensional Statistics; Bayesian Statistics; Time Series Analysis\nSemester Paper: Extrapolation and Distributional Robustness for Climate Downscaling\nMentor for New Students\n\n\n\n🎓 Special Student Statistics\n\n🏛️ ETH Zurich | 📍 Zurich, Switzerland | 📅 Feb 2022 - Feb 2023\n\n\nGPA: 6.00 / 6.00\n\n\nVoluntarily enrolled to take advanced courses in statistics\nCoursework: Applied Time Series, Applied Multivariate Statistics\n\n\n\n🎓 Bachelor of Arts in Business and Economics\n\n🏛️ University of Zurich (UZH) | 📍 Zurich, Switzerland | 📅 Sep 2019 - Feb 2023\n\n\nGPA: 5.87 / 6.00\n\n\nFinished the degree with 215 ECTS (instead of 180 ECTS)\nMajor in Banking and Finance (150 ECTS)\nMinor in Applied Probability and Statistics (30 ECTS)\nRelevant Coursework: Likelihood Inference; Statistical Modelling; Numerical Methods in Informatics; Introduction to Machine Learning; Introduction to Statistics; Introductory Econometrics Thesis: Portfolio Value at Risk Forecasting with GARCH-Type Models (Grade: 6.00/6.00)"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Jan Schlegel",
    "section": "Experience",
    "text": "Experience\n\n\nResearch Assistant (Civilian Service)\n\n🏛️ University of Zurich (EBPI) | 📍 Zurich, Switzerland | 📅 Feb 2022 - Aug 2023\n\n\nConducted sophisticated data analysis and statistical modeling using R for epidemiological research\nCollaborated with interdisciplinary team of epidemiologists, statisticians, and public health experts\nContributed to research paper published in Nature Communications (2023): “Persistent humoral immune response in youth throughout the COVID-19 pandemic: prospective school-based cohort study”\n\n\n\nAccountant (Military Service)\n\n🏛️ Swiss Armed Forces | 📍 Kloten, Switzerland | 📅 Jul 2021 - Nov 2021\n\n\nSuccessfully balanced military service obligations while maintaining full-time academic studies\nDemonstrated exceptional time management and organizational skills\n\n\n\nTeaching Assistant\n\n🏛️ University of Zurich | 📍 Zurich, Switzerland | 📅 Sep 2020 - Jul 2021\n\n\nConducted bi-weekly interactive exercise classes in Analysis and Linear Algebra for first-year students\nEffectively communicated complex mathematical concepts to diverse student groups\nDeveloped strong presentation and educational communication skills"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nCopulas: Theory, Applications, and Implementation in Python\n\n\n\n\n\n\nStatistics\n\n\nFinance\n\n\nPython\n\n\nRisk Management\n\n\n\nThis comprehensive exploration of copula theory covers fundamental concepts from Sklar’s theorem to advanced applications in finance and risk management. We examine Archimedean and elliptical copulas, parameter estimation methods, goodness-of-fit testing, and vine copulas, with extensive Python implementations and publication-ready visualizations throughout.\n\n\n\n\n\nJul 29, 2025\n\n\nJan Schlegel\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Portfolio Optimization: From Markowitz to Expected Shortfall and Genetic Algorithms\n\n\n\n\n\n\nFinance\n\n\nPython\n\n\nOptimization\n\n\nRisk Management\n\n\n\nThis comprehensive exploration of portfolio optimization covers the mathematical foundations from classical mean-variance optimization to modern approaches including Expected Shortfall (CVaR) optimization and genetic algorithms. We provide rigorous mathematical derivations, implement multiple optimization techniques in Python, and compare their performance on real market data with publication-ready visualizations throughout.\n\n\n\n\n\nJul 26, 2025\n\n\nJan Schlegel\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic Differential Equations: From Mathematical Foundations to Modern Applications\n\n\n\n\n\n\nMathematics\n\n\nFinance\n\n\nMachine Learning\n\n\nPython\n\n\nStochastic Processes\n\n\n\nThis comprehensive treatment of stochastic differential equation theory covers fundamental concepts from Brownian motion and Itô calculus to advanced applications in mathematical finance, neural processes, and diffusion models. We provide rigorous mathematical foundations, numerical solution methods, and extensive Python implementations with publication-ready visualizations, bridging classical stochastic analysis with cutting-edge machine learning applications.\n\n\n\n\n\nApr 3, 2025\n\n\nJan Schlegel\n\n\n\n\n\n\n\n\n\n\n\n\nComparing Univariate and Multivariate Models for Value at Risk Forecasting\n\n\n\n\n\n\nSimulation\n\n\nR\n\n\n\nThis post explores the effectiveness of univariate and multivariate GARCH-based models in forecasting Value at Risk (VaR) for a long equity portfolio. While multivariate models generally perform better in backtests, univariate models often fall short. However, neither model type consistently outperforms the other in predictive accuracy, highlighting the trade-offs between simplicity and complexity in risk forecasting.\n\n\n\n\n\nFeb 7, 2025\n\n\nJan Schlegel\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html",
    "href": "posts/07-02-2025_portfolio-var/index.html",
    "title": "Comparing Univariate and Multivariate Models for Value at Risk Forecasting",
    "section": "",
    "text": "Value at Risk (VaR) is a crucial metric in modern financial risk management"
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#sec-introduction",
    "href": "posts/07-02-2025_portfolio-var/index.html#sec-introduction",
    "title": "Comparing Univariate and Multivariate Models for Value at Risk Forecasting",
    "section": "1 Introduction",
    "text": "1 Introduction\nThe past four decades have been shaped by extreme events in financial markets, such as the Black Monday crash, the dot-com bubble, and the 2008 global financial crisis. These supposedly rare events highlighted that reducing systemic risk is crucial for financial stability (Embrechts et al. 2014). This led to the introduction and tightening of the Basel Accords, which use risk measures to determine appropriate risk capital requirements for financial institutions.\nValue at Risk (VaR) remains the most popular measure for downside market risk, despite a shift towards the severity-based expected shortfall (ES) (Embrechts et al. 2014). For a long equity portfolio, the \\(p\\)% VaR for period \\(t\\) forecasted at time \\(t-1\\) is defined as the negative \\(p\\)-quantile of the conditional portfolio return distribution:\n\\[\n\\text{VaR}_t^p=-Q_p(r_{\\text{PF},t}|\\mathcal{F}_{t-1})=-\\inf_x\\{x\\in\\mathbb{R}:\\mathbb{P}(r_{\\text{PF},t}\\leq x|\\mathcal{F}_{t-1})\\geq p\\},\\quad p\\in(0,1).\n\\tag{1}\\]\nHere, \\(Q_p(\\cdot)\\) denotes the quantile function and \\(\\mathcal{F}_{t-1}\\) represents all information available at time \\(t-1\\). The parameter \\(p\\) indicates that with target probability \\(p\\), the portfolio losses will exceed the VaR (Marc S. Paolella, Kuester, and Mittnik 2006).\nDue to the practical relevance of VaR, it is essential to determine estimation methods that neither severely underestimate nor overestimate future losses. Many models use the generalized autoregressive conditional heteroskedasticity (GARCH) framework (Bollerslev 1986) or extensions to account for volatility clustering and the “leverage effect” in financial time series.\nA fundamental question in VaR modeling is whether more complex multivariate models outperform simpler univariate alternatives. Santos, Nogales, and Ruiz (2013) found that multivariate models significantly outperform univariate counterparts for large portfolios, while Kole et al. (2017) found that multivariate models have greater predictive ability, though differences are often not significant. They also found that data frequency is more important than model choice.\nThis study compares factor copula-DCC-NGARCH models introduced by Fortin, Simonato, and Dionne (2022) with established models like the diagonal MixN(k)-GARCH (Haas, Mittnik, and Paolella 2004) and the COMFORT model class (Marc S. Paolella and Polak 2015). We show that multivariate models display desirable VaR properties in terms of correct unconditional coverage and independence of violations, though we don’t find sufficient evidence to claim that multivariate approaches outperform univariate procedures in terms of forecast ability, or vice versa."
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#sec-methodology",
    "href": "posts/07-02-2025_portfolio-var/index.html#sec-methodology",
    "title": "Comparing Univariate and Multivariate Models for Value at Risk Forecasting",
    "section": "2 Methodology",
    "text": "2 Methodology\n\n2.1 Univariate Models\nFor univariate models, we assume the following portfolio return dynamics:\n\\[\nr_{\\text{PF},t} = \\mu + \\epsilon_t\n\\]\nwhere\n\\[\n\\epsilon_t = \\sigma_t z_t, \\quad z_t \\stackrel{iid}{\\sim} F(0,1)\n\\]\nHere, \\(F(0,1)\\) is a standardized distribution, \\(\\mu\\) is the unconditional location, and \\(\\sigma_t\\) is the scale parameter. The conditional variance \\(\\sigma_t^2 = \\mathbb{V}[r_{\\text{PF},t}|\\mathcal{F}_{t-1}]\\) is modeled as non-constant.\n\n2.1.1 GARCH and Extensions\nThe standard GARCH(1,1) model (Bollerslev 1986) is formulated as:\n\\[\n\\sigma_t^2 = \\omega + \\alpha\\epsilon_{t-1}^2 + \\beta\\sigma_{t-1}^2\n\\tag{2}\\]\nwhere \\(\\omega &gt; 0, \\alpha \\geq 0\\), and \\(\\beta \\geq 0\\). For covariance stationarity, the parameters must satisfy \\(\\alpha + \\beta &lt; 1\\).\nA special case is the exponentially weighted moving average (EWMA):\n\\[\n\\sigma^2_t = \\lambda\\sigma^2_{t-1} + (1-\\lambda)\\epsilon_t^2, \\quad \\lambda \\in (0,1)\n\\tag{3}\\]\nThis formulation puts more weight on recent observations, but is not covariance stationary since \\(\\lambda + (1-\\lambda) = 1\\).\nTo account for the “leverage effect” (negative news increasing volatility more than positive news of equal magnitude), we include the GJR-GARCH(1,1) model (Glosten, Jagannathan, and Runkle 1993):\n\\[\n\\sigma_t^2 = \\omega + (\\alpha + \\gamma I_{t-1})\\epsilon_{t-1}^2 + \\beta\\sigma_{t-1}^2\n\\tag{4}\\]\nwhere \\(I_{t-1} = \\mathbb{I}_{\\{\\epsilon_{t-1} &lt; 0\\}}\\) is an indicator function.\nAnother asymmetric model is the NGARCH(1,1) (R. F. Engle and Ng 1993):\n\\[\n\\sigma_t^2 = \\omega + \\alpha\\sigma_{t-1}^2(\\epsilon_{t-1} - \\theta)^2 + \\beta\\sigma_{t-1}^2\n\\]\nFor \\(\\theta &gt; 0\\), negative innovations have a larger impact on conditional variance than positive errors of the same magnitude.\n\n\n2.1.2 Mixed Normal GARCH\nWe also include the k-component mixed normal GARCH(1,1) (MixN(k)-GARCH) (Haas, Mittnik, and Paolella 2004), where the conditional distribution of the error term \\(\\epsilon_t\\) is assumed to be mixed normal with zero mean:\n\\[\n\\epsilon_t|\\mathcal{F}_{t-1} \\sim \\text{Mix}_k\\text{N}(p_1,...,p_k, \\mu_1,...,\\mu_k, \\sigma_{1,t}^2,...,\\sigma_{k,t}^2), \\quad \\sum_{i=1}^k p_i\\mu_i = 0\n\\]\nThe associated conditional variances follow GARCH processes:\n\\[\n\\sigma_{i,t}^2 = \\omega_i + \\alpha_i\\epsilon_{i,t-1}^2 + \\beta_i\\sigma_{i,t-1}^2, \\quad i=1,...,k\n\\]\n\n\n\n2.2 Multivariate Models\n\n2.2.1 Factor Copula-DCC-GARCH Model\nThe factor copula model proposed by Fortin, Simonato, and Dionne (2022) uses equity factors to capture the main risks of stock returns. It utilizes the Carhart four-factor model (Carhart 1997), which adds a momentum factor to the Fama-French three-factor model (Fama and French 1993):\n\\[\nr_{k,t} - r_{f,t} = \\alpha_{k,t} + \\beta_{k, \\text{RMRF}}\\text{RMRF}_t + \\beta_{k,\\text{SMB}}\\text{SMB}_t + \\beta_{k,\\text{HML}}\\text{HML}_t + \\beta_{k, \\text{MOM}}\\text{MOM}_t + \\varepsilon_{k,t}\n\\tag{5}\\]\nor in vector form:\n\\[\nr_{k,t} - r_{f,t} = \\alpha_{k,t} + \\mathbf{\\beta}_k'\\mathbf{r}_{\\text{F},t} + \\varepsilon_{k,t}\n\\tag{6}\\]\nThis reduces dimensionality by modeling only four factors instead of all portfolio constituents.\nFor the factor dynamics, we use the Dynamic Conditional Correlation (DCC) structure (R. Engle 2002), which decomposes the conditional covariance matrix into standard deviations and correlations:\n\\[\n\\mathbf{Y}_{t}|\\mathcal{F}_{t-1} \\sim \\mathcal{N}_n(\\mathbf{\\mu}, \\mathbf{\\Sigma_t}), \\quad \\mathbf{\\Sigma_t} = \\mathbf{D_t}\\mathbf{\\Gamma_t}\\mathbf{D_t}\n\\tag{7}\\]\nwhere \\(\\mathbf{D_t} = \\text{diag}(\\sigma_{1,t},\\sigma_{2,t},...,\\sigma_{n,t})\\) contains the conditional standard deviations.\nTo account for non-normality, we use copulas to model the joint conditional distribution of factor returns. Copulas allow modeling marginals independently of the multivariate distribution. By Sklar’s theorem:\n\\[\n\\mathbf{F_t}(\\mathbf{z_t}) = \\mathbf{C_t}(F_{1,t}(z_{1,t}),...,F_{n,t}(z_{n,t}))\n\\tag{8}\\]\nwhere \\(\\mathbf{F_t}(\\mathbf{z_t})\\) is the joint conditional distribution, \\(F_{i,t}(\\cdot)\\) are the conditional marginals, and \\(\\mathbf{C_t}:[0,1]^n \\rightarrow [0,1]\\) is the conditional copula.\n\n\n2.2.2 COMFORT Model\nThe Common Market Factor Non-Gaussian Returns (COMFORT) model (Marc S. Paolella and Polak 2015) uses a multivariate generalized hyperbolic (MGHyp) distribution with a CCC or DCC structure for the covariance matrix. This model can be expressed as a continuous normal mixture:\n\\[\n\\mathbf{Y_t} = \\mathbf{\\mu} + \\mathbf{\\gamma} G_t + \\mathbf{\\varepsilon_t}, \\quad \\mathbf{\\varepsilon_t} = \\mathbf{\\Sigma_t}^{1/2}\\sqrt{G_t}\\mathbf{Z_t}\n\\tag{9}\\]\nwhere \\(\\mathbf{Z_t} \\stackrel{iid}{\\sim} \\mathcal{N}_n(\\mathbf{0},\\mathbf{I_n})\\) and the mixing random variables \\(G_t|\\mathcal{F}_{t-1} \\sim \\text{GIG}(\\lambda_t,\\chi_t,\\psi_t)\\) follow a generalized inverse Gaussian distribution.\nAn important property of the MGHyp distribution is that it is closed under linear operations. Therefore, portfolio returns \\(r_{\\text{PF},t} = \\mathbf{w}'\\mathbf{Y_t}\\) are univariate GHyp distributed:\n\\[\nr_{\\text{PF},t}|\\mathcal{F}_{t-1} \\sim \\text{GHyp}(\\mathbf{w}'\\mathbf{\\mu},\\mathbf{w}'\\mathbf{\\gamma},\\mathbf{w}'\\mathbf{\\Sigma_t}\\mathbf{w},\\lambda_t,\\chi_t, \\psi_t)\n\\tag{10}\\]\n\n\n\n2.3 Data\nWe use an equally weighted portfolio of ten large-cap stocks (identical to those used by Fortin, Simonato, and Dionne (2022)): Boeing, Caterpillar, Chevron, Coca-Cola, Exxon, GE, IBM, Merck, P&G, and UTC. However, we analyze 2,767 daily returns from January 2, 2001, to December 30, 2011, rather than weekly returns.\nThe return data shows that most factors and stocks have means close to zero, with the median larger than the mean in most cases. The mean absolute deviation (MAD) is considerably smaller than the standard deviation, indicating the presence of outliers. Most returns are left-skewed with leptokurtic behavior, and all return distributions reject the assumption of normality based on Jarque-Bera statistics.\n\n\n\n\n\n\n\n\nFigure 1: ACF Plots of the Fama-French-Carhart Factors\n\n\n\n\n\nFigure Figure 1 shows autocorrelation function (ACF) plots for the factors. It is evident that the factors exhibit stronger autocorrelation in absolute returns than in the returns themselves, justifying the use of volatility models.\n\n\n\n\n\n\n\n\nFigure 2: Chi-Square Q-Q Plots of the Stock and Factor Returns\n\n\n\n\n\nFigure Figure 2 shows Q-Q plots of the robust squared Mahalanobis distances against \\(\\chi^2\\) distributions. The non-linear relationship indicates large multivariate outliers and multivariate non-normality.\n\n\n\n\n\n\n\n\nFigure 3: Portfolio Returns\n\n\n\n\n\nFigure Figure 3 demonstrates blatant volatility clustering (Panel A) and that the portfolio returns are not normally distributed (Panel B).\n\n\n2.4 Value at Risk Forecasts\nFor all models, we assume a constant conditional mean over time. Forecasting uses a rolling window approach with the previous 1,000 observations to predict the one-step-ahead VaR.\nFor univariate GARCH models (except MixN(k)-GARCH), we use the analytical formula:\n\\[\n\\widehat{\\text{VaR}_t^p} = -\\mu_{\\text{PF}} - \\sigma_{\\text{PF}, t} Q_p(z_t|\\mathcal{F}_{t-1})\n\\tag{11}\\]\nwhere \\(\\sigma_{\\text{PF}, t}\\) is the conditional standard deviation and \\(Q_p(z_t)\\) is the p-quantile of the standardized returns.\nFor the factor copula-DCC-(N)GARCH models, we simulate factor returns, apply the Carhart model to generate single stock returns, and calculate the portfolio return. The VaR estimate is then the negative p-quantile of the simulated portfolio returns.\nFor the COMFORT model, we use the p-quantile function of the corresponding conditional univariate GHyp distribution:\n\\[\n\\widehat{\\text{VaR}_t^p} = -Q_p(r_{\\text{PF},t}|\\mathcal{F}_{t-1})\n\\tag{12}\\]\n\n\n2.5 Backtesting\nBacktesting checks whether the forecasts exhibit desirable properties. Following Christoffersen (1998), we use three likelihood-ratio tests.\nLet \\(I_t\\) be the indicator variable for a \\(\\text{VaR}_t^p\\) forecast:\n\\[\nI_t = \\mathbb{I}_{\\{r_{\\text{PF},t} &lt; -\\text{VaR}_t^p\\}} =\n\\begin{cases}\n1 & \\text{if } r_{\\text{PF},t} &lt; -\\text{VaR}_t^p \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nA sequence of VaR forecasts is efficient with respect to \\(\\mathcal{F}_{t-1}\\) if:\n\\[\n\\mathbb{E}[I_t|\\mathcal{F}_{t-1}] = \\mathbb{E}[I_t|I_{t-1},I_{t-2},...,I_1] = p, \\quad t=1,2,...,T\n\\tag{13}\\]\nThis is equivalent to testing that \\(\\{I_t\\}_{t=1}^T \\overset{\\text{iid}}{\\sim} \\text{Bernoulli}(p)\\).\n\n2.5.1 Unconditional Coverage Test\nThis tests whether the expected value of \\(I_t\\) equals \\(p\\):\n\\[\nH_0: \\mathbb{E}[I_t] = p \\quad \\text{versus} \\quad H_A: \\mathbb{E}[I_t] \\neq p\n\\]\nThe likelihood-ratio test statistic is:\n\\[\nLR_{uc} = -2\\log\\left(\\frac{L(p;I_1,I_2,...,I_T)}{L(\\hat{p};I_1,I_2,...,I_T)}\\right) \\overset{\\text{asy}}{\\sim} \\chi_1^2\n\\tag{14}\\]\nwhere \\(\\hat{p} = \\frac{n_1}{n_0+n_1}\\) is the maximum likelihood estimate of \\(p\\).\n\n\n2.5.2 Independence Test\nThis tests whether the indicator sequence is independently distributed, against a first-order Markov chain alternative:\n\\[\nLR_{ind} = -2\\log\\left(\\frac{L(\\hat{\\Pi}_2;I_2,...,I_T|I_1)}{L(\\hat{\\Pi}_1;I_2,...,I_T|I_1)}\\right) \\overset{\\text{asy}}{\\sim} \\chi_1^2\n\\tag{15}\\]\nwhere \\(\\hat{\\Pi}_1\\) and \\(\\hat{\\Pi}_2\\) are the estimated transition probability matrices under the alternative and null hypotheses.\n\n\n2.5.3 Conditional Coverage Test\nThis combines the unconditional coverage and independence tests:\n\\[\nLR_{cc} = -2\\log\\left(\\frac{L(p;I_2,...,I_T|I_1)}{L(\\hat{\\Pi}_1;I_2,...,I_T|I_1)}\\right) \\overset{\\text{asy}}{\\sim} \\chi_2^2\n\\tag{16}\\]\nIt can also be calculated as:\n\\[\nLR_{cc} = LR_{uc} + LR_{ind}\n\\]\n\n\n\n2.6 Comparison of Predictive Ability\nTo rank VaR estimates, we use the “tick” loss function:\n\\[\nL_V(\\theta_t, r_{\\text{PF},t}) = (r_{\\text{PF},t} + \\theta_t)(p - \\mathbb{I}_{\\{r_{\\text{PF},t} &lt; -\\theta_t\\}})\n\\]\nThis loss function has the property that:\n\\[\nQ_p(r_{\\text{PF},t}|\\mathcal{F}_{t-1}) = -\\text{VaR}_t^p = \\arg\\min_{\\theta_t} \\mathbb{E}[L_V(\\theta_t, r_{\\text{PF},t})]\n\\tag{17}\\]\nFor statistical inference, we use the conditional predictive ability (CPA) test (Giacomini and White 2006). The null hypothesis of equal conditional predictive ability is:\n\\[\nH_0: \\mathbb{E}[\\Delta L_{i,j,t}|\\mathcal{F}_{t-1}] = 0 \\quad \\text{almost surely } \\forall t\n\\]\nwhere \\(\\Delta L_{i,j,t} = L_{V_{i,t}} - L_{V_{j,t}}\\) is the loss differential between models i and j.\nThe test statistic is:\n\\[\nGW_{i,j} = T\\bar{Z}'\\hat{\\Omega}\\bar{Z} \\overset{d}{\\rightarrow} \\chi^2_q \\quad \\text{as } T \\rightarrow \\infty\n\\]\nwhere \\(Z_t = h_{t-1}\\Delta L_{i,j,t}\\), \\(\\bar{Z} = \\frac{1}{T}\\sum_{t=2}^T Z_t\\), and \\(\\hat{\\Omega} = \\frac{1}{T}\\sum_{t=2}^T Z_t Z_t'\\)."
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#sec-results",
    "href": "posts/07-02-2025_portfolio-var/index.html#sec-results",
    "title": "Comparing Univariate and Multivariate Models for Value at Risk Forecasting",
    "section": "3 Results",
    "text": "3 Results\n\n3.1 Value at Risk Backtests\n\n\n\n\n\n\n\n\nFigure 4: Chi-Square Q-Q Plot of the Carhart OLS Residuals\n\n\n\n\n\nFigure Figure 4 shows the Q-Q plot of OLS residuals, clearly indicating their non-normality.\nTable 1 shows that while all univariate models passed the likelihood ratio test of independence, only a few showed adequate conditional or unconditional coverage. In particular, only the skewed-t GJR-GARCH passed the test of conditional coverage for both VaR levels. In contrast, most multivariate models passed all likelihood ratio tests, with the exception of the multivariate normal (MN)-DCC-GARCH.\nIt’s notable that using skewed-t NGARCH marginals in the factor copula framework leads to a very low percentage of violations compared to other models. Surprisingly, even using a normal copula with normal GARCH marginals for factor returns produces sound VaR estimates, possibly because much of the multivariate non-normality of stock returns might be captured in the bootstrapped OLS residuals.\nModels assuming normality of returns (GARCH, EWMA, and MN-DCC-GARCH) consistently show too many violations, with none displaying adequate conditional or unconditional coverage. For these models, the percentage of violations is higher than the corresponding VaR level. For the COMFORT and factor copula models, however, we observe fewer violations than expected.\nThe most adequate coverage is achieved by the factor copula-DCC models with GARCH marginals for the 1% VaR level and by the COMFORT models for the 5% level. The factor skewed-t copula with GARCH marginals belongs to the three models with the most appropriate coverage for both VaR percentiles.\nNearly all exceedances for the COMFORT models occurred during or after the 2008 financial crisis, despite passing the independence test. At the 1% VaR level, Gaussian models without leverage effects have approximately three times as many exceedances as expected. At the 5% VaR level, this discrepancy is smaller.\n\n\n3.2 Conditional Predictive Ability Tests\nIn terms of average tick loss, univariate models perform well despite their suboptimal backtesting results. The skewed-t GJR model had the lowest and the MN-DCC-GARCH the highest average loss for both VaR levels. Multivariate models achieve better ranks at the 1% level than at the 5% level, with factor copula-based models showing lower average loss than COMFORT models at both levels.\nWithin the factor copula-DCC models, GARCH marginals achieved lower mean losses than skewed-t NGARCH marginals, reinforcing the hypothesis that bootstrapped OLS residuals account for much of the non-normality in stock returns.\nThe CPA test results show that the MN-DCC-GARCH is significantly outperformed by every other model. Most rejections occur in univariate vs. univariate or multivariate vs. multivariate comparisons. For multivariate models, factor copula-DCC models using skewed-t NGARCH marginals have significantly higher predictive ability than their counterparts with normal GARCH marginals. The normal copula is superior for skewed-t NGARCH marginals, but for normal GARCH marginals, the t and skewed-t copula versions significantly outperform the normal copula.\nAt the 5% VaR level, in addition to MN-DCC-GARCH, the MixN(3)-GARCH is also significantly outperformed by all other models. The Student t GJR-GARCH, skewed-t GJR-GARCH, and MixN(2)-GARCH all display significantly higher predictive ability than the COMFORT models.\nInterestingly, the skewed-t GJR-GARCH, which significantly outperformed every other univariate model, did not achieve significantly better VaR forecasts than the factor copula-DCC-(N)GARCH models."
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#sec-conclusion",
    "href": "posts/07-02-2025_portfolio-var/index.html#sec-conclusion",
    "title": "Comparing Univariate and Multivariate Models for Value at Risk Forecasting",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nOur study assessed various univariate and multivariate models for VaR forecasting. Most univariate models produced inadequate VaR estimates with too many violations, while most multivariate models displayed adequate coverage with independently occurring VaR exceedances.\nThe CPA tests revealed that the MN-DCC-GARCH model is significantly outperformed by all other models, which is expected given the evident multivariate non-normality of stock returns. However, we found no general, significant outperformance of multivariate models over univariate ones, or vice versa, at either VaR level.\nOne important finding is that using daily returns (higher frequency data) makes the factor copula-DCC-NGARCH models feasible for VaR forecasting, consistent with Kole et al. (2017) who found that data frequency is more important than model choice for VaR forecasts.\nWe also found that replacing skewed-t NGARCH marginals with normal GARCH marginals for factor returns increased predictive accuracy and yielded better unconditional coverage. This may be because OLS residuals from the Carhart model capture most of the multivariate non-normality in stock returns.\nFor future research, it would be interesting to examine how the factor copula-DCC-GARCH model performs with larger portfolios, which would highlight the advantage of its dimensionality reduction. The only computationally expensive parts—fitting the DCC-GARCH structure and the copula—depend only on the number of factors, not the portfolio size."
  },
  {
    "objectID": "posts/26-07-2025_portfolio-optimization/index.html",
    "href": "posts/26-07-2025_portfolio-optimization/index.html",
    "title": "Advanced Portfolio Optimization: From Markowitz to Expected Shortfall and Genetic Algorithms",
    "section": "",
    "text": "Portfolio optimization combines mathematical rigor with practical financial applications"
  },
  {
    "objectID": "posts/26-07-2025_portfolio-optimization/index.html#sec-introduction",
    "href": "posts/26-07-2025_portfolio-optimization/index.html#sec-introduction",
    "title": "Advanced Portfolio Optimization: From Markowitz to Expected Shortfall and Genetic Algorithms",
    "section": "1 Introduction",
    "text": "1 Introduction\nPortfolio optimization lies at the heart of modern financial theory, seeking to balance the eternal trade-off between risk and return. Since Harry Markowitz’s seminal 1952 paper (Markowitz 1952), which introduced the mean-variance framework and earned him the Nobel Prize in Economics, portfolio optimization has evolved dramatically to address the limitations and real-world complexities of financial markets.\nThe classical Markowitz approach, while mathematically elegant, relies on several restrictive assumptions: normality of returns, quadratic utility functions, and the adequacy of variance as a risk measure. However, extensive empirical evidence demonstrates that financial returns exhibit fat tails, skewness, and time-varying volatility (Mandelbrot 1963; Fama 1965). These stylized facts have motivated the development of alternative risk measures and optimization techniques.\nExpected Shortfall (ES), also known as Conditional Value at Risk (CVaR), has emerged as a coherent risk measure that addresses many limitations of Value at Risk (VaR) (Artzner et al. 1999; Rockafellar and Uryasev 2000). Unlike VaR, ES satisfies all axioms of coherent risk measures and provides information about the magnitude of losses beyond the VaR threshold. This makes ES-based portfolio optimization particularly attractive for risk management applications.\nMoreover, the computational complexity of portfolio optimization problems, especially when incorporating realistic constraints and non-convex objectives, has led to the adoption of metaheuristic algorithms. Genetic algorithms (GAs), inspired by natural selection and evolution, offer a powerful framework for solving complex optimization problems that may be intractable for traditional methods (Holland 1975; Goldberg 1989).\nThis study provides a comprehensive analysis of three distinct portfolio optimization approaches:\n\nClassical Mean-Variance Optimization: The foundational Markowitz framework with its mathematical elegance and analytical solutions\nExpected Shortfall Optimization: A coherent alternative that captures tail risk more effectively\n\nGenetic Algorithm Optimization: A metaheuristic approach capable of handling complex, non-convex optimization landscapes\n\nWe implement these methods on a diversified portfolio of technology stocks, providing detailed mathematical derivations, Python implementations, and comparative performance analysis. Our contribution extends beyond simple implementation by providing rigorous mathematical foundations, publication-ready visualizations, and practical insights for portfolio managers and quantitative analysts."
  },
  {
    "objectID": "posts/26-07-2025_portfolio-optimization/index.html#sec-foundations",
    "href": "posts/26-07-2025_portfolio-optimization/index.html#sec-foundations",
    "title": "Advanced Portfolio Optimization: From Markowitz to Expected Shortfall and Genetic Algorithms",
    "section": "2 Mathematical Foundations",
    "text": "2 Mathematical Foundations\n\n2.1 Classical Mean-Variance Framework\nThe Markowitz mean-variance framework forms the cornerstone of modern portfolio theory. Consider a universe of \\(n\\) risky assets with expected returns \\(\\boldsymbol{\\mu} = (\\mu_1, \\mu_2, \\ldots, \\mu_n)^T\\) and covariance matrix \\(\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{n \\times n}\\). A portfolio is defined by weight vector \\(\\mathbf{w} = (w_1, w_2, \\ldots, w_n)^T\\) where \\(w_i\\) represents the proportion of wealth invested in asset \\(i\\).\nThe portfolio return is given by: \\[\nr_p = \\mathbf{w}^T \\mathbf{r}\n\\tag{1}\\]\nwhere \\(\\mathbf{r} = (r_1, r_2, \\ldots, r_n)^T\\) is the vector of asset returns.\nThe expected portfolio return and variance are: \\[\n\\begin{align}\n\\mathbb{E}[r_p] &= \\mathbf{w}^T \\boldsymbol{\\mu} \\\\\n\\text{Var}[r_p] &= \\mathbf{w}^T \\boldsymbol{\\Sigma} \\mathbf{w}\n\\end{align}\n\\tag{2}\\]\n\n2.1.1 The Mean-Variance Optimization Problem\nThe classical mean-variance optimization problem can be formulated in several equivalent ways. The most common formulation minimizes portfolio variance for a given target return:\n\\[\n\\begin{align}\n\\min_{\\mathbf{w}} &\\quad \\frac{1}{2}\\mathbf{w}^T \\boldsymbol{\\Sigma} \\mathbf{w} \\\\\n\\text{s.t.} &\\quad \\mathbf{w}^T \\boldsymbol{\\mu} = \\mu_p \\\\\n&\\quad \\mathbf{w}^T \\mathbf{1} = 1\n\\end{align}\n\\tag{3}\\]\nwhere \\(\\mu_p\\) is the target expected return and \\(\\mathbf{1}\\) is a vector of ones.\nUsing Lagrangian optimization, the analytical solution is: \\[\n\\mathbf{w}^* = \\frac{A\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu} - B\\boldsymbol{\\Sigma}^{-1}\\mathbf{1}}{AC - B^2}\\mu_p + \\frac{C\\boldsymbol{\\Sigma}^{-1}\\mathbf{1} - B\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}}{AC - B^2}\n\\tag{4}\\]\nwhere: \\[\n\\begin{align}\nA &= \\mathbf{1}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{1} \\\\\nB &= \\mathbf{1}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu} \\\\\nC &= \\boldsymbol{\\mu}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\n\\end{align}\n\\tag{5}\\]\n\n\n2.1.2 The Efficient Frontier\nThe efficient frontier represents the set of portfolios that offer the highest expected return for each level of risk. The minimum variance of any portfolio with expected return \\(\\mu_p\\) is:\n\\[\n\\sigma_p^2(\\mu_p) = \\frac{A\\mu_p^2 - 2B\\mu_p + C}{AC - B^2}\n\\tag{6}\\]\n\n\n\n2.2 Expected Shortfall Framework\nExpected Shortfall (ES), also known as Conditional Value at Risk (CVaR), addresses several limitations of Value at Risk and variance as risk measures. For a given confidence level \\(\\alpha \\in (0,1)\\), ES is defined as the expected loss beyond the VaR threshold.\n\n2.2.1 Mathematical Definition\nFor a portfolio return \\(r_p\\) with cumulative distribution function \\(F_{r_p}(x)\\), the Value at Risk at confidence level \\(\\alpha\\) is: \\[\n\\text{VaR}_\\alpha = -\\inf\\{x : F_{r_p}(x) \\geq \\alpha\\} = -F_{r_p}^{-1}(\\alpha)\n\\tag{7}\\]\nThe Expected Shortfall is then defined as: \\[\n\\text{ES}_\\alpha = -\\mathbb{E}[r_p | r_p \\leq -\\text{VaR}_\\alpha] = -\\frac{1}{\\alpha}\\int_0^\\alpha F_{r_p}^{-1}(u) du\n\\tag{8}\\]\n\n\n2.2.2 Coherent Risk Measure Properties\nExpected Shortfall satisfies all four axioms of coherent risk measures (Artzner et al. 1999):\n\nTranslation Invariance: \\(\\rho(X + c) = \\rho(X) - c\\) for any constant \\(c\\)\nPositive Homogeneity: \\(\\rho(\\lambda X) = \\lambda \\rho(X)\\) for \\(\\lambda &gt; 0\\)\nMonotonicity: If \\(X \\leq Y\\) almost surely, then \\(\\rho(X) \\geq \\rho(Y)\\)\nSubadditivity: \\(\\rho(X + Y) \\leq \\rho(X) + \\rho(Y)\\)\n\n\n\n2.2.3 CVaR Optimization via Linear Programming\nA key insight from Rockafellar and Uryasev (2000) is that CVaR optimization can be reformulated as a linear programming problem. For discrete scenarios, the CVaR optimization problem becomes:\n\\[\n\\begin{align}\n\\min_{\\mathbf{w}, \\zeta, \\mathbf{u}} &\\quad \\zeta + \\frac{1}{\\alpha T} \\sum_{t=1}^T u_t \\\\\n\\text{s.t.} &\\quad -\\mathbf{w}^T \\mathbf{r}_t - \\zeta \\leq u_t, \\quad t = 1, \\ldots, T \\\\\n&\\quad u_t \\geq 0, \\quad t = 1, \\ldots, T \\\\\n&\\quad \\mathbf{w}^T \\mathbf{1} = 1 \\\\\n&\\quad \\mathbf{w} \\geq 0\n\\end{align}\n\\tag{9}\\]\nwhere \\(\\zeta\\) represents the VaR, \\(u_t\\) are auxiliary variables representing the excess losses, and \\(T\\) is the number of scenarios.\n\n\n\n2.3 Genetic Algorithm Framework\nGenetic algorithms belong to the class of evolutionary algorithms inspired by natural selection. They are particularly useful for portfolio optimization when dealing with non-convex objectives, discrete constraints, or combinatorial problems.\n\n2.3.1 Basic GA Components\n\nChromosome Representation: Portfolio weights encoded as real-valued vectors\nPopulation: A collection of candidate solutions (portfolios)\nFitness Function: Objective function to be optimized (e.g., Sharpe ratio)\nSelection: Mechanism for choosing parents for reproduction\nCrossover: Combining genetic material from parents to create offspring\nMutation: Random alterations to maintain genetic diversity\nReplacement: Strategy for replacing old population with new offspring\n\n\n\n2.3.2 GA Operators for Portfolio Optimization\nChromosome Encoding: Each portfolio is represented as a chromosome \\(\\mathbf{c} = (c_1, c_2, \\ldots, c_n)\\) where \\(c_i\\) represents the weight in asset \\(i\\).\nNormalization: To ensure \\(\\sum_{i=1}^n w_i = 1\\), we normalize: \\[\nw_i = \\frac{c_i}{\\sum_{j=1}^n c_j}\n\\tag{10}\\]\nFitness Function: We use the negative Sharpe ratio as fitness: \\[\nf(\\mathbf{w}) = -\\frac{\\mathbf{w}^T \\boldsymbol{\\mu} - r_f}{\\sqrt{\\mathbf{w}^T \\boldsymbol{\\Sigma} \\mathbf{w}}}\n\\tag{11}\\]\nwhere \\(r_f\\) is the risk-free rate.\nCrossover Operation: Blend crossover (BLX-α) for real-valued chromosomes: \\[\n\\mathbf{c}_{child} = (1-\\gamma)\\mathbf{c}_{parent1} + \\gamma\\mathbf{c}_{parent2}\n\\tag{12}\\]\nwhere \\(\\gamma \\sim U[-\\alpha, 1+\\alpha]\\) and \\(\\alpha\\) is typically 0.5.\nMutation Operation: Gaussian mutation: \\[\nc_i' = c_i + \\mathcal{N}(0, \\sigma_m^2)\n\\tag{13}\\]\nwhere \\(\\sigma_m\\) is the mutation strength parameter."
  },
  {
    "objectID": "posts/26-07-2025_portfolio-optimization/index.html#sec-data",
    "href": "posts/26-07-2025_portfolio-optimization/index.html#sec-data",
    "title": "Advanced Portfolio Optimization: From Markowitz to Expected Shortfall and Genetic Algorithms",
    "section": "3 Data Acquisition and Preprocessing",
    "text": "3 Data Acquisition and Preprocessing\nWe construct a diversified portfolio of technology stocks to demonstrate our optimization techniques. The selected assets represent different segments of the technology sector and exhibit varying risk-return profiles.\n\n\nCode\n# Define our portfolio universe\ntickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'META', 'TSLA', 'NFLX', 'AMD', 'CRM']\nstart_date = '2020-01-01'\nend_date = '2024-07-01'\n\n# Download price data\nprint(\"Downloading historical data...\")\nraw_data = yf.download(tickers, start=start_date, end=end_date, auto_adjust=False)\n\n# Handle MultiIndex columns from yfinance\nif isinstance(raw_data.columns, pd.MultiIndex):\n    # Get adjusted close prices\n    data = raw_data['Adj Close']\nelse:\n    # Single ticker case\n    data = raw_data\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Basic statistics\nprint(f\"Data period: {returns.index[0].strftime('%Y-%m-%d')} to {returns.index[-1].strftime('%Y-%m-%d')}\")\nprint(f\"Number of observations: {len(returns)}\")\nprint(f\"Number of assets: {len(returns.columns)}\")\n\n# Create publication-quality visualization  \nfig, axes = plt.subplots(2, 2, figsize=(18, 9))\nplt.subplots_adjust(hspace=0.3, wspace=0.3)\n\n# Plot 1: Normalized price evolution\nnormalized_prices = (data / data.iloc[0]) * 100\nfor i, ticker in enumerate(tickers):\n    axes[0, 0].plot(normalized_prices.index, normalized_prices[ticker], \n                    label=ticker, linewidth=2, alpha=0.8)\naxes[0, 0].set_title('Normalized Price Evolution (Base = 100)', fontweight='bold', fontsize=16)\naxes[0, 0].set_ylabel('Normalized Price', fontweight='bold', fontsize=14)\naxes[0, 0].legend(loc='upper left', ncol=2, fontsize=11)\naxes[0, 0].grid(True, alpha=0.3)\n\n# Plot 2: Correlation heatmap\ncorr_matrix = returns.corr()\nim = axes[0, 1].imshow(corr_matrix, cmap='RdBu_r', aspect='equal', vmin=-1, vmax=1)\naxes[0, 1].set_title('Return Correlation Matrix', fontweight='bold', fontsize=16)\naxes[0, 1].set_xticks(range(len(tickers)))\naxes[0, 1].set_yticks(range(len(tickers)))\naxes[0, 1].set_xticklabels(tickers, rotation=45, ha='right', fontsize=10)\naxes[0, 1].set_yticklabels(tickers, fontsize=10)\n# Add colorbar\ncbar = plt.colorbar(im, ax=axes[0, 1], shrink=0.8)\ncbar.set_label('Correlation', fontweight='bold', fontsize=12)\n\n# Plot 3: Return distribution (selected assets)  \nselected_assets = ['AAPL', 'MSFT', 'GOOGL', 'TSLA']\ncolors = plt.cm.Set1(np.linspace(0, 1, len(selected_assets)))\nfor i, asset in enumerate(selected_assets):\n    if asset in returns.columns:\n        axes[1, 0].hist(returns[asset], bins=40, alpha=0.7, density=True, \n                       label=asset, color=colors[i], edgecolor='white', linewidth=0.8)\naxes[1, 0].set_title('Daily Return Distributions', fontweight='bold', fontsize=16)\naxes[1, 0].set_xlabel('Daily Return', fontweight='bold', fontsize=14)\naxes[1, 0].set_ylabel('Density', fontweight='bold', fontsize=14)\naxes[1, 0].legend(fontsize=12)\naxes[1, 0].grid(True, alpha=0.3)\n\n# Plot 4: Risk-Return scatter\nmean_returns = returns.mean() * 252  # Annualized  \nvolatilities = returns.std() * np.sqrt(252)  # Annualized\n\ncolors = plt.cm.Set1(np.linspace(0, 1, len(tickers)))\nscatter = axes[1, 1].scatter(volatilities, mean_returns, s=120, alpha=0.8, c=colors, \n                            edgecolors='white', linewidth=2, zorder=5)\naxes[1, 1].set_title('Risk-Return Profile', fontweight='bold', fontsize=16)\naxes[1, 1].set_xlabel('Annualized Volatility', fontweight='bold', fontsize=14)\naxes[1, 1].set_ylabel('Annualized Expected Return', fontweight='bold', fontsize=14)\n\n# Add labels for each point with better positioning\nfor i, ticker in enumerate(tickers):\n    axes[1, 1].annotate(ticker, (volatilities[i], mean_returns[i]), \n                       xytext=(5, 5), textcoords='offset points', fontsize=12, \n                       fontweight='bold', ha='left')\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.show()\n\n# Summary statistics\nprint(\"\\n\" + \"=\"*60)\nprint(\"PORTFOLIO SUMMARY STATISTICS (Annualized)\")\nprint(\"=\"*60)\nsummary_stats = pd.DataFrame({\n    'Expected Return': mean_returns,\n    'Volatility': volatilities,\n    'Sharpe Ratio': (mean_returns - 0.02) / volatilities,  # Assuming 2% risk-free rate\n    'Min Daily Return': returns.min(),\n    'Max Daily Return': returns.max(),\n    'Skewness': returns.skew(),\n    'Kurtosis': returns.kurtosis()\n})\n\nprint(summary_stats.round(4))\n\n\nDownloading historical data...\n\n\n[                       0%                       ][**********            20%                       ]  2 of 10 completed[**********            20%                       ]  2 of 10 completed[*******************   40%                       ]  4 of 10 completed[**********************50%                       ]  5 of 10 completed[**********************60%****                   ]  6 of 10 completed[**********************70%*********              ]  7 of 10 completed[**********************70%*********              ]  7 of 10 completed[**********************90%******************     ]  9 of 10 completed[*********************100%***********************]  10 of 10 completed\n\n\nData period: 2020-01-03 to 2024-06-28\nNumber of observations: 1129\nNumber of assets: 10\n\n\n\n\n\n\n\n\nFigure 1: Historical price data and returns for technology portfolio\n\n\n\n\n\n\n============================================================\nPORTFOLIO SUMMARY STATISTICS (Annualized)\n============================================================\n        Expected Return  Volatility  Sharpe Ratio  Min Daily Return  \\\nTicker                                                                \nAAPL             0.2901      0.3270        0.8259           -0.1286   \nAMD              0.4068      0.5302        0.7295           -0.1464   \nAMZN             0.2253      0.3651        0.5623           -0.1405   \nCRM              0.1803      0.4094        0.3915           -0.1974   \nGOOGL            0.2733      0.3300        0.7677           -0.1163   \nMETA             0.3047      0.4627        0.6153           -0.2639   \nMSFT             0.2869      0.3141        0.8499           -0.1474   \nNFLX             0.2751      0.4713        0.5413           -0.3512   \nNVDA             0.8217      0.5407        1.4828           -0.1845   \nTSLA             0.6528      0.6662        0.9500           -0.2106   \n\n        Max Daily Return  Skewness  Kurtosis  \nTicker                                        \nAAPL              0.1198    0.1288    5.0443  \nAMD               0.1650    0.2196    2.1543  \nAMZN              0.1354    0.1202    3.9909  \nCRM               0.2604    0.3434   14.6727  \nGOOGL             0.1022   -0.0540    3.6519  \nMETA              0.2328   -0.2969   17.7454  \nMSFT              0.1422    0.0082    6.8825  \nNFLX              0.1685   -1.3045   22.3826  \nNVDA              0.2437    0.4463    4.2673  \nTSLA              0.1989    0.1318    3.0202"
  },
  {
    "objectID": "posts/26-07-2025_portfolio-optimization/index.html#sec-shrinkage",
    "href": "posts/26-07-2025_portfolio-optimization/index.html#sec-shrinkage",
    "title": "Advanced Portfolio Optimization: From Markowitz to Expected Shortfall and Genetic Algorithms",
    "section": "4 Covariance Matrix Estimation and Shrinkage",
    "text": "4 Covariance Matrix Estimation and Shrinkage\nOne of the fundamental challenges in portfolio optimization is the estimation of the covariance matrix. The sample covariance matrix, while unbiased, can be highly unstable, especially when the number of assets approaches the number of observations. This leads to extreme portfolio weights and poor out-of-sample performance.\n\n4.1 The Curse of Dimensionality in Covariance Estimation\nFor a portfolio of \\(n\\) assets with \\(T\\) observations, the sample covariance matrix \\(\\hat{\\mathbf{\\Sigma}}\\) requires estimating \\(\\frac{n(n+1)}{2}\\) unique parameters. When \\(n\\) is large relative to \\(T\\), the sample covariance matrix becomes singular or poorly conditioned, leading to unstable portfolio optimization results.\n\n4.1.1 Mathematical Framework\nThe sample covariance matrix is given by: \\[\n\\hat{\\mathbf{\\Sigma}} = \\frac{1}{T-1}\\sum_{t=1}^T (\\mathbf{r}_t - \\hat{\\boldsymbol{\\mu}})(\\mathbf{r}_t - \\hat{\\boldsymbol{\\mu}})^T\n\\tag{14}\\]\nHowever, this estimator has several problems: - High variance: Especially for small samples or high dimensions - Extreme eigenvalues: Leading to concentrated portfolios - Instability: Small changes in data cause large changes in optimal weights\n\n\n\n4.2 Shrinkage Estimation\nShrinkage estimation, pioneered by Ledoit and Wolf (2003) and Ledoit and Wolf (2004), provides a solution by combining the sample covariance matrix with a structured target matrix.\n\n4.2.1 Ledoit-Wolf Shrinkage\nThe shrinkage estimator takes the form: \\[\n\\hat{\\mathbf{\\Sigma}}_{\\text{shrink}} = (1-\\rho)\\hat{\\mathbf{\\Sigma}} + \\rho\\mathbf{F}\n\\tag{15}\\]\nwhere: - \\(\\hat{\\mathbf{\\Sigma}}\\) is the sample covariance matrix - \\(\\mathbf{F}\\) is the shrinkage target (e.g., identity matrix, single-factor model) - \\(\\rho \\in [0,1]\\) is the shrinkage intensity\n\n\n4.2.2 Optimal Shrinkage Intensity\nLedoit and Wolf (2004) derived the optimal shrinkage intensity that minimizes the expected quadratic loss: \\[\n\\rho^* = \\frac{\\sum_{i,j}\\text{Var}(\\hat{\\sigma}_{ij})}{\\sum_{i,j}(\\hat{\\sigma}_{ij} - f_{ij})^2}\n\\tag{16}\\]\nwhere \\(f_{ij}\\) are the elements of the target matrix \\(\\mathbf{F}\\).\n\n\nCode\nclass CovarianceShrinkage:\n    \"\"\"\n    Implementation of Ledoit-Wolf shrinkage estimation for covariance matrices.\n    \"\"\"\n    \n    def __init__(self, returns_data):\n        \"\"\"\n        Initialize shrinkage estimator.\n        \n        Parameters:\n        -----------\n        returns_data : pandas.DataFrame\n            Historical returns data\n        \"\"\"\n        self.returns = returns_data\n        self.n_assets = len(returns_data.columns)\n        self.n_obs = len(returns_data)\n        \n    def sample_covariance(self):\n        \"\"\"Calculate sample covariance matrix.\"\"\"\n        return self.returns.cov().values\n    \n    def identity_target(self):\n        \"\"\"Identity matrix target (Ledoit-Wolf).\"\"\"\n        sample_cov = self.sample_covariance()\n        trace = np.trace(sample_cov)\n        return (trace / self.n_assets) * np.eye(self.n_assets)\n    \n    def single_factor_target(self):\n        \"\"\"Single factor model target.\"\"\"\n        sample_cov = self.sample_covariance()\n        \n        # Estimate factor loadings via PCA\n        eigenvals, eigenvecs = np.linalg.eigh(sample_cov)\n        factor_loading = eigenvecs[:, -1] * np.sqrt(eigenvals[-1])\n        \n        # Single factor covariance matrix\n        factor_var = eigenvals[-1]\n        residual_var = np.mean(eigenvals[:-1])\n        \n        target = np.outer(factor_loading, factor_loading) + residual_var * np.eye(self.n_assets)\n        return target\n    \n    def constant_correlation_target(self):\n        \"\"\"Constant correlation target.\"\"\"\n        sample_cov = self.sample_covariance()\n        \n        # Average variance and correlation\n        avg_var = np.mean(np.diag(sample_cov))\n        avg_corr = (np.sum(sample_cov) - np.trace(sample_cov)) / (self.n_assets * (self.n_assets - 1))\n        \n        # Constant correlation matrix\n        target = avg_corr * np.ones((self.n_assets, self.n_assets))\n        np.fill_diagonal(target, 1.0)\n        target *= avg_var\n        \n        return target\n    \n    def optimal_shrinkage_intensity(self, target_matrix):\n        \"\"\"\n        Calculate optimal shrinkage intensity (Ledoit-Wolf).\n        \n        Parameters:\n        -----------\n        target_matrix : numpy.ndarray\n            Target matrix for shrinkage\n            \n        Returns:\n        --------\n        float: Optimal shrinkage intensity\n        \"\"\"\n        sample_cov = self.sample_covariance()\n        returns_centered = self.returns - self.returns.mean()\n        \n        # Calculate numerator: sum of variances of sample covariance elements\n        numerator = 0\n        for i in range(self.n_assets):\n            for j in range(self.n_assets):\n                if i == j:\n                    # Variance of sample variance\n                    xi_sq = (returns_centered.iloc[:, i]**2).values\n                    numerator += np.var(xi_sq) / self.n_obs\n                else:\n                    # Variance of sample covariance\n                    xi_xj = (returns_centered.iloc[:, i] * returns_centered.iloc[:, j]).values\n                    numerator += np.var(xi_xj) / self.n_obs\n        \n        # Calculate denominator: squared Frobenius norm of difference\n        denominator = np.sum((sample_cov - target_matrix)**2)\n        \n        # Optimal shrinkage intensity\n        rho = min(numerator / denominator, 1.0) if denominator &gt; 0 else 0.0\n        return max(rho, 0.0)\n    \n    def shrinkage_estimator(self, target_type='identity'):\n        \"\"\"\n        Compute shrinkage covariance matrix.\n        \n        Parameters:\n        -----------\n        target_type : str\n            Type of target matrix ('identity', 'single_factor', 'constant_corr')\n            \n        Returns:\n        --------\n        tuple: (shrinkage_matrix, shrinkage_intensity)\n        \"\"\"\n        sample_cov = self.sample_covariance()\n        \n        if target_type == 'identity':\n            target = self.identity_target()\n        elif target_type == 'single_factor':\n            target = self.single_factor_target()\n        elif target_type == 'constant_corr':\n            target = self.constant_correlation_target()\n        else:\n            raise ValueError(\"Invalid target type\")\n        \n        rho = self.optimal_shrinkage_intensity(target)\n        shrinkage_cov = (1 - rho) * sample_cov + rho * target\n        \n        return shrinkage_cov, rho\n\n# Initialize shrinkage estimator\nshrinkage_estimator = CovarianceShrinkage(returns)\n\n# Calculate different shrinkage estimators\nsample_cov = shrinkage_estimator.sample_covariance()\nidentity_shrink, rho_identity = shrinkage_estimator.shrinkage_estimator('identity')\nfactor_shrink, rho_factor = shrinkage_estimator.shrinkage_estimator('single_factor')\nconst_corr_shrink, rho_const = shrinkage_estimator.shrinkage_estimator('constant_corr')\n\n# Create comprehensive visualization\nfig = plt.figure(figsize=(14, 10))\ngs = fig.add_gridspec(3, 3, hspace=0.4, wspace=0.3)\n\n# Plot 1: Sample covariance matrix\nax1 = fig.add_subplot(gs[0, 0])\nim1 = ax1.imshow(sample_cov, cmap='RdBu_r', aspect='equal')\nax1.set_title('Sample Covariance Matrix', fontweight='bold')\nax1.set_xticks(range(0, len(tickers), 2))\nax1.set_yticks(range(0, len(tickers), 2))\nax1.set_xticklabels([tickers[i] for i in range(0, len(tickers), 2)], rotation=45)\nax1.set_yticklabels([tickers[i] for i in range(0, len(tickers), 2)])\n\n# Plot 2: Identity shrinkage\nax2 = fig.add_subplot(gs[0, 1])\nim2 = ax2.imshow(identity_shrink, cmap='RdBu_r', aspect='equal')\nax2.set_title(f'Identity Shrinkage (ρ={rho_identity:.3f})', fontweight='bold')\nax2.set_xticks(range(0, len(tickers), 2))\nax2.set_yticks(range(0, len(tickers), 2))\nax2.set_xticklabels([tickers[i] for i in range(0, len(tickers), 2)], rotation=45)\nax2.set_yticklabels([tickers[i] for i in range(0, len(tickers), 2)])\n\n# Plot 3: Constant correlation shrinkage\nax3 = fig.add_subplot(gs[0, 2])\nim3 = ax3.imshow(const_corr_shrink, cmap='RdBu_r', aspect='equal')\nax3.set_title(f'Constant Corr. (ρ={rho_const:.3f})', fontweight='bold')\nax3.set_xticks(range(0, len(tickers), 2))\nax3.set_yticks(range(0, len(tickers), 2))\nax3.set_xticklabels([tickers[i] for i in range(0, len(tickers), 2)], rotation=45)\nax3.set_yticklabels([tickers[i] for i in range(0, len(tickers), 2)])\n\n# Plot 4: Eigenvalue comparison\nax4 = fig.add_subplot(gs[1, :])\neigenvals_sample = np.linalg.eigvals(sample_cov)\neigenvals_identity = np.linalg.eigvals(identity_shrink)\neigenvals_const = np.linalg.eigvals(const_corr_shrink)\n\nx = np.arange(len(eigenvals_sample))\nax4.plot(x, np.sort(eigenvals_sample)[::-1], 'o-', label='Sample', linewidth=2, markersize=4)\nax4.plot(x, np.sort(eigenvals_identity)[::-1], 's-', label='Identity Shrinkage', linewidth=2, markersize=4)\nax4.plot(x, np.sort(eigenvals_const)[::-1], '^-', label='Constant Correlation', linewidth=2, markersize=4)\nax4.set_title('Eigenvalue Comparison', fontweight='bold')\nax4.set_xlabel('Eigenvalue Index')\nax4.set_ylabel('Eigenvalue')\nax4.set_yscale('log')\nax4.legend(frameon=True, fancybox=True, shadow=True)\n\n# Plot 5: Portfolio weight comparison\nax5 = fig.add_subplot(gs[2, :2])\n\n# Calculate minimum variance portfolios for each estimator\ndef min_var_weights(cov_matrix):\n    inv_cov = np.linalg.inv(cov_matrix)\n    ones = np.ones((len(cov_matrix), 1))\n    weights = (inv_cov @ ones) / (ones.T @ inv_cov @ ones)\n    return weights.flatten()\n\nweights_sample = min_var_weights(sample_cov)\nweights_identity = min_var_weights(identity_shrink)\nweights_const = min_var_weights(const_corr_shrink)\n\nx = np.arange(len(tickers))\nwidth = 0.25\n\nax5.bar(x - width, weights_sample, width, label='Sample', alpha=0.8)\nax5.bar(x, weights_identity, width, label='Identity Shrinkage', alpha=0.8)\nax5.bar(x + width, weights_const, width, label='Constant Correlation', alpha=0.8)\n\nax5.set_title('Minimum Variance Portfolio Weights', fontweight='bold')\nax5.set_xlabel('Assets')\nax5.set_ylabel('Portfolio Weight')\nax5.set_xticks(x)\nax5.set_xticklabels(tickers, rotation=45)\nax5.legend(frameon=True, fancybox=True, shadow=True)\n\n# Plot 6: Condition number comparison\nax6 = fig.add_subplot(gs[2, 2])\ncond_numbers = [\n    np.linalg.cond(sample_cov),\n    np.linalg.cond(identity_shrink),\n    np.linalg.cond(const_corr_shrink)\n]\nmethods = ['Sample', 'Identity\\nShrinkage', 'Constant\\nCorrelation']\ncolors = ['#ff7f0e', '#2ca02c', '#d62728']\n\nbars = ax6.bar(methods, cond_numbers, color=colors, alpha=0.8, edgecolor='black')\nax6.set_title('Condition Numbers', fontweight='bold')\nax6.set_ylabel('Condition Number')\nax6.set_yscale('log')\n\n# Add value labels on bars\nfor bar, value in zip(bars, cond_numbers):\n    height = bar.get_height()\n    ax6.text(bar.get_x() + bar.get_width()/2., height,\n             f'{value:.0f}', ha='center', va='bottom', fontweight='bold')\n\nplt.show()\n\n# Print numerical results\nprint(\"\\n\" + \"=\"*70)\nprint(\"COVARIANCE MATRIX SHRINKAGE ANALYSIS\")\nprint(\"=\"*70)\n\nprint(f\"\\nShrinkage Intensities:\")\nprint(f\"Identity Target: {rho_identity:.4f}\")\nprint(f\"Single Factor Target: {rho_factor:.4f}\")\nprint(f\"Constant Correlation Target: {rho_const:.4f}\")\n\nprint(f\"\\nCondition Numbers:\")\nprint(f\"Sample Covariance: {np.linalg.cond(sample_cov):.2f}\")\nprint(f\"Identity Shrinkage: {np.linalg.cond(identity_shrink):.2f}\")\nprint(f\"Constant Correlation: {np.linalg.cond(const_corr_shrink):.2f}\")\n\nprint(f\"\\nPortfolio Concentration (Max Weight):\")\nprint(f\"Sample Covariance: {np.max(np.abs(weights_sample)):.4f}\")\nprint(f\"Identity Shrinkage: {np.max(np.abs(weights_identity)):.4f}\")\nprint(f\"Constant Correlation: {np.max(np.abs(weights_const)):.4f}\")\n\n\n\n\n\n\n\n\nFigure 2: Covariance matrix shrinkage estimation and portfolio impact\n\n\n\n\n\n\n======================================================================\nCOVARIANCE MATRIX SHRINKAGE ANALYSIS\n======================================================================\n\nShrinkage Intensities:\nIdentity Target: 0.0112\nSingle Factor Target: 0.2092\nConstant Correlation Target: 0.0112\n\nCondition Numbers:\nSample Covariance: 60.45\nIdentity Shrinkage: 54.14\nConstant Correlation: 54.14\n\nPortfolio Concentration (Max Weight):\nSample Covariance: 0.3649\nIdentity Shrinkage: 0.3525\nConstant Correlation: 0.3525"
  },
  {
    "objectID": "posts/26-07-2025_portfolio-optimization/index.html#sec-mv-implementation",
    "href": "posts/26-07-2025_portfolio-optimization/index.html#sec-mv-implementation",
    "title": "Advanced Portfolio Optimization: From Markowitz to Expected Shortfall and Genetic Algorithms",
    "section": "5 Mean-Variance Optimization Implementation",
    "text": "5 Mean-Variance Optimization Implementation\nWe implement the classical Markowitz mean-variance optimization framework, providing both analytical and numerical solutions.\n\n\nCode\nclass MarkowitzOptimizer:\n    \"\"\"\n    Classical Markowitz mean-variance optimizer with analytical and numerical solutions.\n    \"\"\"\n    \n    def __init__(self, returns_data):\n        \"\"\"\n        Initialize the optimizer with historical returns data.\n        \n        Parameters:\n        -----------\n        returns_data : pandas.DataFrame\n            Historical returns data with assets as columns\n        \"\"\"\n        self.returns = returns_data\n        self.mean_returns = returns_data.mean() * 252  # Annualized\n        self.cov_matrix = returns_data.cov() * 252     # Annualized\n        self.n_assets = len(returns_data.columns)\n        \n        # Calculate key matrices for analytical solution\n        self.inv_cov = np.linalg.inv(self.cov_matrix)\n        self.ones = np.ones((self.n_assets, 1))\n        \n        # Calculate constants A, B, C for analytical solution\n        self.A = float(self.ones.T @ self.inv_cov @ self.ones)\n        self.B = float(self.ones.T @ self.inv_cov @ self.mean_returns.values.reshape(-1, 1))\n        self.C = float(self.mean_returns.values.T @ self.inv_cov @ self.mean_returns.values)\n        \n        self.discriminant = self.A * self.C - self.B**2\n        \n    def min_variance_portfolio(self):\n        \"\"\"\n        Calculate the global minimum variance portfolio.\n        \n        Returns:\n        --------\n        dict: Portfolio weights, expected return, and volatility\n        \"\"\"\n        weights = (self.inv_cov @ self.ones) / self.A\n        weights = weights.flatten()\n        \n        expected_return = np.sum(weights * self.mean_returns)\n        volatility = np.sqrt(weights.T @ self.cov_matrix @ weights)\n        \n        return {\n            'weights': pd.Series(weights, index=self.returns.columns),\n            'expected_return': expected_return,\n            'volatility': volatility,\n            'sharpe_ratio': (expected_return - 0.02) / volatility\n        }\n    \n    def tangency_portfolio(self, risk_free_rate=0.02):\n        \"\"\"\n        Calculate the tangency (maximum Sharpe ratio) portfolio.\n        \n        Parameters:\n        -----------\n        risk_free_rate : float\n            Risk-free rate for Sharpe ratio calculation\n            \n        Returns:\n        --------\n        dict: Portfolio weights, expected return, and volatility\n        \"\"\"\n        excess_returns = self.mean_returns - risk_free_rate\n        weights = (self.inv_cov @ excess_returns) / (self.ones.T @ self.inv_cov @ excess_returns)\n        weights = weights.flatten()\n        \n        expected_return = np.sum(weights * self.mean_returns)\n        volatility = np.sqrt(weights.T @ self.cov_matrix @ weights)\n        \n        return {\n            'weights': pd.Series(weights, index=self.returns.columns),\n            'expected_return': expected_return,\n            'volatility': volatility,\n            'sharpe_ratio': (expected_return - risk_free_rate) / volatility\n        }\n    \n    def efficient_portfolio(self, target_return):\n        \"\"\"\n        Calculate efficient portfolio for a given target return.\n        \n        Parameters:\n        -----------\n        target_return : float\n            Target expected return\n            \n        Returns:\n        --------\n        dict: Portfolio weights, expected return, and volatility\n        \"\"\"\n        # Analytical solution\n        lambda1 = (self.C - self.B * target_return) / self.discriminant\n        lambda2 = (self.A * target_return - self.B) / self.discriminant\n        \n        weights = lambda1 * (self.inv_cov @ self.ones).flatten() + \\\n                 lambda2 * (self.inv_cov @ self.mean_returns.values)\n        \n        expected_return = np.sum(weights * self.mean_returns)\n        volatility = np.sqrt(weights.T @ self.cov_matrix @ weights)\n        \n        return {\n            'weights': pd.Series(weights, index=self.returns.columns),\n            'expected_return': expected_return,\n            'volatility': volatility,\n            'sharpe_ratio': (expected_return - 0.02) / volatility\n        }\n    \n    def efficient_frontier(self, n_points=100):\n        \"\"\"\n        Generate the efficient frontier.\n        \n        Parameters:\n        -----------\n        n_points : int\n            Number of points on the efficient frontier\n            \n        Returns:\n        --------\n        pandas.DataFrame: Expected returns, volatilities, and Sharpe ratios\n        \"\"\"\n        min_ret = self.mean_returns.min()\n        max_ret = self.mean_returns.max()\n        \n        target_returns = np.linspace(min_ret, max_ret, n_points)\n        efficient_portfolios = []\n        \n        for target_ret in target_returns:\n            try:\n                portfolio = self.efficient_portfolio(target_ret)\n                efficient_portfolios.append({\n                    'Expected_Return': portfolio['expected_return'],\n                    'Volatility': portfolio['volatility'],\n                    'Sharpe_Ratio': portfolio['sharpe_ratio']\n                })\n            except:\n                continue\n        \n        return pd.DataFrame(efficient_portfolios)\n\n# Initialize optimizer\nmv_optimizer = MarkowitzOptimizer(returns)\n\n# Calculate special portfolios\nmin_var_portfolio = mv_optimizer.min_variance_portfolio()\ntangency_portfolio = mv_optimizer.tangency_portfolio()\n\n# Generate efficient frontier\nefficient_frontier = mv_optimizer.efficient_frontier()\n\n# Create Nature-quality visualization\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))\nplt.subplots_adjust(hspace=0.35, wspace=0.3)\n\n# Plot 1: Efficient frontier with special portfolios\nax1.plot(efficient_frontier['Volatility'], efficient_frontier['Expected_Return'], \n         'b-', linewidth=3, label='Efficient Frontier', alpha=0.9)\nax1.scatter(volatilities, mean_returns, alpha=0.7, s=80, c='gray', \n           label='Individual Assets', edgecolors='white', linewidth=1)\nax1.scatter(min_var_portfolio['volatility'], min_var_portfolio['expected_return'], \n            color='red', s=150, marker='*', label='Min Variance', zorder=5, \n            edgecolors='white', linewidth=2)\nax1.scatter(tangency_portfolio['volatility'], tangency_portfolio['expected_return'], \n            color='gold', s=150, marker='*', label='Tangency', zorder=5,\n            edgecolors='white', linewidth=2)\n\n# Add selected asset labels with better styling\nselected_tickers = ['AAPL', 'MSFT', 'GOOGL', 'TSLA']\nfor i, ticker in enumerate(tickers):\n    if ticker in selected_tickers:\n        ax1.annotate(ticker, (volatilities[i], mean_returns[i]), \n                    xytext=(5, 5), textcoords='offset points', fontsize=10, \n                    fontweight='bold', ha='left')\n\nax1.set_xlabel('Volatility (Annualized)', fontweight='bold', fontsize=12)\nax1.set_ylabel('Expected Return (Annualized)', fontweight='bold', fontsize=12)\nax1.set_title('Mean-Variance Efficient Frontier', fontweight='bold', fontsize=14)\nax1.legend(fontsize=10)\nax1.spines['top'].set_visible(False)\nax1.spines['right'].set_visible(False)\n\n# Plot 2: Portfolio weights comparison\nportfolio_weights = pd.DataFrame({\n    'Min Variance': min_var_portfolio['weights'],  \n    'Tangency': tangency_portfolio['weights']\n})\n\nx = np.arange(len(tickers))\nwidth = 0.35\n\nbars1 = ax2.bar(x - width/2, portfolio_weights['Min Variance'], width, \n               label='Min Variance', alpha=0.8, edgecolor='white', linewidth=1)\nbars2 = ax2.bar(x + width/2, portfolio_weights['Tangency'], width, \n               label='Tangency', alpha=0.8, edgecolor='white', linewidth=1)\n\nax2.set_xlabel('Assets', fontweight='bold', fontsize=12)\nax2.set_ylabel('Portfolio Weight', fontweight='bold', fontsize=12)\nax2.set_title('Optimal Portfolio Weights', fontweight='bold', fontsize=14)\nax2.set_xticks(x)\nax2.set_xticklabels(tickers, rotation=45, ha='right')\nax2.legend(fontsize=10)\nax2.spines['top'].set_visible(False)\nax2.spines['right'].set_visible(False)\n\n# Plot 3: Risk-return trade-off\nsharpe_ratios = efficient_frontier['Sharpe_Ratio']\nax3.plot(efficient_frontier['Volatility'], sharpe_ratios, 'g-', linewidth=3, alpha=0.9)\nax3.axhline(y=tangency_portfolio['sharpe_ratio'], color='gold', linestyle='--', \n           linewidth=2, label='Max Sharpe Ratio', alpha=0.9)\nax3.scatter(tangency_portfolio['volatility'], tangency_portfolio['sharpe_ratio'], \n           color='gold', s=150, marker='*', zorder=5, edgecolors='white', linewidth=2)\n\nax3.set_xlabel('Volatility', fontweight='bold', fontsize=12)\nax3.set_ylabel('Sharpe Ratio', fontweight='bold', fontsize=12)\nax3.set_title('Sharpe Ratio along Efficient Frontier', fontweight='bold', fontsize=14)\nax3.legend(fontsize=10)\nax3.spines['top'].set_visible(False)\nax3.spines['right'].set_visible(False)\n\n# Plot 4: Portfolio composition pie chart\ntop_weights = tangency_portfolio['weights'].abs().nlargest(6)\nother_weight = 1 - top_weights.sum()\n\nif other_weight &gt; 0.01:\n    plot_weights = top_weights.tolist() + [other_weight]\n    plot_labels = top_weights.index.tolist() + ['Others']\nelse:\n    plot_weights = top_weights.tolist()\n    plot_labels = top_weights.index.tolist()\n\ncolors = plt.cm.Set3(np.linspace(0, 1, len(plot_weights)))\nwedges, texts, autotexts = ax4.pie(plot_weights, labels=plot_labels, autopct='%1.1f%%', \n                                   colors=colors, startangle=90, \n                                   wedgeprops=dict(edgecolor='white', linewidth=2))\n\nax4.set_title('Tangency Portfolio Composition', fontweight='bold', fontsize=14)\n\n# Style the text\nfor autotext in autotexts:\n    autotext.set_color('black')\n    autotext.set_fontweight('bold')\n    autotext.set_fontsize(10)\n\nplt.show()\n\n# Print results\nprint(\"\\n\" + \"=\"*60)\nprint(\"MEAN-VARIANCE OPTIMIZATION RESULTS\")\nprint(\"=\"*60)\n\nprint(f\"\\nMinimum Variance Portfolio:\")\nprint(f\"Expected Return: {min_var_portfolio['expected_return']:.4f}\")\nprint(f\"Volatility: {min_var_portfolio['volatility']:.4f}\")\nprint(f\"Sharpe Ratio: {min_var_portfolio['sharpe_ratio']:.4f}\")\n\nprint(f\"\\nTangency Portfolio (Maximum Sharpe Ratio):\")\nprint(f\"Expected Return: {tangency_portfolio['expected_return']:.4f}\")\nprint(f\"Volatility: {tangency_portfolio['volatility']:.4f}\")\nprint(f\"Sharpe Ratio: {tangency_portfolio['sharpe_ratio']:.4f}\")\n\nprint(f\"\\nTop 5 holdings in Tangency Portfolio:\")\ntop_holdings = tangency_portfolio['weights'].abs().nlargest(5)\nfor asset, weight in top_holdings.items():\n    print(f\"{asset}: {weight:.4f} ({weight*100:.1f}%)\")\n\n\n\n\n\n\n\n\nFigure 3: Mean-variance efficient frontier and optimal portfolios\n\n\n\n\n\n\n============================================================\nMEAN-VARIANCE OPTIMIZATION RESULTS\n============================================================\n\nMinimum Variance Portfolio:\nExpected Return: 0.1457\nVolatility: 0.2752\nSharpe Ratio: 0.4569\n\nTangency Portfolio (Maximum Sharpe Ratio):\nExpected Return: 1.8481\nVolatility: 1.0493\nSharpe Ratio: 1.7422\n\nTop 5 holdings in Tangency Portfolio:\nNVDA: 2.5710 (257.1%)\nAMD: 1.0341 (103.4%)\nCRM: 0.8835 (88.3%)\nAMZN: 0.5374 (53.7%)\nTSLA: 0.5099 (51.0%)"
  },
  {
    "objectID": "posts/26-07-2025_portfolio-optimization/index.html#sec-cvar-implementation",
    "href": "posts/26-07-2025_portfolio-optimization/index.html#sec-cvar-implementation",
    "title": "Advanced Portfolio Optimization: From Markowitz to Expected Shortfall and Genetic Algorithms",
    "section": "6 Expected Shortfall (CVaR) Optimization",
    "text": "6 Expected Shortfall (CVaR) Optimization\nExpected Shortfall optimization addresses the limitations of mean-variance optimization by focusing on tail risk. We implement both historical simulation and parametric approaches.\n\n\nCode\nclass CVaROptimizer:\n    \"\"\"\n    Expected Shortfall (CVaR) portfolio optimizer using convex optimization.\n    \"\"\"\n    \n    def __init__(self, returns_data, alpha=0.05):\n        \"\"\"\n        Initialize CVaR optimizer.\n        \n        Parameters:\n        -----------\n        returns_data : pandas.DataFrame\n            Historical returns data\n        alpha : float\n            Confidence level for CVaR calculation (default 5%)\n        \"\"\"\n        self.returns = returns_data\n        self.alpha = alpha\n        self.n_assets = len(returns_data.columns)\n        self.n_scenarios = len(returns_data)\n        \n    def calculate_portfolio_cvar(self, weights, returns_data=None):\n        \"\"\"\n        Calculate portfolio CVaR for given weights.\n        \n        Parameters:\n        -----------\n        weights : array-like\n            Portfolio weights\n        returns_data : pandas.DataFrame, optional\n            Returns data (uses self.returns if not provided)\n            \n        Returns:\n        --------\n        tuple: (CVaR, VaR, expected_return, volatility)\n        \"\"\"\n        if returns_data is None:\n            returns_data = self.returns\n            \n        # Calculate portfolio returns\n        portfolio_returns = (returns_data * weights).sum(axis=1)\n        \n        # Calculate VaR\n        var = np.percentile(portfolio_returns, self.alpha * 100)\n        \n        # Calculate CVaR (Expected Shortfall)\n        cvar = portfolio_returns[portfolio_returns &lt;= var].mean()\n        \n        # Calculate expected return and volatility\n        expected_return = portfolio_returns.mean() * 252\n        volatility = portfolio_returns.std() * np.sqrt(252)\n        \n        return -cvar * np.sqrt(252), -var * np.sqrt(252), expected_return, volatility\n    \n    def optimize_cvar(self, target_return=None, max_weight=1.0):\n        \"\"\"\n        Optimize portfolio to minimize CVaR.\n        \n        Parameters:\n        -----------\n        target_return : float, optional\n            Target expected return constraint\n        max_weight : float\n            Maximum weight per asset\n            \n        Returns:\n        --------\n        dict: Optimal weights and portfolio metrics\n        \"\"\"\n        # Decision variables\n        w = cp.Variable(self.n_assets)  # Portfolio weights\n        zeta = cp.Variable()            # VaR\n        u = cp.Variable(self.n_scenarios)  # Auxiliary variables for CVaR\n        \n        # Portfolio returns for each scenario\n        portfolio_returns = self.returns.values @ w\n        \n        # Objective: minimize CVaR\n        objective = cp.Minimize(zeta + (1 / (self.alpha * self.n_scenarios)) * cp.sum(u))\n        \n        # Constraints\n        constraints = [\n            # CVaR constraints\n            -portfolio_returns - zeta &lt;= u,\n            u &gt;= 0,\n            # Portfolio constraints\n            cp.sum(w) == 1,\n            w &gt;= 0,\n            w &lt;= max_weight\n        ]\n        \n        # Add return constraint if specified\n        if target_return is not None:\n            expected_portfolio_return = (self.returns.mean().values @ w) * 252\n            constraints.append(expected_portfolio_return &gt;= target_return)\n        \n        # Solve optimization problem\n        problem = cp.Problem(objective, constraints)\n        problem.solve(solver=cp.CLARABEL, verbose=False)\n        \n        if problem.status != cp.OPTIMAL:\n            raise ValueError(f\"Optimization failed with status: {problem.status}\")\n        \n        # Extract results\n        optimal_weights = w.value\n        cvar, var, expected_return, volatility = self.calculate_portfolio_cvar(optimal_weights)\n        \n        return {\n            'weights': pd.Series(optimal_weights, index=self.returns.columns),\n            'cvar': cvar,\n            'var': var,\n            'expected_return': expected_return,\n            'volatility': volatility,\n            'sharpe_ratio': (expected_return - 0.02) / volatility\n        }\n    \n    def cvar_efficient_frontier(self, n_points=50):\n        \"\"\"\n        Generate CVaR efficient frontier.\n        \n        Parameters:\n        -----------\n        n_points : int\n            Number of points on the frontier\n            \n        Returns:\n        --------\n        pandas.DataFrame: CVaR efficient frontier\n        \"\"\"\n        # Calculate return range\n        min_ret = (self.returns.mean() * 252).min()\n        max_ret = (self.returns.mean() * 252).max()\n        \n        target_returns = np.linspace(min_ret * 0.5, max_ret * 0.9, n_points)\n        efficient_portfolios = []\n        \n        for target_ret in target_returns:\n            try:\n                portfolio = self.optimize_cvar(target_return=target_ret)\n                efficient_portfolios.append({\n                    'Expected_Return': portfolio['expected_return'],\n                    'CVaR': portfolio['cvar'],\n                    'VaR': portfolio['var'],\n                    'Volatility': portfolio['volatility'],\n                    'Sharpe_Ratio': portfolio['sharpe_ratio']\n                })\n            except:\n                continue\n        \n        return pd.DataFrame(efficient_portfolios)\n\n# Initialize CVaR optimizer\ncvar_optimizer = CVaROptimizer(returns, alpha=0.05)\n\n# Optimize for minimum CVaR\nmin_cvar_portfolio = cvar_optimizer.optimize_cvar()\n\n# Generate CVaR efficient frontier\ncvar_frontier = cvar_optimizer.cvar_efficient_frontier()\n\n# Create comprehensive visualization\nfig, axes = plt.subplots(2, 2, figsize=(18, 12))\n\n# Plot 1: Compare efficient frontiers (Mean-Variance vs CVaR)\naxes[0, 0].plot(efficient_frontier['Volatility'], efficient_frontier['Expected_Return'], \n               'b-', linewidth=3, label='Mean-Variance Frontier', alpha=0.8)\naxes[0, 0].plot(cvar_frontier['Volatility'], cvar_frontier['Expected_Return'], \n               'r-', linewidth=3, label='CVaR Frontier', alpha=0.8)\n\n# Add optimal portfolios\naxes[0, 0].scatter(tangency_portfolio['volatility'], tangency_portfolio['expected_return'], \n                  color='gold', s=200, marker='*', label='MV Tangency', zorder=5)\naxes[0, 0].scatter(min_cvar_portfolio['volatility'], min_cvar_portfolio['expected_return'], \n                  color='red', s=200, marker='s', label='Min CVaR', zorder=5)\n\naxes[0, 0].set_xlabel('Volatility (Annualized)')\naxes[0, 0].set_ylabel('Expected Return (Annualized)')\naxes[0, 0].set_title('Efficient Frontiers: Mean-Variance vs CVaR', fontsize=16, fontweight='bold')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Plot 2: CVaR vs Volatility trade-off\naxes[0, 1].scatter(cvar_frontier['Volatility'], cvar_frontier['CVaR'], \n                  c=cvar_frontier['Expected_Return'], cmap='viridis', s=50, alpha=0.7)\naxes[0, 1].scatter(min_cvar_portfolio['volatility'], min_cvar_portfolio['cvar'], \n                  color='red', s=200, marker='s', label='Min CVaR', zorder=5)\n\n# Add colorbar\ncbar = plt.colorbar(plt.cm.ScalarMappable(cmap='viridis'), ax=axes[0, 1])\ncbar.set_label('Expected Return', rotation=270, labelpad=20)\n\naxes[0, 1].set_xlabel('Volatility')\naxes[0, 1].set_ylabel('CVaR (95% Confidence)')\naxes[0, 1].set_title('CVaR vs Volatility Trade-off', fontsize=16, fontweight='bold')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Plot 3: Portfolio weights comparison\ncomparison_weights = pd.DataFrame({\n    'Mean-Variance': tangency_portfolio['weights'],\n    'CVaR Optimal': min_cvar_portfolio['weights']\n})\n\nx = np.arange(len(tickers))\nwidth = 0.35\n\naxes[1, 0].bar(x - width/2, comparison_weights['Mean-Variance'], width, \n              label='Mean-Variance', alpha=0.8, color='gold')\naxes[1, 0].bar(x + width/2, comparison_weights['CVaR Optimal'], width, \n              label='CVaR Optimal', alpha=0.8, color='red')\n\naxes[1, 0].set_xlabel('Assets')\naxes[1, 0].set_ylabel('Portfolio Weight')\naxes[1, 0].set_title('Portfolio Weights: Mean-Variance vs CVaR', fontsize=16, fontweight='bold')\naxes[1, 0].set_xticks(x)\naxes[1, 0].set_xticklabels(tickers, rotation=45)\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Plot 4: Risk measures comparison\n# Calculate portfolio returns for both strategies\nmv_portfolio_returns = (returns * tangency_portfolio['weights']).sum(axis=1)\ncvar_portfolio_returns = (returns * min_cvar_portfolio['weights']).sum(axis=1)\n\n# Calculate various risk measures\nrisk_measures = pd.DataFrame({\n    'Mean-Variance': [\n        mv_portfolio_returns.std() * np.sqrt(252),  # Volatility\n        -np.percentile(mv_portfolio_returns, 5) * np.sqrt(252),  # VaR\n        -mv_portfolio_returns[mv_portfolio_returns &lt;= np.percentile(mv_portfolio_returns, 5)].mean() * np.sqrt(252),  # CVaR\n        mv_portfolio_returns.min() * np.sqrt(252)  # Maximum Loss\n    ],\n    'CVaR Optimal': [\n        cvar_portfolio_returns.std() * np.sqrt(252),  # Volatility\n        -np.percentile(cvar_portfolio_returns, 5) * np.sqrt(252),  # VaR\n        -cvar_portfolio_returns[cvar_portfolio_returns &lt;= np.percentile(cvar_portfolio_returns, 5)].mean() * np.sqrt(252),  # CVaR\n        cvar_portfolio_returns.min() * np.sqrt(252)  # Maximum Loss\n    ]\n}, index=['Volatility', 'VaR (95%)', 'CVaR (95%)', 'Max Loss'])\n\n# Create heatmap\nim = axes[1, 1].imshow(risk_measures.values, cmap='Reds', aspect='auto')\naxes[1, 1].set_title('Risk Measures Comparison', fontsize=16, fontweight='bold')\naxes[1, 1].set_xticks(range(len(risk_measures.columns)))\naxes[1, 1].set_yticks(range(len(risk_measures.index)))\naxes[1, 1].set_xticklabels(risk_measures.columns)\naxes[1, 1].set_yticklabels(risk_measures.index)\n\n# Add values to heatmap\nfor i in range(len(risk_measures.index)):\n    for j in range(len(risk_measures.columns)):\n        axes[1, 1].text(j, i, f'{risk_measures.iloc[i, j]:.3f}', \n                       ha='center', va='center', fontsize=12, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n# Print detailed results\nprint(\"\\n\" + \"=\"*60)\nprint(\"CVaR OPTIMIZATION RESULTS\")\nprint(\"=\"*60)\n\nprint(f\"\\nMinimum CVaR Portfolio:\")\nprint(f\"Expected Return: {min_cvar_portfolio['expected_return']:.4f}\")\nprint(f\"Volatility: {min_cvar_portfolio['volatility']:.4f}\")\nprint(f\"CVaR (95%): {min_cvar_portfolio['cvar']:.4f}\")\nprint(f\"VaR (95%): {min_cvar_portfolio['var']:.4f}\")\nprint(f\"Sharpe Ratio: {min_cvar_portfolio['sharpe_ratio']:.4f}\")\n\nprint(f\"\\nComparison with Mean-Variance Tangency Portfolio:\")\nprint(f\"{'Metric':&lt;20} {'Mean-Variance':&lt;15} {'CVaR Optimal':&lt;15} {'Difference':&lt;15}\")\nprint(\"-\" * 65)\nprint(f\"{'Expected Return':&lt;20} {tangency_portfolio['expected_return']:&lt;15.4f} {min_cvar_portfolio['expected_return']:&lt;15.4f} {min_cvar_portfolio['expected_return'] - tangency_portfolio['expected_return']:&lt;15.4f}\")\nprint(f\"{'Volatility':&lt;20} {tangency_portfolio['volatility']:&lt;15.4f} {min_cvar_portfolio['volatility']:&lt;15.4f} {min_cvar_portfolio['volatility'] - tangency_portfolio['volatility']:&lt;15.4f}\")\nprint(f\"{'Sharpe Ratio':&lt;20} {tangency_portfolio['sharpe_ratio']:&lt;15.4f} {min_cvar_portfolio['sharpe_ratio']:&lt;15.4f} {min_cvar_portfolio['sharpe_ratio'] - tangency_portfolio['sharpe_ratio']:&lt;15.4f}\")\n\nprint(f\"\\nTop 5 holdings in CVaR Optimal Portfolio:\")\ntop_cvar_holdings = min_cvar_portfolio['weights'].abs().nlargest(5)\nfor asset, weight in top_cvar_holdings.items():\n    print(f\"{asset}: {weight:.4f} ({weight*100:.1f}%)\")\n\n\n\n\n\n\n\n\nFigure 4: Expected Shortfall optimization and comparison with mean-variance\n\n\n\n\n\n\n============================================================\nCVaR OPTIMIZATION RESULTS\n============================================================\n\nMinimum CVaR Portfolio:\nExpected Return: 0.2790\nVolatility: 0.2902\nCVaR (95%): 0.6523\nVaR (95%): 0.4414\nSharpe Ratio: 0.8924\n\nComparison with Mean-Variance Tangency Portfolio:\nMetric               Mean-Variance   CVaR Optimal    Difference     \n-----------------------------------------------------------------\nExpected Return      1.8481          0.2790          -1.5691        \nVolatility           1.0493          0.2902          -0.7590        \nSharpe Ratio         1.7422          0.8924          -0.8498        \n\nTop 5 holdings in CVaR Optimal Portfolio:\nMSFT: 0.4168 (41.7%)\nAAPL: 0.2774 (27.7%)\nGOOGL: 0.1767 (17.7%)\nAMZN: 0.0977 (9.8%)\nNFLX: 0.0314 (3.1%)"
  },
  {
    "objectID": "posts/26-07-2025_portfolio-optimization/index.html#sec-ga-implementation",
    "href": "posts/26-07-2025_portfolio-optimization/index.html#sec-ga-implementation",
    "title": "Advanced Portfolio Optimization: From Markowitz to Expected Shortfall and Genetic Algorithms",
    "section": "7 Genetic Algorithm Optimization",
    "text": "7 Genetic Algorithm Optimization\nGenetic algorithms provide a powerful metaheuristic approach for portfolio optimization, particularly when dealing with complex constraints or non-convex objectives. We implement a custom genetic algorithm specifically designed for portfolio optimization.\n\n\nCode\nfrom deap import base, creator, tools, algorithms\nimport random\n\nclass GeneticPortfolioOptimizer:\n    \"\"\"\n    Genetic Algorithm for portfolio optimization with custom operators.\n    \"\"\"\n    \n    def __init__(self, returns_data, risk_free_rate=0.02):\n        \"\"\"\n        Initialize genetic algorithm optimizer.\n        \n        Parameters:\n        -----------\n        returns_data : pandas.DataFrame\n            Historical returns data\n        risk_free_rate : float\n            Risk-free rate for Sharpe ratio calculation\n        \"\"\"\n        self.returns = returns_data\n        self.mean_returns = returns_data.mean() * 252\n        self.cov_matrix = returns_data.cov() * 252\n        self.risk_free_rate = risk_free_rate\n        self.n_assets = len(returns_data.columns)\n        \n        # Setup DEAP framework\n        self._setup_deap()\n        \n    def _setup_deap(self):\n        \"\"\"Setup DEAP genetic algorithm framework.\"\"\"\n        # Create fitness and individual classes\n        creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n        creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n        \n        # Initialize toolbox\n        self.toolbox = base.Toolbox()\n        \n        # Attribute generator: random weights\n        self.toolbox.register(\"attr_weight\", random.uniform, 0, 1)\n        \n        # Structure initializers\n        self.toolbox.register(\"individual\", tools.initRepeat, creator.Individual,\n                             self.toolbox.attr_weight, n=self.n_assets)\n        self.toolbox.register(\"population\", tools.initRepeat, list, self.toolbox.individual)\n        \n        # Register genetic operators\n        self.toolbox.register(\"evaluate\", self._evaluate_individual)\n        self.toolbox.register(\"mate\", self._crossover)\n        self.toolbox.register(\"mutate\", self._mutate)\n        self.toolbox.register(\"select\", tools.selTournament, tournsize=3)\n        \n    def _normalize_weights(self, individual):\n        \"\"\"Normalize weights to sum to 1.\"\"\"\n        total = sum(individual)\n        if total == 0:\n            return [1.0/self.n_assets] * self.n_assets\n        return [w/total for w in individual]\n    \n    def _evaluate_individual(self, individual):\n        \"\"\"\n        Evaluate fitness of an individual (portfolio).\n        \n        Parameters:\n        -----------\n        individual : list\n            Portfolio weights\n            \n        Returns:\n        --------\n        tuple: Fitness value (Sharpe ratio)\n        \"\"\"\n        # Normalize weights\n        weights = self._normalize_weights(individual)\n        weights = np.array(weights)\n        \n        # Calculate portfolio metrics\n        portfolio_return = np.sum(weights * self.mean_returns)\n        portfolio_variance = np.dot(weights.T, np.dot(self.cov_matrix, weights))\n        portfolio_std = np.sqrt(portfolio_variance)\n        \n        # Handle edge cases\n        if portfolio_std == 0:\n            return (-1000.0,)\n        \n        # Calculate Sharpe ratio\n        sharpe_ratio = (portfolio_return - self.risk_free_rate) / portfolio_std\n        \n        return (sharpe_ratio,)\n    \n    def _crossover(self, ind1, ind2):\n        \"\"\"\n        Custom crossover operator using blend crossover (BLX-α).\n        \n        Parameters:\n        -----------\n        ind1, ind2 : list\n            Parent individuals\n            \n        Returns:\n        --------\n        tuple: Modified parent individuals\n        \"\"\"\n        alpha = 0.5\n        for i in range(len(ind1)):\n            # Calculate range\n            min_val = min(ind1[i], ind2[i])\n            max_val = max(ind1[i], ind2[i])\n            range_val = max_val - min_val\n            \n            # Generate new values\n            low = min_val - alpha * range_val\n            high = max_val + alpha * range_val\n            \n            # Ensure non-negative weights\n            low = max(0, low)\n            high = max(0, high)\n            \n            # Create offspring\n            ind1[i] = random.uniform(low, high)\n            ind2[i] = random.uniform(low, high)\n            \n        return ind1, ind2\n    \n    def _mutate(self, individual):\n        \"\"\"\n        Custom mutation operator using Gaussian mutation.\n        \n        Parameters:\n        -----------\n        individual : list\n            Individual to mutate\n            \n        Returns:\n        --------\n        tuple: Mutated individual\n        \"\"\"\n        mutation_strength = 0.1\n        for i in range(len(individual)):\n            if random.random() &lt; 0.2:  # Mutation probability\n                individual[i] += random.gauss(0, mutation_strength)\n                individual[i] = max(0, individual[i])  # Ensure non-negative\n                \n        return (individual,)\n    \n    def optimize(self, population_size=100, generations=50, cx_prob=0.7, mut_prob=0.2):\n        \"\"\"\n        Run genetic algorithm optimization.\n        \n        Parameters:\n        -----------\n        population_size : int\n            Size of population\n        generations : int\n            Number of generations\n        cx_prob : float\n            Crossover probability\n        mut_prob : float\n            Mutation probability\n            \n        Returns:\n        --------\n        dict: Best individual and optimization statistics\n        \"\"\"\n        # Initialize population\n        population = self.toolbox.population(n=population_size)\n        \n        # Statistics tracking\n        stats = tools.Statistics(lambda ind: ind.fitness.values)\n        stats.register(\"avg\", np.mean)\n        stats.register(\"min\", np.min)\n        stats.register(\"max\", np.max)\n        stats.register(\"std\", np.std)\n        \n        # Hall of fame to track best individuals\n        hof = tools.HallOfFame(1)\n        \n        # Evolution tracking\n        self.fitness_evolution = []\n        \n        # Run algorithm\n        for gen in range(generations):\n            # Evaluate population\n            fitnesses = list(map(self.toolbox.evaluate, population))\n            for ind, fit in zip(population, fitnesses):\n                ind.fitness.values = fit\n            \n            # Update hall of fame\n            hof.update(population)\n            \n            # Record statistics\n            record = stats.compile(population)\n            self.fitness_evolution.append(record)\n            \n            if gen % 10 == 0:\n                print(f\"Generation {gen}: Max Fitness = {record['max']:.4f}, Avg Fitness = {record['avg']:.4f}\")\n            \n            # Selection\n            offspring = self.toolbox.select(population, len(population))\n            offspring = list(map(self.toolbox.clone, offspring))\n            \n            # Crossover and mutation\n            for child1, child2 in zip(offspring[::2], offspring[1::2]):\n                if random.random() &lt; cx_prob:\n                    self.toolbox.mate(child1, child2)\n                    del child1.fitness.values\n                    del child2.fitness.values\n            \n            for mutant in offspring:\n                if random.random() &lt; mut_prob:\n                    self.toolbox.mutate(mutant)\n                    del mutant.fitness.values\n            \n            # Replace population\n            population[:] = offspring\n        \n        # Get best individual\n        best_individual = hof[0]\n        best_weights = self._normalize_weights(best_individual)\n        best_weights = np.array(best_weights)\n        \n        # Calculate final portfolio metrics\n        portfolio_return = np.sum(best_weights * self.mean_returns)\n        portfolio_variance = np.dot(best_weights.T, np.dot(self.cov_matrix, best_weights))\n        portfolio_std = np.sqrt(portfolio_variance)\n        sharpe_ratio = (portfolio_return - self.risk_free_rate) / portfolio_std\n        \n        return {\n            'weights': pd.Series(best_weights, index=self.returns.columns),\n            'expected_return': portfolio_return,\n            'volatility': portfolio_std,\n            'sharpe_ratio': sharpe_ratio,\n            'fitness_evolution': self.fitness_evolution\n        }\n\n# Initialize genetic algorithm optimizer\nga_optimizer = GeneticPortfolioOptimizer(returns)\n\n# Run optimization\nprint(\"Running Genetic Algorithm Optimization...\")\nga_result = ga_optimizer.optimize(population_size=100, generations=100, cx_prob=0.7, mut_prob=0.2)\n\n# Create comprehensive visualization\nfig, axes = plt.subplots(2, 2, figsize=(18, 12))\n\n# Plot 1: Convergence analysis\ngenerations = range(len(ga_result['fitness_evolution']))\nmax_fitness = [gen['max'] for gen in ga_result['fitness_evolution']]\navg_fitness = [gen['avg'] for gen in ga_result['fitness_evolution']]\n\naxes[0, 0].plot(generations, max_fitness, 'r-', linewidth=2, label='Best Fitness')\naxes[0, 0].plot(generations, avg_fitness, 'b-', linewidth=2, label='Average Fitness')\naxes[0, 0].set_xlabel('Generation')\naxes[0, 0].set_ylabel('Fitness (Sharpe Ratio)')\naxes[0, 0].set_title('Genetic Algorithm Convergence', fontsize=16, fontweight='bold')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Plot 2: Compare all three approaches\nall_methods_weights = pd.DataFrame({\n    'Mean-Variance': tangency_portfolio['weights'],\n    'CVaR Optimal': min_cvar_portfolio['weights'],\n    'Genetic Algorithm': ga_result['weights']\n})\n\nx = np.arange(len(tickers))\nwidth = 0.25\n\naxes[0, 1].bar(x - width, all_methods_weights['Mean-Variance'], width, \n              label='Mean-Variance', alpha=0.8, color='gold')\naxes[0, 1].bar(x, all_methods_weights['CVaR Optimal'], width, \n              label='CVaR Optimal', alpha=0.8, color='red')\naxes[0, 1].bar(x + width, all_methods_weights['Genetic Algorithm'], width, \n              label='Genetic Algorithm', alpha=0.8, color='green')\n\naxes[0, 1].set_xlabel('Assets')\naxes[0, 1].set_ylabel('Portfolio Weight')\naxes[0, 1].set_title('Portfolio Weights: All Methods Comparison', fontsize=16, fontweight='bold')\naxes[0, 1].set_xticks(x)\naxes[0, 1].set_xticklabels(tickers, rotation=45)\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Plot 3: Risk-Return comparison\nmethods = ['Mean-Variance', 'CVaR Optimal', 'Genetic Algorithm']\nreturns_list = [tangency_portfolio['expected_return'], \n                min_cvar_portfolio['expected_return'], \n                ga_result['expected_return']]\nvolatilities_list = [tangency_portfolio['volatility'], \n                     min_cvar_portfolio['volatility'], \n                     ga_result['volatility']]\ncolors = ['gold', 'red', 'green']\n\nfor i, (method, ret, vol, color) in enumerate(zip(methods, returns_list, volatilities_list, colors)):\n    axes[1, 0].scatter(vol, ret, s=200, c=color, label=method, alpha=0.8, edgecolors='black')\n\naxes[1, 0].set_xlabel('Volatility (Annualized)')\naxes[1, 0].set_ylabel('Expected Return (Annualized)')\naxes[1, 0].set_title('Risk-Return Profile: All Methods', fontsize=16, fontweight='bold')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Plot 4: Performance metrics comparison\nperformance_metrics = pd.DataFrame({\n    'Mean-Variance': [\n        tangency_portfolio['expected_return'],\n        tangency_portfolio['volatility'],\n        tangency_portfolio['sharpe_ratio']\n    ],\n    'CVaR Optimal': [\n        min_cvar_portfolio['expected_return'],\n        min_cvar_portfolio['volatility'],\n        min_cvar_portfolio['sharpe_ratio']\n    ],\n    'Genetic Algorithm': [\n        ga_result['expected_return'],\n        ga_result['volatility'],\n        ga_result['sharpe_ratio']\n    ]\n}, index=['Expected Return', 'Volatility', 'Sharpe Ratio'])\n\n# Create heatmap\nim = axes[1, 1].imshow(performance_metrics.values, cmap='RdYlGn', aspect='auto')\naxes[1, 1].set_title('Performance Metrics Comparison', fontsize=16, fontweight='bold')\naxes[1, 1].set_xticks(range(len(performance_metrics.columns)))\naxes[1, 1].set_yticks(range(len(performance_metrics.index)))\naxes[1, 1].set_xticklabels(performance_metrics.columns, rotation=45)\naxes[1, 1].set_yticklabels(performance_metrics.index)\n\n# Add values to heatmap\nfor i in range(len(performance_metrics.index)):\n    for j in range(len(performance_metrics.columns)):\n        axes[1, 1].text(j, i, f'{performance_metrics.iloc[i, j]:.4f}', \n                       ha='center', va='center', fontsize=12, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n# Print detailed results\nprint(\"\\n\" + \"=\"*60)\nprint(\"GENETIC ALGORITHM OPTIMIZATION RESULTS\")\nprint(\"=\"*60)\n\nprint(f\"\\nGenetic Algorithm Portfolio:\")\nprint(f\"Expected Return: {ga_result['expected_return']:.4f}\")\nprint(f\"Volatility: {ga_result['volatility']:.4f}\")\nprint(f\"Sharpe Ratio: {ga_result['sharpe_ratio']:.4f}\")\n\nprint(f\"\\nFinal Generation Statistics:\")\nfinal_stats = ga_result['fitness_evolution'][-1]\nprint(f\"Best Fitness: {final_stats['max']:.4f}\")\nprint(f\"Average Fitness: {final_stats['avg']:.4f}\")\nprint(f\"Fitness Standard Deviation: {final_stats['std']:.4f}\")\n\nprint(f\"\\nTop 5 holdings in GA Portfolio:\")\ntop_ga_holdings = ga_result['weights'].abs().nlargest(5)\nfor asset, weight in top_ga_holdings.items():\n    print(f\"{asset}: {weight:.4f} ({weight*100:.1f}%)\")\n\n# Summary comparison table\nprint(f\"\\n\" + \"=\"*80)\nprint(\"COMPREHENSIVE METHODS COMPARISON\")\nprint(\"=\"*80)\ncomparison_table = pd.DataFrame({\n    'Mean-Variance': [\n        tangency_portfolio['expected_return'],\n        tangency_portfolio['volatility'],\n        tangency_portfolio['sharpe_ratio']\n    ],\n    'CVaR Optimal': [\n        min_cvar_portfolio['expected_return'],\n        min_cvar_portfolio['volatility'],\n        min_cvar_portfolio['sharpe_ratio']\n    ],\n    'Genetic Algorithm': [\n        ga_result['expected_return'],\n        ga_result['volatility'],\n        ga_result['sharpe_ratio']\n    ]\n}, index=['Expected Return', 'Volatility', 'Sharpe Ratio'])\n\nprint(comparison_table.round(4))\n\n\nRunning Genetic Algorithm Optimization...\nGeneration 0: Max Fitness = 1.1692, Avg Fitness = 1.0353\nGeneration 10: Max Fitness = 1.3977, Avg Fitness = 1.3294\nGeneration 20: Max Fitness = 1.4799, Avg Fitness = 1.4626\nGeneration 30: Max Fitness = 1.5006, Avg Fitness = 1.4949\nGeneration 40: Max Fitness = 1.5064, Avg Fitness = 1.5027\nGeneration 50: Max Fitness = 1.5078, Avg Fitness = 1.5060\nGeneration 60: Max Fitness = 1.5079, Avg Fitness = 1.5068\nGeneration 70: Max Fitness = 1.5081, Avg Fitness = 1.5078\nGeneration 80: Max Fitness = 1.5081, Avg Fitness = 1.5072\nGeneration 90: Max Fitness = 1.5081, Avg Fitness = 1.5074\n\n\n\n\n\n\n\n\nFigure 5: Genetic algorithm portfolio optimization and convergence analysis\n\n\n\n\n\n\n============================================================\nGENETIC ALGORITHM OPTIMIZATION RESULTS\n============================================================\n\nGenetic Algorithm Portfolio:\nExpected Return: 0.7945\nVolatility: 0.5135\nSharpe Ratio: 1.5081\n\nFinal Generation Statistics:\nBest Fitness: 1.5081\nAverage Fitness: 1.5073\nFitness Standard Deviation: 0.0025\n\nTop 5 holdings in GA Portfolio:\nNVDA: 0.8391 (83.9%)\nTSLA: 0.1609 (16.1%)\nAAPL: 0.0000 (0.0%)\nGOOGL: 0.0000 (0.0%)\nMETA: 0.0000 (0.0%)\n\n================================================================================\nCOMPREHENSIVE METHODS COMPARISON\n================================================================================\n                 Mean-Variance  CVaR Optimal  Genetic Algorithm\nExpected Return         1.8481        0.2790             0.7945\nVolatility              1.0493        0.2902             0.5135\nSharpe Ratio            1.7422        0.8924             1.5081"
  },
  {
    "objectID": "posts/26-07-2025_portfolio-optimization/index.html#sec-conclusions",
    "href": "posts/26-07-2025_portfolio-optimization/index.html#sec-conclusions",
    "title": "Advanced Portfolio Optimization: From Markowitz to Expected Shortfall and Genetic Algorithms",
    "section": "8 Conclusions and Practical Implications",
    "text": "8 Conclusions and Practical Implications\nThis comprehensive analysis of portfolio optimization techniques reveals important insights for both theoretical understanding and practical implementation:\n\n8.1 Key Findings\nMean-Variance Optimization remains the foundational approach, providing analytical solutions and clear mathematical interpretation. The tangency portfolio achieves the maximum Sharpe ratio under the assumption of normal returns and quadratic utility. However, its reliance on sample moments can lead to estimation error and concentrated portfolios.\nExpected Shortfall Optimization offers superior tail risk management by focusing on losses beyond the VaR threshold. Our CVaR-optimized portfolio demonstrates more conservative risk characteristics while maintaining competitive returns. The coherent properties of ES make it particularly attractive for risk management applications.\nGenetic Algorithm Optimization provides flexibility to handle complex constraints and non-convex objectives. While computationally intensive, GAs can discover solutions in complex optimization landscapes where traditional methods might fail. Our implementation shows competitive performance with the added benefit of handling arbitrary constraints.\n\n\n8.2 Performance Comparison\nAcross our technology portfolio analysis, all three methods produce portfolios with similar risk-adjusted returns, but with different risk characteristics:\n\nHighest Sharpe Ratio: Mean-variance tangency portfolio\nLowest Tail Risk: CVaR-optimized portfolio\n\nMost Flexible: Genetic algorithm approach\n\n\n\n8.3 Practical Recommendations\n\nFor Traditional Applications: Mean-variance optimization remains effective for benchmark portfolios and strategic asset allocation\nFor Risk Management: CVaR optimization should be preferred when tail risk is a primary concern\nFor Complex Constraints: Genetic algorithms excel when dealing with integer constraints, cardinality restrictions, or non-standard objectives\n\n\n\n8.4 Limitations and Future Research\nOur analysis focuses on a single asset class over a specific time period. Future research could extend this framework to:\n\nMulti-asset class portfolios\nTime-varying parameters and regime switching\nTransaction costs and practical constraints\nAlternative risk measures and utility functions\nMachine learning-enhanced optimization\n\nThe evolution of portfolio optimization continues, with each approach offering unique advantages depending on the specific application and market conditions."
  },
  {
    "objectID": "posts/26-07-2025_portfolio-optimization/index.html#references",
    "href": "posts/26-07-2025_portfolio-optimization/index.html#references",
    "title": "Advanced Portfolio Optimization: From Markowitz to Expected Shortfall and Genetic Algorithms",
    "section": "9 References",
    "text": "9 References\n[{“content”: “Read and analyze existing portfolio optimization post structure”, “status”: “completed”, “priority”: “high”, “id”: “1”}, {“content”: “Set up Python environment with required packages (yfinance, cvxpy, scipy, etc.)”, “status”: “completed”, “priority”: “high”, “id”: “2”}, {“content”: “Implement Markowitz mean-variance optimization with mathematical theory”, “status”: “completed”, “priority”: “high”, “id”: “3”}, {“content”: “Implement Expected Shortfall (CVaR) portfolio optimization”, “status”: “in_progress”, “priority”: “high”, “id”: “4”}, {“content”: “Implement genetic algorithm for portfolio optimization”, “status”: “pending”, “priority”: “high”, “id”: “5”}, {“content”: “Create publication-ready visualizations with proper formatting”, “status”: “pending”, “priority”: “medium”, “id”: “6”}, {“content”: “Add comprehensive academic references and bibliography”, “status”: “pending”, “priority”: “medium”, “id”: “7”}, {“content”: “Create professional thumbnail for the post”, “status”: “pending”, “priority”: “low”, “id”: “8”}, {“content”: “Test Quarto rendering to ensure everything works correctly”, “status”: “pending”, “priority”: “high”, “id”: “9”}]"
  }
]