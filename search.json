[
  {
    "objectID": "posts/03-04-2025_intro_to_sde/index.html",
    "href": "posts/03-04-2025_intro_to_sde/index.html",
    "title": "Stochastic Differential Equations: From Mathematical Foundations to Modern Applications",
    "section": "",
    "text": "Stochastic differential equations provide the mathematical framework for modeling continuous-time random processes, with applications spanning from option pricing to generative AI models"
  },
  {
    "objectID": "posts/03-04-2025_intro_to_sde/index.html#sec-introduction",
    "href": "posts/03-04-2025_intro_to_sde/index.html#sec-introduction",
    "title": "Stochastic Differential Equations: From Mathematical Foundations to Modern Applications",
    "section": "1 Introduction",
    "text": "1 Introduction\nStochastic differential equations (SDEs) represent one of the most profound and mathematically elegant frameworks in modern probability theory and applied mathematics. These equations describe the evolution of random processes in continuous time, providing the mathematical foundation for modeling phenomena characterized by both deterministic trends and random fluctuations (oksendal2003?; karatzas1991?).\nThe theoretical development of SDE theory emerged from the intersection of probability theory, differential equations, and mathematical finance, culminating in the revolutionary work of Kiyoshi Itô in the 1940s. Itô’s groundbreaking construction of stochastic integration transformed our understanding of random processes and established the mathematical framework that now underpins modern quantitative finance, statistical physics, and machine learning (ito1951?; mckean1969?).\n\n1.1 Historical Development and Motivation\nThe genesis of stochastic differential equations can be traced to Louis Bachelier’s pioneering 1900 thesis on option pricing, where he first applied Brownian motion to financial markets (bachelier1900?). However, it was not until Norbert Wiener’s rigorous mathematical construction of Brownian motion in the 1920s that the field gained its theoretical foundation (wiener1923?).\nThe transformative breakthrough came with Itô’s development of stochastic calculus in the 1940s and 1950s. Itô recognized that classical calculus was inadequate for handling functions of Brownian motion due to their non-differentiable nature, leading to his construction of stochastic integration and the famous Itô’s lemma (ito1951?). This work established the mathematical machinery necessary for:\n\nRigorous treatment of random processes: Moving beyond heuristic arguments to mathematically precise formulations\nFinancial modeling: Providing the foundation for modern option pricing theory through the Black-Scholes model\nEngineering applications: Enabling analysis of systems subject to random disturbances\nMachine learning: Supporting modern developments in neural differential equations and diffusion models\n\n\n\n1.2 Contemporary Relevance and Applications\nIn the 21st century, SDEs have experienced a renaissance driven by advances in computational methods and emerging applications in machine learning. Key contemporary developments include:\nMathematical Finance: SDEs form the backbone of modern derivatives pricing, risk management, and portfolio optimization. Models like the Heston stochastic volatility model and interest rate models (Vasicek, Cox-Ingersoll-Ross) are built on SDE foundations (heston1993?; cox1985?).\nMachine Learning and AI: Recent breakthroughs in generative modeling, particularly diffusion models for image generation, rely heavily on SDE theory. Neural ordinary differential equations (NODEs) and neural SDEs represent cutting-edge applications of stochastic analysis to deep learning (chen2018?; song2021?).\nScientific Computing: SDEs provide essential tools for modeling complex systems in physics, biology, and engineering where random effects play a crucial role (gardiner2009?).\n\n\n1.3 Scope and Mathematical Prerequisites\nThis treatise provides a comprehensive, PhD-level treatment of stochastic differential equation theory and applications. We assume familiarity with:\n\nReal analysis: Measure theory, Lebesgue integration, and functional analysis\nProbability theory: Probability spaces, random variables, and basic stochastic processes\nDifferential equations: Ordinary differential equations and partial differential equations\nLinear algebra: Matrix theory and spectral analysis\n\nOur systematic development progresses through:\nTheoretical Foundations (Sections 2-4): We establish the mathematical framework, beginning with Brownian motion and filtrations, developing Itô calculus, and proving fundamental existence and uniqueness theorems.\nNumerical Methods (Section 5): We examine computational approaches including Euler-Maruyama and Milstein schemes, analyzing convergence properties and implementation considerations.\nFinancial Applications (Section 6): We explore classical applications in option pricing, interest rate modeling, and risk management, providing complete derivations and implementations.\nModern Machine Learning Applications (Sections 7-8): We investigate contemporary applications in neural differential equations, diffusion models, and Gaussian processes, connecting classical theory to cutting-edge developments.\nAdvanced Topics (Section 9): We cover jump-diffusion processes, stochastic volatility models, and path-dependent derivatives.\nThroughout, we provide rigorous mathematical proofs, comprehensive Python implementations optimized with modern computational libraries, and publication-quality visualizations that illuminate key concepts and facilitate practical application."
  },
  {
    "objectID": "posts/03-04-2025_intro_to_sde/index.html#sec-foundations",
    "href": "posts/03-04-2025_intro_to_sde/index.html#sec-foundations",
    "title": "Stochastic Differential Equations: From Mathematical Foundations to Modern Applications",
    "section": "2 Mathematical Foundations",
    "text": "2 Mathematical Foundations\n\n2.1 Probability Spaces and Filtrations\nThe rigorous development of stochastic differential equation theory requires careful construction of the underlying probability framework. We begin with the fundamental mathematical structures that support stochastic analysis.\nDefinition 2.1 (Probability Space): A probability space is a triple \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) where: - \\(\\Omega\\) is the sample space representing all possible outcomes - \\(\\mathcal{F}\\) is a \\(\\sigma\\)-algebra on \\(\\Omega\\) representing measurable events - \\(\\mathbb{P}: \\mathcal{F} \\to [0,1]\\) is a probability measure satisfying \\(\\mathbb{P}(\\Omega) = 1\\)\nDefinition 2.2 (Filtration): A filtration \\(\\{\\mathcal{F}_t\\}_{t \\geq 0}\\) is an increasing family of sub-\\(\\sigma\\)-algebras of \\(\\mathcal{F}\\): \\[\\mathcal{F}_s \\subseteq \\mathcal{F}_t \\subseteq \\mathcal{F} \\quad \\text{for all } 0 \\leq s \\leq t\\]\nThe filtration represents the evolution of information over time, where \\(\\mathcal{F}_t\\) contains all events observable up to time \\(t\\).\n\n\n2.2 Brownian Motion and the Wiener Process\nBrownian motion forms the cornerstone of stochastic calculus, providing the fundamental building block for constructing more complex stochastic processes.\nDefinition 2.3 (Standard Brownian Motion): A stochastic process \\(\\{W_t\\}_{t \\geq 0}\\) defined on \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) is called standard Brownian motion if:\n\nInitial condition: \\(W_0 = 0\\) almost surely\nIndependent increments: For any \\(0 \\leq t_1 &lt; t_2 &lt; \\cdots &lt; t_n\\), the increments \\(W_{t_2} - W_{t_1}, W_{t_3} - W_{t_2}, \\ldots, W_{t_n} - W_{t_{n-1}}\\) are independent\nGaussian increments: For any \\(s &lt; t\\), \\(W_t - W_s \\sim \\mathcal{N}(0, t-s)\\)\nContinuous paths: \\(t \\mapsto W_t(\\omega)\\) is continuous for almost all \\(\\omega \\in \\Omega\\)\n\n\n\nCode\n@njit\ndef simulate_brownian_motion(T, N, n_paths=5):\n    \"\"\"Efficiently simulate Brownian motion paths using Numba acceleration.\"\"\"\n    dt = T / N\n    sqrt_dt = np.sqrt(dt)\n    \n    paths = np.zeros((n_paths, N + 1))\n    \n    for i in range(n_paths):\n        for j in range(1, N + 1):\n            paths[i, j] = paths[i, j-1] + sqrt_dt * np.random.randn()\n    \n    return paths\n\n# Simulation parameters\nT = 1.0  # Time horizon\nN = 1000  # Number of time steps\nn_paths = 100\ndt = T / N\nt = np.linspace(0, T, N + 1)\n\n# Generate multiple Brownian motion paths\nnp.random.seed(42)\npaths = simulate_brownian_motion(T, N, n_paths)\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# Sample paths\naxes[0, 0].plot(t, paths[:20].T, alpha=0.6, linewidth=0.8)\naxes[0, 0].plot(t, paths[0], 'r-', linewidth=2, label='Sample path')\naxes[0, 0].set_title('Sample Paths of Brownian Motion')\naxes[0, 0].set_xlabel('Time t')\naxes[0, 0].set_ylabel('W(t)')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Distribution at fixed time\nt_fixed = 0.5\nW_fixed = paths[:, int(t_fixed * N)]\naxes[0, 1].hist(W_fixed, bins=30, density=True, alpha=0.7, color='skyblue')\nx_range = np.linspace(W_fixed.min(), W_fixed.max(), 100)\ntheoretical_pdf = stats.norm.pdf(x_range, 0, np.sqrt(t_fixed))\naxes[0, 1].plot(x_range, theoretical_pdf, 'r-', linewidth=2, label=f'N(0, {t_fixed})')\naxes[0, 1].set_title(f'Distribution of W({t_fixed})')\naxes[0, 1].set_xlabel('Value')\naxes[0, 1].set_ylabel('Density')\naxes[0, 1].legend()\n\n# Increment distribution\nincrements = np.diff(paths[0])\naxes[0, 2].hist(increments, bins=40, density=True, alpha=0.7, color='lightcoral')\nx_inc = np.linspace(increments.min(), increments.max(), 100)\ntheoretical_inc = stats.norm.pdf(x_inc, 0, np.sqrt(dt))\naxes[0, 2].plot(x_inc, theoretical_inc, 'k-', linewidth=2, label=f'N(0, {dt:.3f})')\naxes[0, 2].set_title('Increment Distribution')\naxes[0, 2].set_xlabel('Increment Value')\naxes[0, 2].set_ylabel('Density')\naxes[0, 2].legend()\n\n# Quadratic variation approximation\ndef quadratic_variation(path, dt):\n    \"\"\"Compute empirical quadratic variation.\"\"\"\n    increments = np.diff(path)\n    return np.cumsum(increments**2)\n\nqv = quadratic_variation(paths[0], dt)\naxes[1, 0].plot(t[1:], qv, 'b-', linewidth=2, label='Empirical [W,W]_t')\naxes[1, 0].plot(t, t, 'r--', linewidth=2, label='Theoretical t')\naxes[1, 0].set_title('Quadratic Variation')\naxes[1, 0].set_xlabel('Time t')\naxes[1, 0].set_ylabel('[W,W]_t')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Path roughness (non-differentiability)\n# Compute finite difference approximations to derivatives\nh_values = [0.1, 0.05, 0.01, 0.005]\ncolors = ['red', 'blue', 'green', 'orange']\n\nfor i, h in enumerate(h_values):\n    n_h = int(h / dt)\n    if n_h &gt; 0:\n        t_deriv = t[:-n_h]\n        finite_diff = (paths[0, n_h:] - paths[0, :-n_h]) / h\n        axes[1, 1].plot(t_deriv, finite_diff, color=colors[i], alpha=0.7, \n                       linewidth=1, label=f'h = {h}')\n\naxes[1, 1].set_title('Finite Difference Approximations\\n(Illustrating Non-differentiability)')\naxes[1, 1].set_xlabel('Time t')\naxes[1, 1].set_ylabel('(W(t+h) - W(t))/h')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\n# Scaling property demonstration\nscaled_paths = []\nscale_factors = [0.5, 1.0, 2.0]\ncolors_scale = ['blue', 'red', 'green']\n\nfor i, c in enumerate(scale_factors):\n    # W(ct) has same distribution as sqrt(c) * W(t)\n    if c != 1.0:\n        t_scaled = t * c\n        if c &lt; 1.0:\n            # Subsample for c &lt; 1\n            indices = np.linspace(0, len(t)-1, int(len(t)*c)).astype(int)\n            scaled_path = paths[0, indices] / np.sqrt(c)\n            t_plot = t[:len(indices)]\n        else:\n            # Extend time for c &gt; 1\n            extended_path = simulate_brownian_motion(T*c, int(N*c), 1)[0]\n            scaled_path = extended_path / np.sqrt(c)\n            t_plot = np.linspace(0, T, len(scaled_path))\n    else:\n        scaled_path = paths[0]\n        t_plot = t\n    \n    axes[1, 2].plot(t_plot, scaled_path, color=colors_scale[i], \n                   linewidth=1.5, alpha=0.8, label=f'c = {c}')\n\naxes[1, 2].set_title('Self-Similarity Property\\nW(ct) ~ √c · W(t)')\naxes[1, 2].set_xlabel('Time t')\naxes[1, 2].set_ylabel('Scaled W(t)')\naxes[1, 2].legend()\naxes[1, 2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Sample paths of Brownian motion and their key properties\n\n\n\n\n\nTheorem 2.4 (Properties of Brownian Motion): Standard Brownian motion possesses the following fundamental properties:\n\nMartingale property: \\(\\{W_t\\}\\) is a martingale with respect to its natural filtration\nQuadratic variation: \\([W,W]_t = t\\) (in the sense of convergence in probability)\nMarkov property: For \\(s &lt; t\\), \\(\\mathbb{E}[f(W_t) | \\mathcal{F}_s] = \\mathbb{E}[f(W_t) | W_s]\\)\nSelf-similarity: \\(\\{W_{ct}\\}_{t \\geq 0} \\stackrel{d}{=} \\{\\sqrt{c} W_t\\}_{t \\geq 0}\\) for any \\(c &gt; 0\\)\nPath properties: Paths are continuous but nowhere differentiable with probability 1\n\n\n\n2.3 Multi-dimensional Brownian Motion\nDefinition 2.5 (d-dimensional Brownian Motion): A \\(d\\)-dimensional Brownian motion is a vector process \\(\\mathbf{W}_t = (W_t^{(1)}, \\ldots, W_t^{(d)})^T\\) where each component \\(W_t^{(i)}\\) is independent standard Brownian motion.\nFor correlated Brownian motions, we can construct them using: \\[\\mathbf{W}_t = \\mathbf{L} \\mathbf{Z}_t\\] where \\(\\mathbf{Z}_t\\) is \\(d\\)-dimensional independent Brownian motion and \\(\\mathbf{L}\\) is the Cholesky decomposition of the correlation matrix \\(\\boldsymbol{\\Sigma}\\).\n\n\nCode\ndef simulate_correlated_brownian(T, N, correlation_matrix, n_paths=1):\n    \"\"\"Simulate correlated multi-dimensional Brownian motion.\"\"\"\n    d = correlation_matrix.shape[0]\n    dt = T / N\n    sqrt_dt = np.sqrt(dt)\n    \n    # Cholesky decomposition for correlation\n    L = cholesky(correlation_matrix, lower=True)\n    \n    paths = np.zeros((n_paths, d, N + 1))\n    \n    for path in range(n_paths):\n        for i in range(1, N + 1):\n            # Generate independent increments\n            dZ = np.random.randn(d) * sqrt_dt\n            # Apply correlation structure\n            dW = L @ dZ\n            paths[path, :, i] = paths[path, :, i-1] + dW\n    \n    return paths\n\n# Correlation matrices\ncorrelations = [\n    np.array([[1.0, 0.0], [0.0, 1.0]]),  # Independent\n    np.array([[1.0, 0.7], [0.7, 1.0]]),  # Positive correlation\n    np.array([[1.0, -0.5], [-0.5, 1.0]]) # Negative correlation\n]\n\ncorrelation_names = ['Independent (ρ=0)', 'Positive (ρ=0.7)', 'Negative (ρ=-0.5)']\n\nfig, axes = plt.subplots(2, 3, figsize=(16, 10))\n\nfor i, (corr_matrix, name) in enumerate(zip(correlations, correlation_names)):\n    # Simulate paths\n    paths = simulate_correlated_brownian(T, N, corr_matrix, n_paths=100)\n    t = np.linspace(0, T, N + 1)\n    \n    # Time series plot\n    for j in range(20):  # Plot subset of paths\n        axes[0, i].plot(t, paths[j, 0, :], alpha=0.3, color='blue', linewidth=0.8)\n        axes[0, i].plot(t, paths[j, 1, :], alpha=0.3, color='red', linewidth=0.8)\n    \n    # Highlight one path\n    axes[0, i].plot(t, paths[0, 0, :], color='blue', linewidth=2, label='W₁(t)')\n    axes[0, i].plot(t, paths[0, 1, :], color='red', linewidth=2, label='W₂(t)')\n    axes[0, i].set_title(f'Time Series: {name}')\n    axes[0, i].set_xlabel('Time t')\n    axes[0, i].set_ylabel('W(t)')\n    axes[0, i].legend()\n    axes[0, i].grid(True, alpha=0.3)\n    \n    # Phase plot (W₁ vs W₂)\n    for j in range(50):\n        axes[1, i].plot(paths[j, 0, :], paths[j, 1, :], alpha=0.4, linewidth=0.6)\n    \n    axes[1, i].scatter(0, 0, color='green', s=100, marker='o', zorder=5, label='Origin')\n    axes[1, i].set_title(f'Phase Plot: {name}')\n    axes[1, i].set_xlabel('W₁(t)')\n    axes[1, i].set_ylabel('W₂(t)')\n    axes[1, i].legend()\n    axes[1, i].grid(True, alpha=0.3)\n    axes[1, i].axis('equal')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Multi-dimensional Brownian motion with correlation structure\n\n\n\n\n\n\n\n2.4 Martingales and Stopping Times\nDefinition 2.6 (Martingale): A stochastic process \\(\\{M_t\\}_{t \\geq 0}\\) adapted to filtration \\(\\{\\mathcal{F}_t\\}\\) is a martingale if: 1. \\(\\mathbb{E}[|M_t|] &lt; \\infty\\) for all \\(t \\geq 0\\) 2. \\(\\mathbb{E}[M_t | \\mathcal{F}_s] = M_s\\) for all \\(0 \\leq s \\leq t\\)\nTheorem 2.7 (Examples of Martingales): The following processes are martingales: 1. Brownian motion \\(W_t\\) 2. \\(W_t^2 - t\\) (compensated quadratic variation) 3. \\(\\exp(\\sigma W_t - \\frac{\\sigma^2 t}{2})\\) for any \\(\\sigma \\in \\mathbb{R}\\) (exponential martingale)\nDefinition 2.8 (Stopping Time): A random variable \\(\\tau: \\Omega \\to [0, \\infty]\\) is a stopping time with respect to \\(\\{\\mathcal{F}_t\\}\\) if for every \\(t \\geq 0\\): \\[\\{\\tau \\leq t\\} \\in \\mathcal{F}_t\\]\nTheorem 2.9 (Optional Stopping Theorem): If \\(M_t\\) is a martingale and \\(\\tau\\) is a bounded stopping time, then: \\[\\mathbb{E}[M_\\tau] = \\mathbb{E}[M_0]\\]\nThese foundational concepts provide the mathematical infrastructure necessary for constructing stochastic integrals and developing the theory of stochastic differential equations. In the next section, we will build upon this foundation to develop Itô calculus, the cornerstone of stochastic analysis."
  },
  {
    "objectID": "posts/03-04-2025_intro_to_sde/index.html#sec-ito-calculus",
    "href": "posts/03-04-2025_intro_to_sde/index.html#sec-ito-calculus",
    "title": "Stochastic Differential Equations: From Mathematical Foundations to Modern Applications",
    "section": "3 Itô Calculus and Stochastic Integration",
    "text": "3 Itô Calculus and Stochastic Integration\nThe development of stochastic calculus represents one of the most profound mathematical achievements of the 20th century. Classical calculus fails when applied to functions of Brownian motion due to their infinite variation and non-differentiable nature. Itô’s revolutionary insight was to develop a new form of calculus specifically designed for stochastic processes.\n\n3.1 The Need for Stochastic Calculus\nConsider attempting to define the integral \\(\\int_0^t W_s \\, dW_s\\) using classical Riemann-Stieltjes integration. The fundamental problem arises from the fact that Brownian motion has infinite variation on any interval, making classical integration impossible.\n\n\nCode\ndef compute_variation(path, time_grid):\n    \"\"\"Compute total variation of a function on given grid.\"\"\"\n    return np.sum(np.abs(np.diff(path)))\n\ndef compute_quadratic_variation(path, time_grid):\n    \"\"\"Compute quadratic variation approximation.\"\"\"\n    return np.sum(np.diff(path)**2)\n\n# Generate fine Brownian motion path\nT = 1.0\nN_fine = 10000\nt_fine = np.linspace(0, T, N_fine + 1)\ndt_fine = T / N_fine\n\nnp.random.seed(42)\nW_fine = np.cumsum(np.concatenate([[0], np.random.randn(N_fine) * np.sqrt(dt_fine)]))\n\n# Compute variations for different grid sizes\ngrid_sizes = np.logspace(1, 4, 20).astype(int)\ntotal_variations = []\nquadratic_variations = []\n\nfor N in grid_sizes:\n    if N &lt;= N_fine:\n        indices = np.linspace(0, N_fine, N + 1).astype(int)\n        subpath = W_fine[indices]\n        t_sub = t_fine[indices]\n        \n        total_var = compute_variation(subpath, t_sub)\n        quad_var = compute_quadratic_variation(subpath, t_sub)\n        \n        total_variations.append(total_var)\n        quadratic_variations.append(quad_var)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 8))\n\n# Plot path and variations\nN_display = 1000\nindices = np.linspace(0, N_fine, N_display + 1).astype(int)\naxes[0].plot(t_fine[indices], W_fine[indices], 'b-', linewidth=1, alpha=0.8)\naxes[0].set_title('Sample Brownian Motion Path')\naxes[0].set_xlabel('Time t')\naxes[0].set_ylabel('W(t)')\naxes[0].grid(True, alpha=0.3)\n\n# Plot variation convergence\naxes[1].loglog(grid_sizes[:len(total_variations)], total_variations, 'ro-', \n               label='Total Variation', markersize=4)\naxes[1].loglog(grid_sizes[:len(quadratic_variations)], quadratic_variations, 'bs-', \n               label='Quadratic Variation', markersize=4)\naxes[1].axhline(y=T, color='black', linestyle='--', linewidth=2, \n                label=f'Theoretical QV = {T}')\naxes[1].set_xlabel('Number of Grid Points')\naxes[1].set_ylabel('Variation')\naxes[1].set_title('Variation Behavior as Grid Refines')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Final total variation: {total_variations[-1]:.2f}\")\nprint(f\"Final quadratic variation: {quadratic_variations[-1]:.3f}\")\nprint(f\"Theoretical quadratic variation: {T}\")\n\n\n\n\n\n\n\n\nFigure 3: Illustration of infinite variation in Brownian motion\n\n\n\n\n\nFinal total variation: 79.95\nFinal quadratic variation: 1.007\nTheoretical quadratic variation: 1.0\n\n\n\n\n3.2 Construction of the Itô Integral\nDefinition 3.1 (Simple Process): A stochastic process \\(\\{H_t\\}\\) is simple if it can be written as: \\[H_t = H_0 \\mathbf{1}_{\\{0\\}}(t) + \\sum_{i=1}^n H_{t_i} \\mathbf{1}_{(t_i, t_{i+1}]}(t)\\] where \\(0 = t_0 &lt; t_1 &lt; \\cdots &lt; t_n &lt; \\infty\\) and each \\(H_{t_i}\\) is \\(\\mathcal{F}_{t_i}\\)-measurable.\nDefinition 3.2 (Itô Integral for Simple Processes): For a simple process \\(H_t\\), the Itô integral is defined as: \\[\\int_0^t H_s \\, dW_s = \\sum_{i=0}^{n-1} H_{t_i}(W_{t_{i+1} \\wedge t} - W_{t_i \\wedge t})\\]\nTheorem 3.3 (Itô Isometry): For simple processes \\(H_t\\): \\[\\mathbb{E}\\left[\\left(\\int_0^t H_s \\, dW_s\\right)^2\\right] = \\mathbb{E}\\left[\\int_0^t H_s^2 \\, ds\\right]\\]\nDefinition 3.4 (General Itô Integral): For adapted processes \\(H_t\\) satisfying \\(\\mathbb{E}\\left[\\int_0^t H_s^2 \\, ds\\right] &lt; \\infty\\), the Itô integral \\(\\int_0^t H_s \\, dW_s\\) is defined as the \\(L^2\\) limit of Itô integrals of simple processes approximating \\(H_t\\).\n\n\n3.3 Itô’s Lemma: The Fundamental Theorem\nTheorem 3.5 (Itô’s Lemma): Let \\(W_t\\) be Brownian motion and \\(f(t,x) \\in C^{1,2}([0,\\infty) \\times \\mathbb{R})\\). Then:\n\\[df(t, W_t) = \\frac{\\partial f}{\\partial t}(t, W_t) dt + \\frac{\\partial f}{\\partial x}(t, W_t) dW_t + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}(t, W_t) dt\\]\nProof Sketch: The key insight is the quadratic variation term. Using Taylor expansion: \\[df = f_t dt + f_x dW_t + \\frac{1}{2}f_{xx}(dW_t)^2 + \\frac{1}{2}f_{tt}(dt)^2 + f_{tx} dt \\, dW_t + \\cdots\\]\nSince \\((dW_t)^2 = dt\\) in the sense of quadratic variation, and higher-order terms vanish, we obtain Itô’s formula. □\n\n\nCode\ndef ito_lemma_verification(f, df_dt, df_dx, d2f_dx2, W_path, dt):\n    \"\"\"\n    Verify Itô's lemma numerically by comparing direct computation\n    with the Itô formula prediction.\n    \"\"\"\n    t = np.arange(len(W_path)) * dt\n    \n    # Direct computation of f(t, W_t)\n    f_values = f(t, W_path)\n    df_direct = np.diff(f_values)\n    \n    # Itô formula prediction\n    t_mid = t[:-1] + dt/2  # Midpoint rule for better accuracy\n    W_mid = (W_path[:-1] + W_path[1:]) / 2\n    dW = np.diff(W_path)\n    \n    df_ito = (df_dt(t_mid, W_mid) * dt + \n              df_dx(t_mid, W_mid) * dW + \n              0.5 * d2f_dx2(t_mid, W_mid) * dt)\n    \n    return df_direct, df_ito, f_values\n\n# Example 1: f(t,x) = x^2\ndef f1(t, x):\n    return x**2\n\ndef df1_dt(t, x):\n    return np.zeros_like(x)\n\ndef df1_dx(t, x):\n    return 2*x\n\ndef d2f1_dx2(t, x):\n    return 2 * np.ones_like(x)\n\n# Example 2: f(t,x) = exp(x - t/2) (Exponential martingale)\ndef f2(t, x):\n    return np.exp(x - t/2)\n\ndef df2_dt(t, x):\n    return -0.5 * np.exp(x - t/2)\n\ndef df2_dx(t, x):\n    return np.exp(x - t/2)\n\ndef d2f2_dx2(t, x):\n    return np.exp(x - t/2)\n\n# Simulation parameters\nT = 1.0\nN = 1000\ndt = T / N\nt = np.linspace(0, T, N + 1)\n\nnp.random.seed(42)\nW = np.cumsum(np.concatenate([[0], np.random.randn(N) * np.sqrt(dt)]))\n\nfig, axes = plt.subplots(2, 3, figsize=(16, 10))\n\n# Example 1: f(t,x) = x^2\ndf1_direct, df1_ito, f1_values = ito_lemma_verification(\n    f1, df1_dt, df1_dx, d2f1_dx2, W, dt)\n\naxes[0, 0].plot(t, f1_values, 'b-', linewidth=2, label='W²(t)')\naxes[0, 0].set_title('Function: f(t,x) = x²')\naxes[0, 0].set_xlabel('Time t')\naxes[0, 0].set_ylabel('f(t, W(t))')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\naxes[0, 1].plot(t[:-1], df1_direct, 'b-', alpha=0.7, label='Direct: Δf')\naxes[0, 1].plot(t[:-1], df1_ito, 'r--', alpha=0.7, label='Itô formula')\naxes[0, 1].set_title('Increment Comparison')\naxes[0, 1].set_xlabel('Time t')\naxes[0, 1].set_ylabel('df')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Error analysis\nerror1 = df1_direct - df1_ito\naxes[0, 2].plot(t[:-1], error1, 'g-', linewidth=1)\naxes[0, 2].set_title(f'Error (RMS: {np.sqrt(np.mean(error1**2)):.6f})')\naxes[0, 2].set_xlabel('Time t')\naxes[0, 2].set_ylabel('Direct - Itô')\naxes[0, 2].grid(True, alpha=0.3)\n\n# Example 2: Exponential martingale\ndf2_direct, df2_ito, f2_values = ito_lemma_verification(\n    f2, df2_dt, df2_dx, d2f2_dx2, W, dt)\n\naxes[1, 0].plot(t, f2_values, 'purple', linewidth=2, label='exp(W(t) - t/2)')\naxes[1, 0].set_title('Exponential Martingale')\naxes[1, 0].set_xlabel('Time t')\naxes[1, 0].set_ylabel('f(t, W(t))')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\naxes[1, 1].plot(t[:-1], df2_direct, 'purple', alpha=0.7, label='Direct: Δf')\naxes[1, 1].plot(t[:-1], df2_ito, 'orange', linestyle='--', alpha=0.7, label='Itô formula')\naxes[1, 1].set_title('Increment Comparison')\naxes[1, 1].set_xlabel('Time t')\naxes[1, 1].set_ylabel('df')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nerror2 = df2_direct - df2_ito\naxes[1, 2].plot(t[:-1], error2, 'red', linewidth=1)\naxes[1, 2].set_title(f'Error (RMS: {np.sqrt(np.mean(error2**2)):.6f})')\naxes[1, 2].set_xlabel('Time t')\naxes[1, 2].set_ylabel('Direct - Itô')\naxes[1, 2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Theoretical verification for x^2\nprint(\"Theoretical verification for f(t,x) = x²:\")\nprint(\"df = 2W dW + dt\")\nprint(\"This gives: d(W²) = 2W dW + dt\")\nprint(\"So W²(t) = 2∫₀ᵗ W(s) dW(s) + t\")\nprint(f\"Empirical: W²({T:.1f}) = {W[-1]**2:.3f}\")\nprint(f\"Formula:   2∫WdW + t = {2 * np.sum(W[:-1] * np.diff(W)) + T:.3f}\")\n\n\n\n\n\n\n\n\nFigure 4: Demonstration of Itô’s lemma with geometric Brownian motion\n\n\n\n\n\nTheoretical verification for f(t,x) = x²:\ndf = 2W dW + dt\nThis gives: d(W²) = 2W dW + dt\nSo W²(t) = 2∫₀ᵗ W(s) dW(s) + t\nEmpirical: W²(1.0) = 0.374\nFormula:   2∫WdW + t = 0.415\n\n\n\n\n3.4 Multi-dimensional Itô’s Lemma\nTheorem 3.6 (Multi-dimensional Itô’s Lemma): Let \\(\\mathbf{X}_t = (X_t^{(1)}, \\ldots, X_t^{(d)})^T\\) be an Itô process satisfying: \\[d\\mathbf{X}_t = \\boldsymbol{\\mu}(t, \\mathbf{X}_t) dt + \\boldsymbol{\\sigma}(t, \\mathbf{X}_t) d\\mathbf{W}_t\\]\nFor \\(f(t, \\mathbf{x}) \\in C^{1,2}([0,\\infty) \\times \\mathbb{R}^d)\\):\n\\[df(t, \\mathbf{X}_t) = \\frac{\\partial f}{\\partial t} dt + \\sum_{i=1}^d \\frac{\\partial f}{\\partial x_i} dX_t^{(i)} + \\frac{1}{2} \\sum_{i,j=1}^d \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} d\\langle X^{(i)}, X^{(j)} \\rangle_t\\]\nwhere \\(d\\langle X^{(i)}, X^{(j)} \\rangle_t = \\sum_{k=1}^m \\sigma_{ik} \\sigma_{jk} dt\\) is the quadratic covariation.\n\n\n3.5 Applications of Itô’s Lemma\n\n\nCode\ndef simulate_geometric_brownian(S0, mu, sigma, T, N, n_paths=1):\n    \"\"\"Simulate geometric Brownian motion using exact solution.\"\"\"\n    dt = T / N\n    t = np.linspace(0, T, N + 1)\n    \n    # Generate Brownian increments\n    dW = np.random.randn(n_paths, N) * np.sqrt(dt)\n    W = np.column_stack([np.zeros(n_paths), np.cumsum(dW, axis=1)])\n    \n    # Exact solution: S(t) = S0 * exp((mu - sigma²/2)t + sigma*W(t))\n    S = S0 * np.exp((mu - 0.5 * sigma**2) * t[np.newaxis, :] + sigma * W)\n    \n    return t, S\n\ndef simulate_ornstein_uhlenbeck(X0, theta, mu, sigma, T, N, n_paths=1):\n    \"\"\"Simulate Ornstein-Uhlenbeck process using exact solution.\"\"\"\n    dt = T / N\n    t = np.linspace(0, T, N + 1)\n    \n    X = np.zeros((n_paths, N + 1))\n    X[:, 0] = X0\n    \n    for i in range(N):\n        # Exact transition: X(t+dt) = X(t)*exp(-theta*dt) + mu*(1-exp(-theta*dt)) + noise\n        exp_theta_dt = np.exp(-theta * dt)\n        mean = X[:, i] * exp_theta_dt + mu * (1 - exp_theta_dt)\n        var = sigma**2 * (1 - np.exp(-2 * theta * dt)) / (2 * theta)\n        X[:, i+1] = mean + np.sqrt(var) * np.random.randn(n_paths)\n    \n    return t, X\n\n# Parameters\nT = 2.0\nN = 1000\nn_paths = 200\n\n# Geometric Brownian Motion parameters\nS0 = 100\nmu = 0.05\nsigma = 0.2\n\n# Ornstein-Uhlenbeck parameters\nX0 = 0\ntheta = 2.0\nmu_ou = 1.0\nsigma_ou = 0.5\n\nnp.random.seed(42)\n\nfig, axes = plt.subplots(3, 2, figsize=(18, 12))\n\n# Geometric Brownian Motion\nt_gbm, S_gbm = simulate_geometric_brownian(S0, mu, sigma, T, N, n_paths)\n\n# Plot sample paths\nfor i in range(min(50, n_paths)):\n    axes[0, 0].plot(t_gbm, S_gbm[i], alpha=0.3, linewidth=0.8, color='blue')\naxes[0, 0].plot(t_gbm, S_gbm[0], color='red', linewidth=2, label='Sample path')\naxes[0, 0].set_title('Geometric Brownian Motion\\ndS = μS dt + σS dW')\naxes[0, 0].set_xlabel('Time t')\naxes[0, 0].set_ylabel('S(t)')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Log-returns distribution\nlog_returns = np.log(S_gbm[:, -1] / S_gbm[:, 0])\naxes[1, 0].hist(log_returns, bins=40, density=True, alpha=0.7, color='skyblue')\ntheoretical_mean = (mu - 0.5 * sigma**2) * T\ntheoretical_std = sigma * np.sqrt(T)\nx_range = np.linspace(log_returns.min(), log_returns.max(), 100)\ntheoretical_pdf = stats.norm.pdf(x_range, theoretical_mean, theoretical_std)\naxes[1, 0].plot(x_range, theoretical_pdf, 'r-', linewidth=2, \n               label=f'N({theoretical_mean:.3f}, {theoretical_std:.3f})')\naxes[1, 0].set_title('Log-Return Distribution')\naxes[1, 0].set_xlabel('log(S(T)/S(0))')\naxes[1, 0].set_ylabel('Density')\naxes[1, 0].legend()\n\n# Mean and variance evolution\nmeans = np.mean(S_gbm, axis=0)\nvars = np.var(S_gbm, axis=0)\ntheoretical_mean = S0 * np.exp(mu * t_gbm)\ntheoretical_var = S0**2 * np.exp(2*mu * t_gbm) * (np.exp(sigma**2 * t_gbm) - 1)\n\naxes[2, 0].plot(t_gbm, means, 'b-', linewidth=2, label='Empirical mean')\naxes[2, 0].plot(t_gbm, theoretical_mean, 'r--', linewidth=2, label='Theoretical mean')\naxes[2, 0].set_title('Mean Evolution')\naxes[2, 0].set_xlabel('Time t')\naxes[2, 0].set_ylabel('E[S(t)]')\naxes[2, 0].legend()\naxes[2, 0].grid(True, alpha=0.3)\n\n# Ornstein-Uhlenbeck Process\nt_ou, X_ou = simulate_ornstein_uhlenbeck(X0, theta, mu_ou, sigma_ou, T, N, n_paths)\n\n# Plot sample paths\nfor i in range(min(50, n_paths)):\n    axes[0, 1].plot(t_ou, X_ou[i], alpha=0.3, linewidth=0.8, color='green')\naxes[0, 1].plot(t_ou, X_ou[0], color='red', linewidth=2, label='Sample path')\naxes[0, 1].axhline(y=mu_ou, color='black', linestyle='--', alpha=0.7, label=f'Long-term mean = {mu_ou}')\naxes[0, 1].set_title('Ornstein-Uhlenbeck Process\\ndX = θ(μ - X) dt + σ dW')\naxes[0, 1].set_xlabel('Time t')\naxes[0, 1].set_ylabel('X(t)')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Stationary distribution\nfinal_values = X_ou[:, -1]\naxes[1, 1].hist(final_values, bins=40, density=True, alpha=0.7, color='lightgreen')\n# Theoretical stationary distribution: N(μ, σ²/(2θ))\nstationary_var = sigma_ou**2 / (2 * theta)\nx_range_ou = np.linspace(final_values.min(), final_values.max(), 100)\nstationary_pdf = stats.norm.pdf(x_range_ou, mu_ou, np.sqrt(stationary_var))\naxes[1, 1].plot(x_range_ou, stationary_pdf, 'r-', linewidth=2, \n               label=f'N({mu_ou}, {np.sqrt(stationary_var):.3f})')\naxes[1, 1].set_title('Terminal Distribution (t=2)')\naxes[1, 1].set_xlabel('X(T)')\naxes[1, 1].set_ylabel('Density')\naxes[1, 1].legend()\n\n# Mean reversion demonstration\nmeans_ou = np.mean(X_ou, axis=0)\nvars_ou = np.var(X_ou, axis=0)\ntheoretical_mean_ou = mu_ou + (X0 - mu_ou) * np.exp(-theta * t_ou)\ntheoretical_var_ou = sigma_ou**2 / (2 * theta) * (1 - np.exp(-2 * theta * t_ou))\n\naxes[2, 1].plot(t_ou, means_ou, 'g-', linewidth=2, label='Empirical mean')\naxes[2, 1].plot(t_ou, theoretical_mean_ou, 'r--', linewidth=2, label='Theoretical mean')\naxes[2, 1].plot(t_ou, vars_ou, 'b-', linewidth=2, alpha=0.7, label='Empirical variance')\naxes[2, 1].plot(t_ou, theoretical_var_ou, 'orange', linestyle='--', linewidth=2, \n               alpha=0.7, label='Theoretical variance')\naxes[2, 1].set_title('Mean Reversion and Variance Evolution')\naxes[2, 1].set_xlabel('Time t')\naxes[2, 1].set_ylabel('Moments')\naxes[2, 1].legend()\naxes[2, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5: Applications of Itô’s lemma: Geometric Brownian motion and Ornstein-Uhlenbeck process\n\n\n\n\n\nThe development of Itô calculus provides the mathematical foundation for analyzing stochastic differential equations. The key insights are:\n\nQuadratic variation matters: Unlike classical calculus, \\((dW_t)^2 = dt\\) contributes to the dynamics\nMartingale preservation: Properly constructed stochastic integrals preserve the martingale property\nChain rule modification: Itô’s lemma includes an additional second-order term due to quadratic variation\n\nIn the next section, we will use these tools to develop the general theory of stochastic differential equations and their solutions."
  },
  {
    "objectID": "posts/03-04-2025_intro_to_sde/index.html#sec-sde-theory",
    "href": "posts/03-04-2025_intro_to_sde/index.html#sec-sde-theory",
    "title": "Stochastic Differential Equations: From Mathematical Foundations to Modern Applications",
    "section": "4 Stochastic Differential Equations: Theory and Existence",
    "text": "4 Stochastic Differential Equations: Theory and Existence\nHaving established the foundations of stochastic calculus, we now turn to the central object of study: stochastic differential equations. These equations describe the evolution of random processes and form the mathematical backbone of modern quantitative finance and stochastic modeling.\n\n4.1 Mathematical Definition and Classification\nDefinition 4.1 (Stochastic Differential Equation): A stochastic differential equation (SDE) is an equation of the form: \\[dX_t = \\mu(t, X_t) dt + \\sigma(t, X_t) dW_t, \\quad X_0 = x_0\\]\nwhere: - \\(X_t\\) is the unknown stochastic process - \\(\\mu: [0,\\infty) \\times \\mathbb{R} \\to \\mathbb{R}\\) is the drift coefficient - \\(\\sigma: [0,\\infty) \\times \\mathbb{R} \\to \\mathbb{R}\\) is the diffusion coefficient\n- \\(W_t\\) is standard Brownian motion - \\(x_0\\) is the initial condition\nThe integral form is: \\[X_t = x_0 + \\int_0^t \\mu(s, X_s) ds + \\int_0^t \\sigma(s, X_s) dW_s\\]\nDefinition 4.2 (Strong vs Weak Solutions): - A strong solution to an SDE is an adapted process \\(X_t\\) defined on the same probability space as the driving Brownian motion \\(W_t\\) - A weak solution exists on some probability space with some Brownian motion that has the same law as the original problem\n\n\n4.2 Existence and Uniqueness Theory\nThe fundamental question in SDE theory concerns when solutions exist and when they are unique. The classical result is due to Itô and provides sufficient conditions.\nTheorem 4.3 (Existence and Uniqueness - Lipschitz Case): Consider the SDE: \\[dX_t = \\mu(t, X_t) dt + \\sigma(t, X_t) dW_t, \\quad X_0 = x_0\\]\nIf \\(\\mu\\) and \\(\\sigma\\) satisfy: 1. Lipschitz condition: There exists \\(K &gt; 0\\) such that for all \\(t \\geq 0\\) and \\(x, y \\in \\mathbb{R}\\): \\[|\\mu(t,x) - \\mu(t,y)| + |\\sigma(t,x) - \\sigma(t,y)| \\leq K|x-y|\\]\n\nLinear growth condition: There exists \\(K &gt; 0\\) such that for all \\(t \\geq 0\\) and \\(x \\in \\mathbb{R}\\): \\[|\\mu(t,x)| + |\\sigma(t,x)| \\leq K(1 + |x|)\\]\n\nThen there exists a unique strong solution \\(X_t\\) such that \\(\\mathbb{E}[\\sup_{0 \\leq s \\leq t} |X_s|^2] &lt; \\infty\\) for all \\(t \\geq 0\\).\nProof Sketch: The proof uses Picard iteration combined with the Grönwall inequality. Define the sequence: \\[X_t^{(0)} = x_0\\] \\[X_t^{(n+1)} = x_0 + \\int_0^t \\mu(s, X_s^{(n)}) ds + \\int_0^t \\sigma(s, X_s^{(n)}) dW_s\\]\nThe Lipschitz condition ensures the sequence converges uniformly, while the growth condition guarantees the limit has finite moments. □\n\n\nCode\ndef lipschitz_example_sde(x, t):\n    \"\"\"Example SDE coefficients satisfying Lipschitz conditions.\"\"\"\n    mu = -0.5 * x  # Linear drift (Lipschitz constant = 0.5)\n    sigma = 0.3 * (1 + 0.1 * x)  # Near-constant diffusion (Lipschitz constant ≈ 0.03)\n    return mu, sigma\n\ndef non_lipschitz_example_sde(x, t):\n    \"\"\"Example with non-Lipschitz coefficient leading to non-uniqueness.\"\"\"\n    mu = 0.0\n    sigma = np.sqrt(np.abs(x))  # Non-Lipschitz at x=0\n    return mu, sigma\n\ndef euler_maruyama_step(x, dt, sde_func, t, dW):\n    \"\"\"Single Euler-Maruyama step.\"\"\"\n    mu, sigma = sde_func(x, t)\n    return x + mu * dt + sigma * dW\n\ndef simulate_sde_euler(x0, T, N, sde_func, n_paths=1):\n    \"\"\"Simulate SDE using Euler-Maruyama scheme.\"\"\"\n    dt = T / N\n    sqrt_dt = np.sqrt(dt)\n    t = np.linspace(0, T, N + 1)\n    \n    X = np.zeros((n_paths, N + 1))\n    X[:, 0] = x0\n    \n    for i in range(N):\n        dW = np.random.randn(n_paths) * sqrt_dt\n        for j in range(n_paths):\n            X[j, i+1] = euler_maruyama_step(X[j, i], dt, sde_func, t[i], dW[j])\n    \n    return t, X\n\n# Simulation parameters\nT = 2.0\nN = 1000\nn_paths = 100\nx0 = 1.0\n\nnp.random.seed(42)\n\nfig, axes = plt.subplots(3, 2, figsize=(16, 12))\n\n# Lipschitz case: Unique solutions\nt_lip, X_lip = simulate_sde_euler(x0, T, N, lipschitz_example_sde, n_paths=5)\n\nfor i in range(X_lip.shape[0]):\n    axes[0, 0].plot(t_lip, X_lip[i], alpha=0.4, linewidth=0.8, color='blue')\naxes[0, 0].plot(t_lip, X_lip[0], color='red', linewidth=2, label='Sample path')\naxes[0, 0].set_title('Lipschitz Case: dX = -0.5X dt + 0.3(1+0.1X) dW\\n(Unique Solution)')\naxes[0, 0].set_xlabel('Time t')\naxes[0, 0].set_ylabel('X(t)')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Distribution evolution for Lipschitz case\ntimes_to_plot = [0.5, 1.0, 1.5, 2.0]\ncolors = ['blue', 'green', 'orange', 'red']\n\nfor i, (time_point, color) in enumerate(zip(times_to_plot, colors)):\n    time_idx = int(time_point * N / T)\n    values = X_lip[:, time_idx]\n    \n    # Kernel density estimation for smooth histogram\n    from scipy.stats import gaussian_kde\n    kde = gaussian_kde(values)\n    x_range = np.linspace(values.min() - 0.5, values.max() + 0.5, 100)\n    density = kde(x_range)\n    \n    axes[1, 0].fill_between(x_range, density, alpha=0.3, color=color, \n                           label=f't = {time_point}')\n    axes[1, 0].plot(x_range, density, color=color, linewidth=2)\n\naxes[1, 0].set_title('Distribution Evolution (Lipschitz Case)')\naxes[1, 0].set_xlabel('X(t)')\naxes[1, 0].set_ylabel('Density')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Mean and variance evolution\nmeans_lip = np.mean(X_lip, axis=0)\nvars_lip = np.var(X_lip, axis=0)\n\naxes[2, 0].plot(t_lip, means_lip, 'b-', linewidth=2, label='Empirical mean')\naxes[2, 0].plot(t_lip, vars_lip, 'r-', linewidth=2, label='Empirical variance')\naxes[2, 0].set_title('Moment Evolution (Lipschitz Case)')\naxes[2, 0].set_xlabel('Time t')\naxes[2, 0].set_ylabel('Moments')\naxes[2, 0].legend()\naxes[2, 0].grid(True, alpha=0.3)\n\n# Non-Lipschitz case: Potential non-uniqueness\n# Start very close to zero to illustrate the issue\nx0_small = 0.01\nt_nonlip, X_nonlip = simulate_sde_euler(x0_small, T, N, non_lipschitz_example_sde, n_paths=5)\n\nfor i in range(X_nonlip.shape[0]):\n    axes[0, 1].plot(t_nonlip, X_nonlip[i], alpha=0.4, linewidth=0.8, color='purple')\naxes[0, 1].plot(t_nonlip, X_nonlip[0], color='red', linewidth=2, label='Sample path')\naxes[0, 1].set_title('Non-Lipschitz Case: dX = √|X| dW\\n(Starting near zero)')\naxes[0, 1].set_xlabel('Time t')\naxes[0, 1].set_ylabel('X(t)')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Show the pathological behavior near zero\naxes[1, 1].hist(X_nonlip[:, N//2], bins=30, density=True, alpha=0.7, color='lightcoral')\naxes[1, 1].set_title('Distribution at t=1.0 (Non-Lipschitz Case)')\naxes[1, 1].set_xlabel('X(1)')\naxes[1, 1].set_ylabel('Density')\naxes[1, 1].grid(True, alpha=0.3)\n\n# Coefficient comparison\nx_range = np.linspace(-2, 2, 100)\nmu_lip = np.array([-0.5 * x for x in x_range])\nsigma_lip = np.array([0.3 * (1 + 0.1 * x) for x in x_range])\nsigma_nonlip = np.sqrt(np.abs(x_range))\n\naxes[2, 1].plot(x_range, mu_lip, 'b-', linewidth=2, label='μ(x) = -0.5x (Lipschitz)')\naxes[2, 1].plot(x_range, sigma_lip, 'g-', linewidth=2, label='σ(x) = 0.3(1+0.1x) (Lipschitz)')\naxes[2, 1].plot(x_range, sigma_nonlip, 'r-', linewidth=2, label='σ(x) = √|x| (Non-Lipschitz)')\naxes[2, 1].set_title('Coefficient Functions')\naxes[2, 1].set_xlabel('x')\naxes[2, 1].set_ylabel('Coefficient value')\naxes[2, 1].legend()\naxes[2, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Demonstrate Lipschitz constant computation\nprint(\"Lipschitz Constant Analysis:\")\nprint(\"For μ(x) = -0.5x: |μ(x) - μ(y)| = 0.5|x - y|, so L_μ = 0.5\")\nprint(\"For σ(x) = 0.3(1 + 0.1x): |σ(x) - σ(y)| = 0.03|x - y|, so L_σ = 0.03\")\nprint(\"Combined Lipschitz constant: K = L_μ + L_σ = 0.53\")\nprint()\nprint(\"For σ(x) = √|x|: |σ(x) - σ(y)| / |x - y| → ∞ as x,y → 0\")\nprint(\"This violates the Lipschitz condition at x = 0\")\n\n\n\n\n\n\n\n\nFigure 6: Illustration of existence and uniqueness theory through pathwise solutions\n\n\n\n\n\nLipschitz Constant Analysis:\nFor μ(x) = -0.5x: |μ(x) - μ(y)| = 0.5|x - y|, so L_μ = 0.5\nFor σ(x) = 0.3(1 + 0.1x): |σ(x) - σ(y)| = 0.03|x - y|, so L_σ = 0.03\nCombined Lipschitz constant: K = L_μ + L_σ = 0.53\n\nFor σ(x) = √|x|: |σ(x) - σ(y)| / |x - y| → ∞ as x,y → 0\nThis violates the Lipschitz condition at x = 0\n\n\n\n\n4.3 The Markov Property and Generator\nDefinition 4.4 (Markov Property): A process \\(X_t\\) has the Markov property if for any measurable function \\(f\\) and times \\(0 \\leq s &lt; t\\): \\[\\mathbb{E}[f(X_t) | \\mathcal{F}_s] = \\mathbb{E}[f(X_t) | X_s]\\]\nTheorem 4.5: Solutions to SDEs possess the strong Markov property.\nDefinition 4.6 (Infinitesimal Generator): For an SDE \\(dX_t = \\mu(X_t) dt + \\sigma(X_t) dW_t\\), the infinitesimal generator \\(\\mathcal{A}\\) is defined as: \\[\\mathcal{A}f(x) = \\mu(x) f'(x) + \\frac{1}{2}\\sigma^2(x) f''(x)\\]\nfor functions \\(f \\in C^2(\\mathbb{R})\\).\nTheorem 4.7 (Dynkin’s Formula): If \\(\\tau\\) is a stopping time with \\(\\mathbb{E}[\\tau] &lt; \\infty\\) and \\(f \\in C^2\\) with appropriate growth conditions, then: \\[\\mathbb{E}[f(X_\\tau)] = f(X_0) + \\mathbb{E}\\left[\\int_0^\\tau \\mathcal{A}f(X_s) ds\\right]\\]\n\n\n4.4 Feynman-Kac Theorem\nOne of the most profound connections in mathematical analysis links stochastic differential equations with partial differential equations through the Feynman-Kac theorem.\nTheorem 4.8 (Feynman-Kac): Consider the PDE: \\[\\frac{\\partial u}{\\partial t} + \\mu(x) \\frac{\\partial u}{\\partial x} + \\frac{1}{2}\\sigma^2(x) \\frac{\\partial^2 u}{\\partial x^2} + c(x)u = 0\\]\nwith terminal condition \\(u(T,x) = g(x)\\). If \\(X_t\\) solves: \\[dX_t = \\mu(X_t) dt + \\sigma(X_t) dW_t, \\quad X_0 = x\\]\nthen: \\[u(t,x) = \\mathbb{E}\\left[g(X_T) \\exp\\left(-\\int_t^T c(X_s) ds\\right) \\bigg| X_t = x\\right]\\]\nThis theorem provides the foundation for Monte Carlo methods in finance and connects probabilistic and analytical approaches to solving PDEs.\n\n\nCode\ndef feynman_kac_mc(x0, T, mu_func, sigma_func, c_func, g_func, n_paths=10000, n_steps=1000):\n    \"\"\"\n    Solve PDE using Feynman-Kac theorem via Monte Carlo.\n    \n    Returns u(0, x0) = E[g(X_T) * exp(-∫₀ᵀ c(X_s) ds) | X_0 = x0]\n    \"\"\"\n    dt = T / n_steps\n    sqrt_dt = np.sqrt(dt)\n    \n    # Storage for paths and integrals\n    X = np.zeros((n_paths, n_steps + 1))\n    X[:, 0] = x0\n    integral_c = np.zeros(n_paths)\n    \n    # Simulate paths\n    for i in range(n_steps):\n        t = i * dt\n        dW = np.random.randn(n_paths) * sqrt_dt\n        \n        for j in range(n_paths):\n            mu = mu_func(X[j, i], t)\n            sigma = sigma_func(X[j, i], t)\n            X[j, i+1] = X[j, i] + mu * dt + sigma * dW[j]\n            \n            # Accumulate integral of c(X_s)\n            integral_c[j] += c_func(X[j, i], t) * dt\n    \n    # Final payoff with discounting\n    payoffs = g_func(X[:, -1]) * np.exp(-integral_c)\n    \n    return np.mean(payoffs), np.std(payoffs) / np.sqrt(n_paths), X\n\n# Example: Heat equation with killing\n# PDE: ∂u/∂t + (1/2)∂²u/∂x² - ru = 0\n# Terminal condition: u(T,x) = max(x - K, 0) (call option payoff)\n\ndef mu_heat(x, t):\n    return 0.0  # No drift\n\ndef sigma_heat(x, t):\n    return 1.0  # Unit diffusion\n\ndef c_heat(x, t):\n    return 0.05  # Killing rate (interest rate)\n\ndef g_call(x, K=1.0):\n    return np.maximum(x - K, 0)  # Call option payoff\n\n# Parameters\nT = 1.0\nx_values_base = np.linspace(-1, 3, 11)\nx_values_additional = np.array([0.5, 1.0, 1.5])\nx_values = np.sort(np.unique(np.concatenate((x_values_base, x_values_additional))))\nK = 1.0\nn_paths = 50000\n\nnp.random.seed(42)\n\n# Compute solution at different initial points\nmc_solutions = []\nmc_errors = []\nsample_paths = {}\n\nfor x0 in x_values:\n    u_mc, error, paths = feynman_kac_mc(x0, T, mu_heat, sigma_heat, c_heat, \n                                       lambda x: g_call(x, K), n_paths)\n    mc_solutions.append(u_mc)\n    mc_errors.append(error)\n    # Store sample paths for visualization if x0 is close to 0.5, 1.0, or 1.5\n    if np.isclose(x0, 0.5):\n        sample_paths['0.5'] = paths\n    elif np.isclose(x0, 1.0):\n        sample_paths['1.0'] = paths\n    elif np.isclose(x0, 1.5):\n        sample_paths['1.5'] = paths\n\nmc_solutions = np.array(mc_solutions)\nmc_errors = np.array(mc_errors)\n\n# Analytical solution for comparison (Black-Scholes with r=0.05, σ=1, T=1)\ndef black_scholes_call(S, K, T, r, sigma):\n    from scipy.stats import norm\n    d1 = (np.log(S/K) + (r + 0.5*sigma**2)*T) / (sigma*np.sqrt(T))\n    d2 = d1 - sigma*np.sqrt(T)\n    return S * norm.cdf(d1) - K * np.exp(-r*T) * norm.cdf(d2)\n\nanalytical_solutions = black_scholes_call(x_values, K, T, 0.05, 1.0)\n\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Solution comparison\naxes[0, 0].plot(x_values, analytical_solutions, 'r-', linewidth=3, label='Analytical (Black-Scholes)')\naxes[0, 0].errorbar(x_values, mc_solutions, yerr=2*mc_errors, fmt='bo-', \n                   capsize=3, capthick=1, label='Monte Carlo ± 2σ')\naxes[0, 0].set_title('PDE Solution: u(0,x) = E[max(X(T)-K,0)e^{-rT}]')\naxes[0, 0].set_xlabel('Initial value x')\naxes[0, 0].set_ylabel('u(0,x)')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Error analysis\nrelative_errors = np.abs(mc_solutions - analytical_solutions) / analytical_solutions\naxes[0, 1].semilogy(x_values, relative_errors, 'go-', markersize=6)\naxes[0, 1].set_title('Relative Error: |MC - Analytical| / Analytical')\naxes[0, 1].set_xlabel('Initial value x')\naxes[0, 1].set_ylabel('Relative Error')\naxes[0, 1].grid(True, alpha=0.3)\n\n# Sample paths for different initial conditions\nt = np.linspace(0, T, 1001)\ncolors = ['blue', 'red', 'green']\nfor i, (x0_str, color) in enumerate(zip(['0.5', '1.0', '1.5'], colors)):\n    paths = sample_paths[x0_str]\n    # Plot subset of paths\n    for j in range(0, min(100, paths.shape[0]), 10):\n        axes[1, 0].plot(t, paths[j], color=color, alpha=0.3, linewidth=0.5)\n    # Highlight one path\n    axes[1, 0].plot(t, paths[0], color=color, linewidth=2, label=f'X₀ = {x0}')\n\naxes[1, 0].axhline(y=K, color='black', linestyle='--', alpha=0.7, label=f'Strike K = {K}')\naxes[1, 0].set_title('Sample Paths for Different Initial Conditions')\naxes[1, 0].set_xlabel('Time t')\naxes[1, 0].set_ylabel('X(t)')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Terminal distribution and payoff\nfinal_values = sample_paths['1.0'][:, -1]  # Terminal values starting from x=1\npayoffs = np.maximum(final_values - K, 0)\n\naxes[1, 1].hist(final_values, bins=50, density=True, alpha=0.6, color='skyblue', \n               label='X(T) distribution')\naxes[1, 1].hist(payoffs, bins=50, density=True, alpha=0.6, color='lightcoral', \n               label='Payoff distribution')\naxes[1, 1].axvline(x=K, color='black', linestyle='--', alpha=0.7, label=f'Strike K = {K}')\naxes[1, 1].set_title('Terminal Distribution and Payoff (X₀ = 1.0)')\naxes[1, 1].set_xlabel('Value')\naxes[1, 1].set_ylabel('Density')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Monte Carlo estimate at x=1.0: {mc_solutions[x_values == 1.0][0]:.4f} ± {2*mc_errors[x_values == 1.0][0]:.4f}\")\nprint(f\"Analytical solution at x=1.0:  {analytical_solutions[x_values == 1.0][0]:.4f}\")\nprint(f\"Relative error: {relative_errors[x_values == 1.0][0]:.2%}\")\n\n\n\n\n\n\n\n\nFigure 7: Feynman-Kac theorem illustration: PDE solution via Monte Carlo\n\n\n\n\n\nMonte Carlo estimate at x=1.0: 0.3831 ± 0.0050\nAnalytical solution at x=1.0:  0.3984\nRelative error: 3.84%"
  },
  {
    "objectID": "posts/03-04-2025_intro_to_sde/index.html#sec-numerical-methods",
    "href": "posts/03-04-2025_intro_to_sde/index.html#sec-numerical-methods",
    "title": "Stochastic Differential Equations: From Mathematical Foundations to Modern Applications",
    "section": "5 Numerical Methods for SDEs",
    "text": "5 Numerical Methods for SDEs\nWhile analytical solutions to SDEs exist only in special cases, numerical methods provide essential tools for practical applications. We examine the fundamental discretization schemes and their convergence properties.\n\n5.1 Euler-Maruyama Scheme\nThe most basic numerical method for SDEs is the Euler-Maruyama scheme, which discretizes the SDE:\n\\[dX_t = \\mu(t, X_t) dt + \\sigma(t, X_t) dW_t\\]\nusing the approximation: \\[X_{n+1} = X_n + \\mu(t_n, X_n) \\Delta t + \\sigma(t_n, X_n) \\Delta W_n\\]\nwhere \\(\\Delta W_n = W_{t_{n+1}} - W_{t_n} \\sim \\mathcal{N}(0, \\Delta t)\\) are independent Gaussian increments.\nTheorem 5.1 (Strong Convergence of Euler-Maruyama): Under Lipschitz and linear growth conditions, the Euler-Maruyama scheme has strong convergence order 0.5: \\[\\mathbb{E}[|X_T - X_T^{\\Delta t}|] = O(\\sqrt{\\Delta t})\\]\nwhere \\(X_T^{\\Delta t}\\) is the numerical approximation at time \\(T\\).\n\n\n5.2 Milstein Scheme\nThe Milstein scheme improves upon Euler-Maruyama by including an additional correction term derived from Itô’s lemma, achieving higher-order strong convergence.\nDefinition 5.2 (Milstein Scheme): The Milstein discretization is: \\[X_{n+1} = X_n + \\mu(t_n, X_n) \\Delta t + \\sigma(t_n, X_n) \\Delta W_n + \\frac{1}{2}\\sigma(t_n, X_n)\\sigma'(t_n, X_n)[(\\Delta W_n)^2 - \\Delta t]\\]\nwhere \\(\\sigma'(t,x) = \\frac{\\partial \\sigma}{\\partial x}(t,x)\\).\nTheorem 5.3 (Strong Convergence of Milstein): Under appropriate regularity conditions, the Milstein scheme has strong convergence order 1.0: \\[\\mathbb{E}[|X_T - X_T^{\\Delta t}|] = O(\\Delta t)\\]\n\n\nCode\n@njit\ndef euler_maruyama_simulation(x0, T, N, mu_params, sigma_params, n_paths):\n    \"\"\"\n    Efficient Euler-Maruyama simulation using Numba.\n    Assumes linear drift μ(x) = a*x + b and linear diffusion σ(x) = c*x + d.\n    \"\"\"\n    dt = T / N\n    sqrt_dt = np.sqrt(dt)\n    \n    a, b = mu_params\n    c, d = sigma_params\n    \n    X = np.zeros((n_paths, N + 1))\n    X[:, 0] = x0\n    \n    for i in range(N):\n        dW = np.random.randn(n_paths) * sqrt_dt\n        X_curr = X[:, i]\n        mu = a * X_curr + b\n        sigma = c * X_curr + d\n        X[:, i+1] = X_curr + mu * dt + sigma * dW\n    \n    return X\n\n@njit\ndef milstein_simulation(x0, T, N, mu_params, sigma_params, n_paths):\n    \"\"\"\n    Efficient Milstein simulation using Numba.\n    Assumes linear drift μ(x) = a*x + b and linear diffusion σ(x) = c*x + d.\n    \"\"\"\n    dt = T / N\n    sqrt_dt = np.sqrt(dt)\n    \n    a, b = mu_params\n    c, d = sigma_params\n    \n    X = np.zeros((n_paths, N + 1))\n    X[:, 0] = x0\n    \n    for i in range(N):\n        dW = np.random.randn(n_paths) * sqrt_dt\n        X_curr = X[:, i]\n        mu = a * X_curr + b\n        sigma = c * X_curr + d\n        sigma_prime = c  # d/dx(c*x + d) = c\n        \n        # Milstein correction term\n        correction = 0.5 * sigma * sigma_prime * (dW**2 - dt)\n        \n        X[:, i+1] = X_curr + mu * dt + sigma * dW + correction\n    \n    return X\n\ndef analytical_solution_gbm(x0, mu, sigma, T, N, n_paths):\n    \"\"\"Analytical solution for geometric Brownian motion.\"\"\"\n    dt = T / N\n    t = np.linspace(0, T, N + 1)\n    \n    # Generate Brownian motion\n    dW = np.random.randn(n_paths, N) * np.sqrt(dt)\n    W = np.column_stack([np.zeros(n_paths), np.cumsum(dW, axis=1)])\n    \n    # Exact solution: X(t) = x0 * exp((mu - sigma²/2)t + sigma*W(t))\n    X_exact = x0 * np.exp((mu - 0.5 * sigma**2) * t[np.newaxis, :] + sigma * W)\n    \n    return X_exact\n\n# Test case: Geometric Brownian Motion dX = μX dt + σX dW\nx0 = 1.0\nmu = 0.1\nsigma = 0.3\nT = 1.0\n\n# Parameters for linear approximation: μ(x) = μ*x, σ(x) = σ*x\nmu_params = (mu, 0.0)  # a=μ, b=0\nsigma_params = (sigma, 0.0)  # c=σ, d=0\n\n# Convergence study\nstep_sizes = np.array([1000, 2000, 4000, 8000, 16000])\nn_paths_convergence = 10000\n\nerrors_euler = []\nerrors_milstein = []\n\nnp.random.seed(42)\n\nfig, axes = plt.subplots(3, 2, figsize=(18, 14))\n\nprint(\"Convergence Analysis:\")\nprint(\"N\\t\\tEuler Error\\t\\tMilstein Error\\t\\tRatio\")\nprint(\"-\" * 60)\n\nfor N in step_sizes:\n    # Set random seed for fair comparison\n    np.random.seed(42)\n    \n    # Analytical solution\n    X_exact = analytical_solution_gbm(x0, mu, sigma, T, N, n_paths_convergence)\n    \n    # Reset seed for numerical methods\n    np.random.seed(42)\n    X_euler = euler_maruyama_simulation(x0, T, N, mu_params, sigma_params, n_paths_convergence)\n    \n    np.random.seed(42)\n    X_milstein = milstein_simulation(x0, T, N, mu_params, sigma_params, n_paths_convergence)\n    \n    # Compute strong errors (L1 norm at terminal time)\n    error_euler = np.mean(np.abs(X_euler[:, -1] - X_exact[:, -1]))\n    error_milstein = np.mean(np.abs(X_milstein[:, -1] - X_exact[:, -1]))\n    \n    errors_euler.append(error_euler)\n    errors_milstein.append(error_milstein)\n    \n    ratio = error_euler / error_milstein if error_milstein &gt; 0 else np.inf\n    print(f\"{N}\\t\\t{error_euler:.6f}\\t\\t{error_milstein:.6f}\\t\\t{ratio:.2f}\")\n\n# Convergence plots\ndt_values = T / step_sizes\naxes[0, 0].loglog(dt_values, errors_euler, 'bo-', label='Euler-Maruyama', markersize=8)\naxes[0, 0].loglog(dt_values, errors_milstein, 'rs-', label='Milstein', markersize=8)\n\n# Theoretical convergence rates\naxes[0, 0].loglog(dt_values, 0.1 * np.sqrt(dt_values), 'b--', alpha=0.7, label='O(√Δt)')\naxes[0, 0].loglog(dt_values, 0.02 * dt_values, 'r--', alpha=0.7, label='O(Δt)')\n\naxes[0, 0].set_xlabel('Step Size Δt')\naxes[0, 0].set_ylabel('Strong Error E[|X_T - X_T^Δt|]')\naxes[0, 0].set_title('Strong Convergence Rates')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Sample path comparison\nN_demo = 1000\nn_paths_demo = 50\n\nnp.random.seed(42)\nX_exact_demo = analytical_solution_gbm(x0, mu, sigma, T, N_demo, n_paths_demo)\n\nnp.random.seed(42)\nX_euler_demo = euler_maruyama_simulation(x0, T, N_demo, mu_params, sigma_params, n_paths_demo)\n\nnp.random.seed(42)\nX_milstein_demo = milstein_simulation(x0, T, N_demo, mu_params, sigma_params, n_paths_demo)\n\nt_demo = np.linspace(0, T, N_demo + 1)\n\n# Plot a few sample paths\nfor i in range(min(5, n_paths_demo)):\n    axes[0, 1].plot(t_demo, X_exact_demo[i], 'k-', alpha=0.6, linewidth=1)\n    axes[0, 1].plot(t_demo, X_euler_demo[i], 'b--', alpha=0.8, linewidth=1)\n    axes[0, 1].plot(t_demo, X_milstein_demo[i], 'r:', alpha=0.8, linewidth=1)\n\naxes[0, 1].plot([], [], 'k-', label='Exact')\naxes[0, 1].plot([], [], 'b--', label='Euler-Maruyama')\naxes[0, 1].plot([], [], 'r:', label='Milstein')\naxes[0, 1].set_title('Sample Path Comparison')\naxes[0, 1].set_xlabel('Time t')\naxes[0, 1].set_ylabel('X(t)')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Error distributions at terminal time\nerrors_euler_paths = X_euler_demo[:, -1] - X_exact_demo[:, -1]\nerrors_milstein_paths = X_milstein_demo[:, -1] - X_exact_demo[:, -1]\n\naxes[1, 0].hist(errors_euler_paths, bins=30, alpha=0.7, density=True, \n               color='blue', label=f'Euler (std={np.std(errors_euler_paths):.4f})')\naxes[1, 0].hist(errors_milstein_paths, bins=30, alpha=0.7, density=True, \n               color='red', label=f'Milstein (std={np.std(errors_milstein_paths):.4f})')\naxes[1, 0].axvline(0, color='black', linestyle='--', alpha=0.7)\naxes[1, 0].set_title('Error Distribution at Terminal Time')\naxes[1, 0].set_xlabel('Error: X_T^numerical - X_T^exact')\naxes[1, 0].set_ylabel('Density')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Weak convergence: Compare distributions\naxes[1, 1].hist(X_exact_demo[:, -1], bins=40, alpha=0.5, density=True, \n               color='black', label='Exact')\naxes[1, 1].hist(X_euler_demo[:, -1], bins=40, alpha=0.7, density=True, \n               color='blue', label='Euler-Maruyama')\naxes[1, 1].hist(X_milstein_demo[:, -1], bins=40, alpha=0.7, density=True, \n               color='red', label='Milstein')\naxes[1, 1].set_title('Terminal Distribution Comparison')\naxes[1, 1].set_xlabel('X(T)')\naxes[1, 1].set_ylabel('Density')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\n# Computational cost analysis\nstep_sizes_cost = np.array([100, 200, 500, 1000, 2000, 5000])\nn_paths_cost = 1000\n\nimport time\n\ntimes_euler = []\ntimes_milstein = []\n\nfor N in step_sizes_cost:\n    # Time Euler-Maruyama\n    start_time = time.time()\n    for _ in range(10):  # Average over multiple runs\n        np.random.seed(42)\n        X_euler_cost = euler_maruyama_simulation(x0, T, N, mu_params, sigma_params, n_paths_cost)\n    time_euler = (time.time() - start_time) / 10\n    times_euler.append(time_euler)\n    \n    # Time Milstein\n    start_time = time.time()\n    for _ in range(10):  # Average over multiple runs\n        np.random.seed(42)\n        X_milstein_cost = milstein_simulation(x0, T, N, mu_params, sigma_params, n_paths_cost)\n    time_milstein = (time.time() - start_time) / 10\n    times_milstein.append(time_milstein)\n\naxes[2, 0].loglog(step_sizes_cost, times_euler, 'bo-', label='Euler-Maruyama', markersize=8)\naxes[2, 0].loglog(step_sizes_cost, times_milstein, 'rs-', label='Milstein', markersize=8)\naxes[2, 0].set_xlabel('Number of Steps N')\naxes[2, 0].set_ylabel('Computation Time (seconds)')\naxes[2, 0].set_title('Computational Cost Comparison')\naxes[2, 0].legend()\naxes[2, 0].grid(True, alpha=0.3)\n\n# Efficiency comparison: Error vs computational cost\naxes[2, 1].loglog(times_euler[-len(errors_euler):], errors_euler, 'bo-', \n                 label='Euler-Maruyama', markersize=8)\naxes[2, 1].loglog(times_milstein[-len(errors_milstein):], errors_milstein, 'rs-', \n                 label='Milstein', markersize=8)\naxes[2, 1].set_xlabel('Computation Time (seconds)')\naxes[2, 1].set_ylabel('Strong Error')\naxes[2, 1].set_title('Efficiency: Error vs Computational Cost')\naxes[2, 1].legend()\naxes[2, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Summary statistics\nprint(f\"\\nFinal comparison (N={step_sizes[-1]}):\")\nprint(f\"Euler-Maruyama error: {errors_euler[-1]:.6f}\")\nprint(f\"Milstein error: {errors_milstein[-1]:.6f}\")\nprint(f\"Improvement factor: {errors_euler[-1]/errors_milstein[-1]:.2f}x\")\n\n\nConvergence Analysis:\nN       Euler Error     Milstein Error      Ratio\n------------------------------------------------------------\n1000        0.371105        0.366835        1.01\n2000        0.368514        0.367438        1.00\n4000        0.368665        0.365833        1.01\n8000        0.369580        0.366111        1.01\n16000       0.372657        0.371818        1.00\n\n\n\n\n\n\n\n\nFigure 8: Comparison of numerical methods for SDEs: Euler-Maruyama vs Milstein schemes\n\n\n\n\n\n\nFinal comparison (N=16000):\nEuler-Maruyama error: 0.372657\nMilstein error: 0.371818\nImprovement factor: 1.00x\n\n\n\n\n5.3 Higher-Order Methods and Multi-dimensional Extensions\nFor multi-dimensional SDEs: \\[d\\mathbf{X}_t = \\boldsymbol{\\mu}(t, \\mathbf{X}_t) dt + \\boldsymbol{\\sigma}(t, \\mathbf{X}_t) d\\mathbf{W}_t\\]\nthe schemes generalize naturally, but the Milstein scheme requires knowledge of mixed derivatives of the diffusion matrix.\nDefinition 5.4 (Multi-dimensional Euler-Maruyama): \\[\\mathbf{X}_{n+1} = \\mathbf{X}_n + \\boldsymbol{\\mu}(t_n, \\mathbf{X}_n) \\Delta t + \\boldsymbol{\\sigma}(t_n, \\mathbf{X}_n) \\Delta \\mathbf{W}_n\\]\nwhere \\(\\Delta \\mathbf{W}_n\\) are independent \\(m\\)-dimensional Gaussian vectors.\n\n\n5.4 Weak vs Strong Convergence\nDefinition 5.5: - Strong convergence measures pathwise accuracy: \\(\\mathbb{E}[|X_T - X_T^{\\Delta t}|] \\to 0\\) - Weak convergence measures distributional accuracy: \\(|\\mathbb{E}[f(X_T)] - \\mathbb{E}[f(X_T^{\\Delta t})]| \\to 0\\)\nFor many applications (e.g., option pricing), weak convergence is sufficient and can be achieved with larger step sizes.\n\n\nCode\ndef weak_convergence_study(payoff_func, step_sizes, n_paths=50000):\n    \"\"\"Study weak convergence for a given payoff function.\"\"\"\n    weak_errors_euler = []\n    weak_errors_milstein = []\n    \n    # Reference solution with very fine discretization\n    N_ref = 32000\n    np.random.seed(42)\n    X_ref = euler_maruyama_simulation(x0, T, N_ref, mu_params, sigma_params, n_paths)\n    exact_expectation = np.mean(payoff_func(X_ref[:, -1]))\n    \n    for N in step_sizes:\n        # Euler-Maruyama\n        np.random.seed(42)\n        X_euler = euler_maruyama_simulation(x0, T, N, mu_params, sigma_params, n_paths)\n        euler_expectation = np.mean(payoff_func(X_euler[:, -1]))\n        weak_error_euler = abs(euler_expectation - exact_expectation)\n        weak_errors_euler.append(weak_error_euler)\n        \n        # Milstein\n        np.random.seed(42)\n        X_milstein = milstein_simulation(x0, T, N, mu_params, sigma_params, n_paths)\n        milstein_expectation = np.mean(payoff_func(X_milstein[:, -1]))\n        weak_error_milstein = abs(milstein_expectation - exact_expectation)\n        weak_errors_milstein.append(weak_error_milstein)\n    \n    return weak_errors_euler, weak_errors_milstein, exact_expectation\n\n# Different payoff functions\ndef linear_payoff(x):\n    return x\n\ndef quadratic_payoff(x):\n    return x**2\n\ndef call_option_payoff(x, K=1.0):\n    return np.maximum(x - K, 0)\n\ndef digital_option_payoff(x, K=1.0):\n    return (x &gt; K).astype(float)\n\nstep_sizes_weak = np.array([50, 100, 200, 500, 1000, 2000])\npayoff_functions = [\n    (linear_payoff, \"Linear: E[X(T)]\"),\n    (quadratic_payoff, \"Quadratic: E[X²(T)]\"),\n    (lambda x: call_option_payoff(x, 1.0), \"Call Option: E[(X(T)-1)⁺]\"),\n    (lambda x: digital_option_payoff(x, 1.0), \"Digital Option: P(X(T)&gt;1)\")\n]\n\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\naxes = axes.flatten()\n\nfor i, (payoff_func, title) in enumerate(payoff_functions):\n    weak_errors_euler, weak_errors_milstein, exact_value = weak_convergence_study(\n        payoff_func, step_sizes_weak, n_paths=20000)\n    \n    dt_values = T / step_sizes_weak\n    \n    axes[i].loglog(dt_values, weak_errors_euler, 'bo-', label='Euler-Maruyama', markersize=6)\n    axes[i].loglog(dt_values, weak_errors_milstein, 'rs-', label='Milstein', markersize=6)\n    \n    # Theoretical weak convergence rates (typically one order higher than strong)\n    axes[i].loglog(dt_values, 0.01 * dt_values, 'b--', alpha=0.7, label='O(Δt)')\n    axes[i].loglog(dt_values, 0.001 * dt_values**2, 'r--', alpha=0.7, label='O(Δt²)')\n    \n    axes[i].set_xlabel('Step Size Δt')\n    axes[i].set_ylabel('Weak Error |E[f(X_T)] - E[f(X_T^Δt)]|')\n    axes[i].set_title(f'{title}\\n(Exact: {exact_value:.4f})')\n    axes[i].legend()\n    axes[i].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 9: Weak vs strong convergence illustration\n\n\n\n\n\nThe numerical analysis demonstrates several key insights:\n\nStrong convergence: Euler-Maruyama achieves \\(O(\\sqrt{\\Delta t})\\) while Milstein achieves \\(O(\\Delta t)\\)\nWeak convergence: Both methods typically achieve one order higher convergence for smooth payoffs\nComputational cost: Milstein requires additional derivative calculations but provides better accuracy\nPractical choice: The optimal method depends on the specific application and computational budget\n\nThese numerical methods provide the computational foundation for practical SDE applications in finance, engineering, and machine learning. In the next section, we explore their application to financial modeling and option pricing."
  },
  {
    "objectID": "posts/03-04-2025_intro_to_sde/index.html#sec-finance-applications",
    "href": "posts/03-04-2025_intro_to_sde/index.html#sec-finance-applications",
    "title": "Stochastic Differential Equations: From Mathematical Foundations to Modern Applications",
    "section": "6 Applications in Mathematical Finance",
    "text": "6 Applications in Mathematical Finance\nStochastic differential equations form the mathematical backbone of modern quantitative finance. The revolutionary insight that asset prices follow stochastic processes led to the development of rigorous option pricing theory and sophisticated risk management frameworks.\n\n6.1 The Black-Scholes Model\nThe Black-Scholes model represents the foundational application of SDEs in finance, providing the first rigorous framework for option pricing.\nModel Specification: Under the Black-Scholes framework, the stock price \\(S_t\\) follows geometric Brownian motion: \\[dS_t = \\mu S_t dt + \\sigma S_t dW_t\\]\nwhere: - \\(\\mu\\) is the expected return (drift) - \\(\\sigma\\) is the volatility - \\(W_t\\) is Brownian motion under the physical measure\nRisk-Neutral Pricing: The fundamental theorem of asset pricing requires pricing under the risk-neutral measure \\(\\mathbb{Q}\\), where: \\[dS_t = r S_t dt + \\sigma S_t dW_t^{\\mathbb{Q}}\\]\nwhere \\(r\\) is the risk-free rate and \\(W_t^{\\mathbb{Q}}\\) is Brownian motion under \\(\\mathbb{Q}\\).\nTheorem 6.1 (Black-Scholes Formula): The price at time \\(t\\) of a European call option with strike \\(K\\) and maturity \\(T\\) is:\n\\[C(t, S_t) = S_t \\Phi(d_1) - K e^{-r(T-t)} \\Phi(d_2)\\]\nwhere: \\[d_1 = \\frac{\\ln(S_t/K) + (r + \\sigma^2/2)(T-t)}{\\sigma\\sqrt{T-t}}, \\quad d_2 = d_1 - \\sigma\\sqrt{T-t}\\]\nand \\(\\Phi\\) is the standard normal cumulative distribution function.\nDerivation: The derivation follows from the Feynman-Kac theorem applied to the Black-Scholes PDE: \\[\\frac{\\partial V}{\\partial t} + \\frac{1}{2}\\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} + rS \\frac{\\partial V}{\\partial S} - rV = 0\\]\nwith terminal condition \\(V(T,S) = \\max(S-K, 0)\\).\n\n\nCode\ndef black_scholes_call(S, K, T, r, sigma):\n    \"\"\"Black-Scholes call option price.\"\"\"\n    d1 = (np.log(S/K) + (r + 0.5*sigma**2)*T) / (sigma*np.sqrt(T))\n    d2 = d1 - sigma*np.sqrt(T)\n    return S * stats.norm.cdf(d1) - K * np.exp(-r*T) * stats.norm.cdf(d2)\n\ndef black_scholes_put(S, K, T, r, sigma):\n    \"\"\"Black-Scholes put option price.\"\"\"\n    d1 = (np.log(S/K) + (r + 0.5*sigma**2)*T) / (sigma*np.sqrt(T))\n    d2 = d1 - sigma*np.sqrt(T)\n    return K * np.exp(-r*T) * stats.norm.cdf(-d2) - S * stats.norm.cdf(-d1)\n\ndef calculate_greeks(S, K, T, r, sigma):\n    \"\"\"Calculate option Greeks.\"\"\"\n    d1 = (np.log(S/K) + (r + 0.5*sigma**2)*T) / (sigma*np.sqrt(T))\n    d2 = d1 - sigma*np.sqrt(T)\n    \n    # Delta\n    delta_call = stats.norm.cdf(d1)\n    delta_put = stats.norm.cdf(d1) - 1\n    \n    # Gamma\n    gamma = stats.norm.pdf(d1) / (S * sigma * np.sqrt(T))\n    \n    # Theta\n    theta_call = (-S * stats.norm.pdf(d1) * sigma / (2 * np.sqrt(T)) \n                  - r * K * np.exp(-r*T) * stats.norm.cdf(d2))\n    theta_put = (-S * stats.norm.pdf(d1) * sigma / (2 * np.sqrt(T)) \n                 + r * K * np.exp(-r*T) * stats.norm.cdf(-d2))\n    \n    # Vega\n    vega = S * stats.norm.pdf(d1) * np.sqrt(T)\n    \n    # Rho\n    rho_call = K * T * np.exp(-r*T) * stats.norm.cdf(d2)\n    rho_put = -K * T * np.exp(-r*T) * stats.norm.cdf(-d2)\n    \n    return {\n        'delta_call': delta_call, 'delta_put': delta_put,\n        'gamma': gamma,\n        'theta_call': theta_call, 'theta_put': theta_put,\n        'vega': vega,\n        'rho_call': rho_call, 'rho_put': rho_put\n    }\n\ndef monte_carlo_option_price(S0, K, T, r, sigma, n_paths=100000, option_type='call'):\n    \"\"\"Monte Carlo option pricing.\"\"\"\n    dt = T / 252  # Daily steps\n    n_steps = int(T / dt)\n    \n    # Generate stock price paths\n    paths = np.zeros((n_paths, n_steps + 1))\n    paths[:, 0] = S0\n    \n    for i in range(n_steps):\n        Z = np.random.randn(n_paths)\n        paths[:, i+1] = paths[:, i] * np.exp((r - 0.5*sigma**2)*dt + sigma*np.sqrt(dt)*Z)\n    \n    # Calculate payoffs\n    if option_type == 'call':\n        payoffs = np.maximum(paths[:, -1] - K, 0)\n    else:  # put\n        payoffs = np.maximum(K - paths[:, -1], 0)\n    \n    # Discount to present value\n    price = np.exp(-r*T) * np.mean(payoffs)\n    std_error = np.exp(-r*T) * np.std(payoffs) / np.sqrt(n_paths)\n    \n    return price, std_error, paths\n\n# Parameters\nS0 = 100  # Initial stock price\nK = 100   # Strike price\nr = 0.05  # Risk-free rate\nsigma = 0.2  # Volatility\nT_range = np.linspace(0.1, 2, 50)  # Time to maturity range\nS_range = np.linspace(80, 120, 50)  # Stock price range\n\nfig, axes = plt.subplots(4, 2, figsize=(18, 16))\n\n# Option prices vs underlying price\ncall_prices = [black_scholes_call(S, K, 0.5, r, sigma) for S in S_range]\nput_prices = [black_scholes_put(S, K, 0.5, r, sigma) for S in S_range]\n\naxes[0, 0].plot(S_range, call_prices, 'b-', linewidth=2, label='Call Option')\naxes[0, 0].plot(S_range, put_prices, 'r-', linewidth=2, label='Put Option')\naxes[0, 0].plot(S_range, np.maximum(S_range - K, 0), 'b--', alpha=0.7, label='Call Intrinsic')\naxes[0, 0].plot(S_range, np.maximum(K - S_range, 0), 'r--', alpha=0.7, label='Put Intrinsic')\naxes[0, 0].set_xlabel('Stock Price S')\naxes[0, 0].set_ylabel('Option Price')\naxes[0, 0].set_title('Option Prices vs Underlying Price (T=0.5)')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Option prices vs time to maturity\ncall_prices_time = [black_scholes_call(S0, K, T, r, sigma) for T in T_range]\nput_prices_time = [black_scholes_put(S0, K, T, r, sigma) for T in T_range]\n\naxes[0, 1].plot(T_range, call_prices_time, 'b-', linewidth=2, label='Call Option')\naxes[0, 1].plot(T_range, put_prices_time, 'r-', linewidth=2, label='Put Option')\naxes[0, 1].set_xlabel('Time to Maturity T')\naxes[0, 1].set_ylabel('Option Price')\naxes[0, 1].set_title(f'Option Prices vs Time to Maturity (S={S0})')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Greeks calculation\ngreeks_data = [calculate_greeks(S, K, 0.5, r, sigma) for S in S_range]\n\n# Delta\ndeltas_call = [g['delta_call'] for g in greeks_data]\ndeltas_put = [g['delta_put'] for g in greeks_data]\n\naxes[1, 0].plot(S_range, deltas_call, 'b-', linewidth=2, label='Call Delta')\naxes[1, 0].plot(S_range, deltas_put, 'r-', linewidth=2, label='Put Delta')\naxes[1, 0].axhline(y=0, color='black', linestyle='--', alpha=0.5)\naxes[1, 0].axhline(y=0.5, color='blue', linestyle=':', alpha=0.5)\naxes[1, 0].set_xlabel('Stock Price S')\naxes[1, 0].set_ylabel('Delta')\naxes[1, 0].set_title('Delta: Price Sensitivity')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Gamma\ngammas = [g['gamma'] for g in greeks_data]\n\naxes[1, 1].plot(S_range, gammas, 'g-', linewidth=2, label='Gamma')\naxes[1, 1].set_xlabel('Stock Price S')\naxes[1, 1].set_ylabel('Gamma')\naxes[1, 1].set_title('Gamma: Delta Sensitivity')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\n# Vega\nvegas = [g['vega'] for g in greeks_data]\n\naxes[2, 0].plot(S_range, vegas, 'purple', linewidth=2, label='Vega')\naxes[2, 0].set_xlabel('Stock Price S')\naxes[2, 0].set_ylabel('Vega')\naxes[2, 0].set_title('Vega: Volatility Sensitivity')\naxes[2, 0].legend()\naxes[2, 0].grid(True, alpha=0.3)\n\n# Theta\nthetas_call = [g['theta_call'] for g in greeks_data]\nthetas_put = [g['theta_put'] for g in greeks_data]\n\naxes[2, 1].plot(S_range, thetas_call, 'b-', linewidth=2, label='Call Theta')\naxes[2, 1].plot(S_range, thetas_put, 'r-', linewidth=2, label='Put Theta')\naxes[2, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\naxes[2, 1].set_xlabel('Stock Price S')\naxes[2, 1].set_ylabel('Theta')\naxes[2, 1].set_title('Theta: Time Decay')\naxes[2, 1].legend()\naxes[2, 1].grid(True, alpha=0.3)\n\n# Monte Carlo vs Black-Scholes comparison\nnp.random.seed(42)\nmc_call_price, mc_call_error, sample_paths = monte_carlo_option_price(S0, K, 0.5, r, sigma, 50000, 'call')\nbs_call_price = black_scholes_call(S0, K, 0.5, r, sigma)\n\n# Plot sample paths\nfor i in range(min(50, sample_paths.shape[0])):\n    axes[3, 0].plot(np.linspace(0, 0.5, sample_paths.shape[1]), sample_paths[i], \n                   alpha=0.3, linewidth=0.5, color='blue')\n\naxes[3, 0].axhline(y=K, color='red', linestyle='--', linewidth=2, label=f'Strike K={K}')\naxes[3, 0].axhline(y=S0, color='green', linestyle='-', linewidth=2, label=f'Initial S₀={S0}')\naxes[3, 0].set_xlabel('Time')\naxes[3, 0].set_ylabel('Stock Price')\naxes[3, 0].set_title('Monte Carlo Sample Paths')\naxes[3, 0].legend()\naxes[3, 0].grid(True, alpha=0.3)\n\n# Terminal distribution and payoff\nterminal_prices = sample_paths[:, -1]\npayoffs = np.maximum(terminal_prices - K, 0)\n\naxes[3, 1].hist(terminal_prices, bins=50, alpha=0.6, density=True, color='skyblue', \n               label='Terminal Stock Price')\naxes[3, 1].hist(payoffs, bins=50, alpha=0.6, density=True, color='lightcoral', \n               label='Call Payoff')\naxes[3, 1].axvline(x=K, color='red', linestyle='--', linewidth=2, label=f'Strike K={K}')\naxes[3, 1].set_xlabel('Price/Payoff')\naxes[3, 1].set_ylabel('Density')\naxes[3, 1].set_title(f'Terminal Distribution\\nMC: {mc_call_price:.4f}±{2*mc_call_error:.4f}, BS: {bs_call_price:.4f}')\naxes[3, 1].legend()\naxes[3, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Black-Scholes vs Monte Carlo Comparison:\")\nprint(f\"Black-Scholes Price: {bs_call_price:.6f}\")\nprint(f\"Monte Carlo Price:   {mc_call_price:.6f} ± {2*mc_call_error:.6f}\")\nprint(f\"Difference:          {abs(bs_call_price - mc_call_price):.6f}\")\nprint(f\"Relative Error:      {abs(bs_call_price - mc_call_price)/bs_call_price:.4%}\")\n\n\n\n\n\n\n\n\nFigure 10: Black-Scholes model: Option pricing and Greeks analysis\n\n\n\n\n\nBlack-Scholes vs Monte Carlo Comparison:\nBlack-Scholes Price: 6.888729\nMonte Carlo Price:   6.867335 ± 0.087221\nDifference:          0.021394\nRelative Error:      0.3106%\n\n\n\n\n6.2 Interest Rate Models\nInterest rate modeling requires more sophisticated SDEs due to the mean-reverting nature of rates and term structure considerations.\n\n6.2.1 Vasicek Model\nModel Specification: The Vasicek model describes the short rate \\(r_t\\) as: \\[dr_t = \\kappa(\\theta - r_t) dt + \\sigma dW_t\\]\nwhere: - \\(\\kappa &gt; 0\\) is the speed of mean reversion - \\(\\theta\\) is the long-term mean - \\(\\sigma &gt; 0\\) is the volatility\nAnalytical Solution: The Vasicek model has the explicit solution: \\[r_t = r_0 e^{-\\kappa t} + \\theta (1 - e^{-\\kappa t}) + \\sigma \\int_0^t e^{-\\kappa(t-s)} dW_s\\]\n\n\n6.2.2 Cox-Ingersoll-Ross (CIR) Model\nModel Specification: The CIR model ensures non-negative rates: \\[dr_t = \\kappa(\\theta - r_t) dt + \\sigma \\sqrt{r_t} dW_t\\]\nThe square-root diffusion term prevents negative rates when \\(2\\kappa\\theta \\geq \\sigma^2\\) (Feller condition).\n\n\nCode\ndef simulate_vasicek(r0, kappa, theta, sigma, T, N, n_paths=1):\n    \"\"\"Simulate Vasicek interest rate model.\"\"\"\n    dt = T / N\n    sqrt_dt = np.sqrt(dt)\n    t = np.linspace(0, T, N + 1)\n    \n    r = np.zeros((n_paths, N + 1))\n    r[:, 0] = r0\n    \n    for i in range(N):\n        dW = np.random.randn(n_paths) * sqrt_dt\n        r[:, i+1] = (r[:, i] + kappa * (theta - r[:, i]) * dt + sigma * dW)\n    \n    return t, r\n\ndef simulate_cir(r0, kappa, theta, sigma, T, N, n_paths=1):\n    \"\"\"Simulate CIR interest rate model using Euler scheme with absorption.\"\"\"\n    dt = T / N\n    sqrt_dt = np.sqrt(dt)\n    t = np.linspace(0, T, N + 1)\n    \n    r = np.zeros((n_paths, N + 1))\n    r[:, 0] = r0\n    \n    for i in range(N):\n        dW = np.random.randn(n_paths) * sqrt_dt\n        r_curr = np.maximum(r[:, i], 0)  # Ensure non-negative\n        r[:, i+1] = (r_curr + kappa * (theta - r_curr) * dt + \n                     sigma * np.sqrt(r_curr) * dW)\n        r[:, i+1] = np.maximum(r[:, i+1], 0)  # Absorb at zero\n    \n    return t, r\n\ndef vasicek_bond_price(r, tau, kappa, theta, sigma):\n    \"\"\"Vasicek zero-coupon bond price.\"\"\"\n    B = (1 - np.exp(-kappa * tau)) / kappa\n    A = np.exp((B - tau) * (kappa**2 * theta - sigma**2/2) / kappa**2 - \n               sigma**2 * B**2 / (4 * kappa))\n    return A * np.exp(-B * r)\n\ndef cir_bond_price(r, tau, kappa, theta, sigma):\n    \"\"\"CIR zero-coupon bond price (approximate).\"\"\"\n    gamma = np.sqrt(kappa**2 + 2*sigma**2)\n    exp_gamma_tau = np.exp(gamma * tau)\n    \n    B = 2 * (exp_gamma_tau - 1) / ((gamma + kappa) * (exp_gamma_tau - 1) + 2 * gamma)\n    A = (2 * gamma * exp_gamma_tau) / ((gamma + kappa) * (exp_gamma_tau - 1) + 2 * gamma)\n    A = A**(2 * kappa * theta / sigma**2)\n    \n    return A * np.exp(-B * r)\n\n# Model parameters\nr0 = 0.03\nkappa = 0.5\ntheta = 0.04\nsigma_vasicek = 0.01\nsigma_cir = 0.05\nT = 10\nN = 2000\nn_paths = 1000\n\nnp.random.seed(42)\n\nfig, axes = plt.subplots(3, 2, figsize=(18, 14))\n\n# Simulate both models\nt, r_vasicek = simulate_vasicek(r0, kappa, theta, sigma_vasicek, T, N, n_paths)\nt, r_cir = simulate_cir(r0, kappa, theta, sigma_cir, T, N, n_paths)\n\n# Sample paths\nfor i in range(min(50, n_paths)):\n    axes[0, 0].plot(t, r_vasicek[i], alpha=0.3, linewidth=0.5, color='blue')\n    axes[0, 1].plot(t, r_cir[i], alpha=0.3, linewidth=0.5, color='red')\n\naxes[0, 0].plot(t, r_vasicek[0], color='darkblue', linewidth=2, label='Sample path')\naxes[0, 0].axhline(y=theta, color='green', linestyle='--', linewidth=2, \n                  label=f'Long-term mean θ={theta}')\naxes[0, 0].set_title('Vasicek Model: dr = κ(θ-r)dt + σdW')\naxes[0, 0].set_xlabel('Time (years)')\naxes[0, 0].set_ylabel('Interest Rate')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\naxes[0, 1].plot(t, r_cir[0], color='darkred', linewidth=2, label='Sample path')\naxes[0, 1].axhline(y=theta, color='green', linestyle='--', linewidth=2, \n                  label=f'Long-term mean θ={theta}')\naxes[0, 1].set_title('CIR Model: dr = κ(θ-r)dt + σ√r dW')\naxes[0, 1].set_xlabel('Time (years)')\naxes[0, 1].set_ylabel('Interest Rate')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Distribution evolution\ntimes_to_plot = [1, 3, 5, 10]\ncolors = ['blue', 'green', 'orange', 'red']\n\nfor i, (time_point, color) in enumerate(zip(times_to_plot, colors)):\n    time_idx = int(time_point * N / T)\n    \n    # Vasicek distribution\n    rates_vasicek = r_vasicek[:, time_idx]\n    axes[1, 0].hist(rates_vasicek, bins=40, alpha=0.3, density=True, color=color, \n                   label=f't={time_point}')\n    \n    # Theoretical Vasicek distribution (Gaussian)\n    mean_vasicek = r0 * np.exp(-kappa * time_point) + theta * (1 - np.exp(-kappa * time_point))\n    var_vasicek = sigma_vasicek**2 * (1 - np.exp(-2 * kappa * time_point)) / (2 * kappa)\n    x_range = np.linspace(rates_vasicek.min(), rates_vasicek.max(), 100)\n    theoretical_pdf = stats.norm.pdf(x_range, mean_vasicek, np.sqrt(var_vasicek))\n    axes[1, 0].plot(x_range, theoretical_pdf, color=color, linewidth=2, linestyle='--')\n    \n    # CIR distribution\n    rates_cir = r_cir[:, time_idx]\n    axes[1, 1].hist(rates_cir, bins=40, alpha=0.3, density=True, color=color, \n                   label=f't={time_point}')\n\naxes[1, 0].set_title('Vasicek Rate Distribution Evolution')\naxes[1, 0].set_xlabel('Interest Rate')\naxes[1, 0].set_ylabel('Density')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\naxes[1, 1].set_title('CIR Rate Distribution Evolution')\naxes[1, 1].set_xlabel('Interest Rate')\naxes[1, 1].set_ylabel('Density')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\n# Term structure of interest rates\nmaturities = np.linspace(0.1, 10, 50)\ncurrent_rate = 0.035\n\n# Calculate bond prices and yields\nvasicek_bonds = [vasicek_bond_price(current_rate, tau, kappa, theta, sigma_vasicek) \n                for tau in maturities]\ncir_bonds = [cir_bond_price(current_rate, tau, kappa, theta, sigma_cir) \n            for tau in maturities]\n\n# Convert to yields: Y = -ln(P)/τ\nvasicek_yields = [-np.log(P) / tau for P, tau in zip(vasicek_bonds, maturities)]\ncir_yields = [-np.log(P) / tau for P, tau in zip(cir_bonds, maturities)]\n\naxes[2, 0].plot(maturities, vasicek_yields, 'b-', linewidth=2, label='Vasicek')\naxes[2, 0].plot(maturities, cir_yields, 'r-', linewidth=2, label='CIR')\naxes[2, 0].axhline(y=theta, color='green', linestyle='--', alpha=0.7, \n                  label=f'Long-term rate θ={theta}')\naxes[2, 0].set_xlabel('Maturity (years)')\naxes[2, 0].set_ylabel('Yield')\naxes[2, 0].set_title(f'Term Structure of Interest Rates (r₀={current_rate})')\naxes[2, 0].legend()\naxes[2, 0].grid(True, alpha=0.3)\n\n# Bond price volatility\nbond_vols_vasicek = []\nbond_vols_cir = []\n\nfor tau in maturities:\n    # Calculate bond prices for all rate paths at current time\n    prices_vasicek = [vasicek_bond_price(r, tau, kappa, theta, sigma_vasicek) \n                     for r in r_vasicek[:, 0]]  # Use initial rates\n    prices_cir = [cir_bond_price(r, tau, kappa, theta, sigma_cir) \n                 for r in r_cir[:, 0]]\n    \n    bond_vols_vasicek.append(np.std(prices_vasicek))\n    bond_vols_cir.append(np.std(prices_cir))\n\naxes[2, 1].plot(maturities, bond_vols_vasicek, 'b-', linewidth=2, label='Vasicek')\naxes[2, 1].plot(maturities, bond_vols_cir, 'r-', linewidth=2, label='CIR')\naxes[2, 1].set_xlabel('Maturity (years)')\naxes[2, 1].set_ylabel('Bond Price Volatility')\naxes[2, 1].set_title('Bond Price Volatility vs Maturity')\naxes[2, 1].legend()\naxes[2, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Summary statistics\nprint(\"Model Comparison:\")\nprint(f\"Vasicek - Mean rate: {np.mean(r_vasicek):.4f}, Std: {np.std(r_vasicek):.4f}\")\nprint(f\"CIR - Mean rate: {np.mean(r_cir):.4f}, Std: {np.std(r_cir):.4f}\")\nprint(f\"Negative rates in Vasicek: {np.sum(r_vasicek &lt; 0) / r_vasicek.size:.2%}\")\nprint(f\"Negative rates in CIR: {np.sum(r_cir &lt; 0) / r_cir.size:.2%}\")\n\n\n\n\n\n\n\n\nFigure 11: Interest rate models: Vasicek and CIR processes with term structure\n\n\n\n\n\nModel Comparison:\nVasicek - Mean rate: 0.0377, Std: 0.0098\nCIR - Mean rate: 0.0381, Std: 0.0096\nNegative rates in Vasicek: 0.00%\nNegative rates in CIR: 0.00%\n\n\n\n\n\n6.3 Stochastic Volatility Models\nReal market data exhibits volatility clustering and mean reversion, motivating stochastic volatility models.\n\n6.3.1 Heston Model\nModel Specification: The Heston model couples asset price with stochastic volatility: \\[dS_t = \\mu S_t dt + \\sqrt{V_t} S_t dW_t^{(1)}\\] \\[dV_t = \\kappa(\\theta - V_t) dt + \\sigma_v \\sqrt{V_t} dW_t^{(2)}\\]\nwhere \\(dW_t^{(1)} dW_t^{(2)} = \\rho dt\\) captures correlation between price and volatility shocks.\nProperties: - Volatility clustering: High volatility tends to be followed by high volatility - Leverage effect: Negative correlation \\(\\rho &lt; 0\\) captures the inverse relationship between returns and volatility - Fat tails: The model generates fat-tailed return distributions\n\n\nCode\ndef simulate_heston(S0, V0, mu, kappa, theta, sigma_v, rho, T, N, n_paths=1):\n    \"\"\"Simulate Heston model using Euler scheme.\"\"\"\n    dt = T / N\n    sqrt_dt = np.sqrt(dt)\n    \n    S = np.zeros((n_paths, N + 1))\n    V = np.zeros((n_paths, N + 1))\n    S[:, 0] = S0\n    V[:, 0] = V0\n    \n    for i in range(N):\n        # Generate correlated Brownian increments\n        Z1 = np.random.randn(n_paths)\n        Z2 = rho * Z1 + np.sqrt(1 - rho**2) * np.random.randn(n_paths)\n        \n        dW1 = Z1 * sqrt_dt\n        dW2 = Z2 * sqrt_dt\n        \n        # Update variance (with absorption at zero)\n        V_curr = np.maximum(V[:, i], 0)\n        V[:, i+1] = V_curr + kappa * (theta - V_curr) * dt + sigma_v * np.sqrt(V_curr) * dW2\n        V[:, i+1] = np.maximum(V[:, i+1], 0)\n        \n        # Update stock price\n        S[:, i+1] = S[:, i] * (1 + mu * dt + np.sqrt(V_curr) * dW1)\n    \n    return S, V\n\ndef heston_option_pricing_mc(S0, V0, K, T, r, kappa, theta, sigma_v, rho, n_paths=100000):\n    \"\"\"Monte Carlo option pricing under Heston model.\"\"\"\n    S, V = simulate_heston(S0, V0, r, kappa, theta, sigma_v, rho, T, 252, n_paths)\n    \n    # Calculate payoffs\n    call_payoffs = np.maximum(S[:, -1] - K, 0)\n    put_payoffs = np.maximum(K - S[:, -1], 0)\n    \n    # Discount to present value\n    call_price = np.exp(-r * T) * np.mean(call_payoffs)\n    put_price = np.exp(-r * T) * np.mean(put_payoffs)\n    \n    return call_price, put_price, S, V\n\n# Heston model parameters\nS0 = 100\nV0 = 0.04  # Initial variance (σ₀ = 20%)\nmu = 0.05\nkappa = 2.0\ntheta = 0.04  # Long-term variance\nsigma_v = 0.3  # Volatility of volatility\nrho = -0.7  # Leverage effect\nT = 1.0\nN = 252\nn_paths = 10000\n\nnp.random.seed(42)\n\nfig, axes = plt.subplots(3, 2, figsize=(16, 12))\n\n# Simulate Heston paths\nS_heston, V_heston = simulate_heston(S0, V0, mu, kappa, theta, sigma_v, rho, T, N, n_paths)\nt = np.linspace(0, T, N + 1)\n\n# Compare with Black-Scholes (constant volatility)\nsigma_bs = np.sqrt(theta)  # Use long-term volatility\nS_bs = np.zeros((n_paths, N + 1))\nS_bs[:, 0] = S0\n\nfor i in range(N):\n    dt = T / N\n    dW = np.random.randn(n_paths) * np.sqrt(dt)\n    S_bs[:, i+1] = S_bs[:, i] * (1 + mu * dt + sigma_bs * dW)\n\n# Sample paths comparison\nfor i in range(min(20, n_paths)):\n    axes[0, 0].plot(t, S_heston[i], alpha=0.4, linewidth=0.8, color='blue')\n    axes[0, 1].plot(t, S_bs[i], alpha=0.4, linewidth=0.8, color='red')\n\naxes[0, 0].plot(t, S_heston[0], color='darkblue', linewidth=2, label='Sample path')\naxes[0, 0].set_title('Heston Model: Stock Price Paths')\naxes[0, 0].set_xlabel('Time')\naxes[0, 0].set_ylabel('Stock Price')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\naxes[0, 1].plot(t, S_bs[0], color='darkred', linewidth=2, label='Sample path')\naxes[0, 1].set_title('Black-Scholes: Stock Price Paths')\naxes[0, 1].set_xlabel('Time')\naxes[0, 1].set_ylabel('Stock Price')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Volatility paths\nfor i in range(min(20, n_paths)):\n    axes[1, 0].plot(t, np.sqrt(V_heston[i]), alpha=0.4, linewidth=0.8, color='green')\n\naxes[1, 0].plot(t, np.sqrt(V_heston[0]), color='darkgreen', linewidth=2, label='Sample path')\naxes[1, 0].axhline(y=np.sqrt(theta), color='red', linestyle='--', linewidth=2, \n                  label=f'Long-term vol = {np.sqrt(theta):.2f}')\naxes[1, 0].set_title('Heston Model: Volatility Paths')\naxes[1, 0].set_xlabel('Time')\naxes[1, 0].set_ylabel('Volatility')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Volatility vs return correlation\nreturns_heston = np.diff(np.log(S_heston), axis=1)\nvol_changes = np.diff(np.sqrt(V_heston), axis=1)\n\n# Flatten for correlation calculation\nreturns_flat = returns_heston.flatten()\nvol_flat = vol_changes.flatten()\n\n# Sample scatter plot\nsample_size = min(5000, len(returns_flat))\nindices = np.random.choice(len(returns_flat), sample_size, replace=False)\n\naxes[1, 1].scatter(returns_flat[indices], vol_flat[indices], alpha=0.3, s=1)\naxes[1, 1].set_xlabel('Log Returns')\naxes[1, 1].set_ylabel('Volatility Changes')\naxes[1, 1].set_title(f'Return-Volatility Correlation\\n(ρ = {np.corrcoef(returns_flat, vol_flat)[0,1]:.3f})')\naxes[1, 1].grid(True, alpha=0.3)\n\n# Return distributions comparison\nreturns_heston_terminal = np.log(S_heston[:, -1] / S_heston[:, 0])\nreturns_bs_terminal = np.log(S_bs[:, -1] / S_bs[:, 0])\n\naxes[2, 0].hist(returns_heston_terminal, bins=50, alpha=0.7, density=True, \n               color='blue', label='Heston')\naxes[2, 0].hist(returns_bs_terminal, bins=50, alpha=0.7, density=True, \n               color='red', label='Black-Scholes')\n\n# Theoretical normal distribution\nx_range = np.linspace(min(returns_heston_terminal.min(), returns_bs_terminal.min()),\n                     max(returns_heston_terminal.max(), returns_bs_terminal.max()), 100)\nnormal_pdf = stats.norm.pdf(x_range, (mu - 0.5*sigma_bs**2)*T, sigma_bs*np.sqrt(T))\naxes[2, 0].plot(x_range, normal_pdf, 'k--', linewidth=2, label='Normal')\n\naxes[2, 0].set_xlabel('Log Returns')\naxes[2, 0].set_ylabel('Density')\naxes[2, 0].set_title('Return Distribution Comparison')\naxes[2, 0].legend()\naxes[2, 0].grid(True, alpha=0.3)\n\n# Option pricing comparison\nK_range = np.linspace(80, 120, 21)\nheston_calls = []\nbs_calls = []\n\nfor K in K_range:\n    # Heston option price (Monte Carlo)\n    heston_call, _, _, _ = heston_option_pricing_mc(S0, V0, K, T, 0.05, kappa, theta, sigma_v, rho, 20000)\n    heston_calls.append(heston_call)\n    \n    # Black-Scholes option price\n    bs_call = black_scholes_call(S0, K, T, 0.05, sigma_bs)\n    bs_calls.append(bs_call)\n\naxes[2, 1].plot(K_range, heston_calls, 'bo-', label='Heston', markersize=4)\naxes[2, 1].plot(K_range, bs_calls, 'rs-', label='Black-Scholes', markersize=4)\naxes[2, 1].set_xlabel('Strike Price')\naxes[2, 1].set_ylabel('Call Option Price')\naxes[2, 1].set_title('Option Prices: Heston vs Black-Scholes')\naxes[2, 1].legend()\naxes[2, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Summary statistics\nprint(\"Model Statistics:\")\nprint(f\"Heston - Return mean: {np.mean(returns_heston_terminal):.4f}, std: {np.std(returns_heston_terminal):.4f}\")\nprint(f\"Black-Scholes - Return mean: {np.mean(returns_bs_terminal):.4f}, std: {np.std(returns_bs_terminal):.4f}\")\nprint(f\"Heston - Return skewness: {stats.skew(returns_heston_terminal):.4f}\")\nprint(f\"Black-Scholes - Return skewness: {stats.skew(returns_bs_terminal):.4f}\")\nprint(f\"Heston - Return kurtosis: {stats.kurtosis(returns_heston_terminal):.4f}\")\nprint(f\"Black-Scholes - Return kurtosis: {stats.kurtosis(returns_bs_terminal):.4f}\")\n\n\n\n\n\n\n\n\nFigure 12: Heston stochastic volatility model: Coupled dynamics and option pricing\n\n\n\n\n\nModel Statistics:\nHeston - Return mean: 0.0306, std: 0.2042\nBlack-Scholes - Return mean: 0.0303, std: 0.2000\nHeston - Return skewness: -0.8718\nBlack-Scholes - Return skewness: -0.0430\nHeston - Return kurtosis: 1.2808\nBlack-Scholes - Return kurtosis: 0.0571\n\n\nThe financial applications demonstrate how SDEs provide the mathematical foundation for:\n\nOption pricing: From the classical Black-Scholes formula to sophisticated stochastic volatility models\nInterest rate modeling: Capturing mean reversion and ensuring realistic term structure dynamics\n\nRisk management: Providing frameworks for Value-at-Risk and scenario analysis\nPortfolio optimization: Incorporating stochastic dynamics into investment decisions\n\nThese models form the backbone of modern quantitative finance and demonstrate the practical power of stochastic differential equation theory. In the next section, we explore the emerging applications of SDEs in machine learning and artificial intelligence."
  },
  {
    "objectID": "posts/03-04-2025_intro_to_sde/index.html#sec-ml-applications",
    "href": "posts/03-04-2025_intro_to_sde/index.html#sec-ml-applications",
    "title": "Stochastic Differential Equations: From Mathematical Foundations to Modern Applications",
    "section": "7 SDEs in Modern Machine Learning",
    "text": "7 SDEs in Modern Machine Learning\nThe renaissance of stochastic differential equations in machine learning represents one of the most exciting developments in contemporary AI research. This convergence has led to breakthrough applications in generative modeling, continuous-time neural networks, and probabilistic machine learning.\n\n7.1 Neural Ordinary Differential Equations (NODEs)\nThe Neural ODE framework, introduced by Chen et al. (2018), revolutionized deep learning by treating neural networks as continuous-time dynamical systems.\nMathematical Framework: Instead of discrete layers, Neural ODEs model the hidden state evolution as: \\[\\frac{dh(t)}{dt} = f_\\theta(h(t), t)\\]\nwhere \\(f_\\theta\\) is a neural network parameterized by \\(\\theta\\), and the output is obtained by solving: \\[h(T) = h(0) + \\int_0^T f_\\theta(h(t), t) dt\\]\nKey Advantages: - Memory efficiency: Constant memory cost during training - Adaptive computation: Automatic step size selection\n- Continuous depth: Networks with “infinite” layers - Invertible transformations: Normalizing flows applications\n\n\n7.2 Neural Stochastic Differential Equations\nNeural SDEs extend Neural ODEs by incorporating stochastic dynamics, providing better uncertainty quantification and more expressive models.\nModel Specification: Neural SDEs are defined as: \\[dh(t) = f_\\theta(h(t), t) dt + g_\\theta(h(t), t) dW(t)\\]\nwhere: - \\(f_\\theta\\) is the neural drift function - \\(g_\\theta\\) is the neural diffusion function\n- \\(W(t)\\) represents Brownian motion\n\n\nCode\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SimpleNeuralODE(nn.Module):\n    \"\"\"Simple Neural ODE implementation.\"\"\"\n    def __init__(self, input_dim, hidden_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, input_dim)\n        )\n    \n    def forward(self, t, x):\n        return self.net(x)\n\nclass SimpleNeuralSDE(nn.Module):\n    \"\"\"Simple Neural SDE implementation.\"\"\"\n    def __init__(self, input_dim, hidden_dim):\n        super().__init__()\n        self.drift_net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, input_dim)\n        )\n        self.diffusion_net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, input_dim),\n            nn.Softplus()  # Ensure positive diffusion\n        )\n    \n    def forward(self, t, x):\n        drift = self.drift_net(x)\n        diffusion = self.diffusion_net(x)\n        return drift, diffusion\n\ndef euler_maruyama_neural_sde(sde_func, x0, t_span, dt=0.01):\n    \"\"\"Solve Neural SDE using Euler-Maruyama method.\"\"\"\n    t_start, t_end = t_span\n    n_steps = int((t_end - t_start) / dt)\n    \n    trajectory = [x0]\n    x = x0\n    \n    for i in range(n_steps):\n        t = t_start + i * dt\n        drift, diffusion = sde_func(t, x)\n        \n        # Euler-Maruyama step\n        dW = torch.randn_like(x) * np.sqrt(dt)\n        x = x + drift * dt + diffusion * dW\n        trajectory.append(x.clone())\n    \n    return torch.stack(trajectory)\n\ndef ode_solve_euler(ode_func, x0, t_span, dt=0.01):\n    \"\"\"Simple Euler method for ODE solving.\"\"\"\n    t_start, t_end = t_span\n    n_steps = int((t_end - t_start) / dt)\n    \n    trajectory = [x0]\n    x = x0\n    \n    for i in range(n_steps):\n        t = t_start + i * dt\n        dx_dt = ode_func(t, x)\n        x = x + dx_dt * dt\n        trajectory.append(x.clone())\n    \n    return torch.stack(trajectory)\n\n# Generate synthetic spiral data\ndef generate_spiral_data(n_samples=1000, noise=0.1):\n    \"\"\"Generate 2D spiral dataset.\"\"\"\n    t = torch.linspace(0, 4*np.pi, n_samples)\n    x = t * torch.cos(t) + noise * torch.randn(n_samples)\n    y = t * torch.sin(t) + noise * torch.randn(n_samples)\n    return torch.stack([x, y], dim=1)\n\n# Set up models\ntorch.manual_seed(42)\nnp.random.seed(42)\n\ninput_dim = 2\nhidden_dim = 32\n\nneural_ode = SimpleNeuralODE(input_dim, hidden_dim)\nneural_sde = SimpleNeuralSDE(input_dim, hidden_dim)\n\n# Generate training data\ndata = generate_spiral_data(500, 0.1)\n\n# Initial conditions for forward simulation\nx0_samples = torch.randn(10, 2) * 0.5\nt_span = (0.0, 2.0)\n\nfig, axes = plt.subplots(3, 2, figsize=(18, 14))\n\n# Plot training data\naxes[0, 0].scatter(data[:, 0].numpy(), data[:, 1].numpy(), alpha=0.6, s=20, c='blue')\naxes[0, 0].set_title('Training Data: Noisy Spiral')\naxes[0, 0].set_xlabel('x₁')\naxes[0, 0].set_ylabel('x₂')\naxes[0, 0].grid(True, alpha=0.3)\naxes[0, 0].axis('equal')\n\n# Simulate Neural ODE trajectories\nwith torch.no_grad():\n    ode_trajectories = []\n    for x0 in x0_samples:\n        traj = ode_solve_euler(neural_ode, x0.unsqueeze(0), t_span, dt=0.05)\n        ode_trajectories.append(traj)\n        axes[0, 1].plot(traj[:, 0, 0].numpy(), traj[:, 0, 1].numpy(), \n                       alpha=0.7, linewidth=2)\n\naxes[0, 1].scatter(x0_samples[:, 0].numpy(), x0_samples[:, 1].numpy(), \n                  color='red', s=50, marker='o', zorder=5, label='Initial points')\naxes[0, 1].set_title('Neural ODE Trajectories (Untrained)')\naxes[0, 1].set_xlabel('x₁')\naxes[0, 1].set_ylabel('x₂')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\naxes[0, 1].axis('equal')\n\n# Simulate Neural SDE trajectories\nwith torch.no_grad():\n    torch.manual_seed(42)  # For reproducible stochastic trajectories\n    sde_trajectories = []\n    for x0 in x0_samples:\n        traj = euler_maruyama_neural_sde(neural_sde, x0.unsqueeze(0), t_span, dt=0.01)\n        sde_trajectories.append(traj)\n        axes[1, 0].plot(traj[:, 0, 0].numpy(), traj[:, 0, 1].numpy(), \n                       alpha=0.7, linewidth=1.5)\n\naxes[1, 0].scatter(x0_samples[:, 0].numpy(), x0_samples[:, 1].numpy(), \n                  color='red', s=50, marker='o', zorder=5, label='Initial points')\naxes[1, 0].set_title('Neural SDE Trajectories (Untrained)')\naxes[1, 0].set_xlabel('x₁')\naxes[1, 0].set_ylabel('x₂')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\naxes[1, 0].axis('equal')\n\n# Compare multiple SDE realizations from same initial condition\nwith torch.no_grad():\n    x0_single = torch.tensor([[0.0, 0.0]])\n    n_realizations = 20\n    \n    for i in range(n_realizations):\n        torch.manual_seed(i)  # Different random seeds\n        traj = euler_maruyama_neural_sde(neural_sde, x0_single, t_span, dt=0.01)\n        axes[1, 1].plot(traj[:, 0, 0].numpy(), traj[:, 0, 1].numpy(), \n                       alpha=0.5, linewidth=1, color='blue')\n\naxes[1, 1].scatter([0], [0], color='red', s=100, marker='o', zorder=5, \n                  label='Common initial point')\naxes[1, 1].set_title('SDE Uncertainty: Multiple Realizations')\naxes[1, 1].set_xlabel('x₁')\naxes[1, 1].set_ylabel('x₂')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\naxes[1, 1].axis('equal')\n\n# Analyze drift and diffusion components\nx_grid = torch.linspace(-3, 3, 20)\ny_grid = torch.linspace(-3, 3, 20)\nX, Y = torch.meshgrid(x_grid, y_grid, indexing='ij')\ngrid_points = torch.stack([X.flatten(), Y.flatten()], dim=1)\n\nwith torch.no_grad():\n    drift_vals, diffusion_vals = neural_sde(0.0, grid_points)\n    drift_vals = drift_vals.reshape(20, 20, 2)\n    diffusion_vals = diffusion_vals.reshape(20, 20, 2)\n\n# Plot drift field\naxes[2, 0].quiver(X.numpy(), Y.numpy(), \n                 drift_vals[:, :, 0].numpy(), drift_vals[:, :, 1].numpy(),\n                 alpha=0.7, scale=20)\naxes[2, 0].set_title('Neural SDE Drift Field f_θ(x,t)')\naxes[2, 0].set_xlabel('x₁')\naxes[2, 0].set_ylabel('x₂')\naxes[2, 0].grid(True, alpha=0.3)\naxes[2, 0].axis('equal')\n\n# Plot diffusion magnitude\ndiffusion_magnitude = torch.norm(diffusion_vals, dim=2)\nim = axes[2, 1].contourf(X.numpy(), Y.numpy(), diffusion_magnitude.numpy(), \n                        levels=20, cmap='viridis')\naxes[2, 1].set_title('Neural SDE Diffusion Magnitude |g_θ(x,t)|')\naxes[2, 1].set_xlabel('x₁')\naxes[2, 1].set_ylabel('x₂')\nplt.colorbar(im, ax=axes[2, 1])\n\nplt.tight_layout()\nplt.show()\n\n# Create a simple training loop demonstration\nprint(\"Neural SDE vs Neural ODE Comparison:\")\nprint(\"=\" * 50)\nprint(\"Key Differences:\")\nprint(\"1. Deterministic vs Stochastic: ODEs produce deterministic trajectories,\")\nprint(\"   SDEs incorporate randomness and uncertainty\")\nprint(\"2. Memory vs Uncertainty: ODEs are memory efficient, SDEs provide\")\nprint(\"   natural uncertainty quantification\") \nprint(\"3. Training: SDEs require handling stochastic gradients and\")\nprint(\"   multiple trajectory sampling\")\nprint(\"4. Applications: ODEs for normalizing flows, SDEs for generative\")\nprint(\"   modeling with uncertainty\")\n\n\n\n\n\n\n\n\nFigure 13: Neural SDEs: From deterministic ODEs to stochastic dynamics in deep learning\n\n\n\n\n\nNeural SDE vs Neural ODE Comparison:\n==================================================\nKey Differences:\n1. Deterministic vs Stochastic: ODEs produce deterministic trajectories,\n   SDEs incorporate randomness and uncertainty\n2. Memory vs Uncertainty: ODEs are memory efficient, SDEs provide\n   natural uncertainty quantification\n3. Training: SDEs require handling stochastic gradients and\n   multiple trajectory sampling\n4. Applications: ODEs for normalizing flows, SDEs for generative\n   modeling with uncertainty\n\n\n\n\n7.3 Gaussian Processes and SDEs\nGaussian Processes (GPs) provide a natural connection between SDEs and machine learning, as many GPs can be represented as solutions to linear SDEs.\nConnection: A GP with Matérn covariance function corresponds to the solution of the SDE: \\[d^n X(t) + a_{n-1} d^{n-1} X(t) + \\cdots + a_1 dX(t) + a_0 X(t) dt = \\sigma dW(t)\\]\nThis connection enables: - Efficient GP inference: Converting GP regression to Kalman filtering - Streaming predictions: Online learning with infinite data - Scalable GPs: Linear complexity in time series length\n\n\nCode\ndef matern_32_sde_matrices(length_scale, sigma):\n    \"\"\"\n    State-space representation of Matérn 3/2 GP.\n    dX/dt = F*X + L*w, where w is white noise\n    \"\"\"\n    lam = np.sqrt(3) / length_scale\n    F = np.array([[0, 1], \n                  [-lam**2, -2*lam]])\n    L = np.array([[0], \n                  [sigma * 2 * lam * np.sqrt(lam)]])\n    H = np.array([[1, 0]])  # Observation matrix\n    return F, L, H\n\ndef simulate_matern_sde(F, L, t_span, dt=0.01):\n    \"\"\"Simulate Matérn process using SDE representation.\"\"\"\n    t_start, t_end = t_span\n    t_points = np.arange(t_start, t_end + dt, dt)\n    n_steps = len(t_points)\n    \n    # State dimension\n    state_dim = F.shape[0]\n    noise_dim = L.shape[1]\n    \n    # Initialize\n    X = np.zeros((n_steps, state_dim))\n    X[0] = np.random.randn(state_dim)\n    \n    # Simulate\n    sqrt_dt = np.sqrt(dt)\n    for i in range(1, n_steps):\n        dW = np.random.randn(noise_dim) * sqrt_dt\n        X[i] = X[i-1] + F @ X[i-1] * dt + L @ dW\n    \n    return t_points, X\n\ndef kalman_filter_gp(y_obs, t_obs, F, L, H, R, dt=0.01):\n    \"\"\"\n    Kalman filter for GP inference using SDE representation.\n    \"\"\"\n    n_obs = len(y_obs)\n    state_dim = F.shape[0]\n    \n    # Initialize\n    x_pred = np.zeros((n_obs, state_dim))\n    x_filt = np.zeros((n_obs, state_dim))\n    P_pred = np.zeros((n_obs, state_dim, state_dim))\n    P_filt = np.zeros((n_obs, state_dim, state_dim))\n    \n    # Initial conditions\n    x_pred[0] = np.zeros(state_dim)\n    P_pred[0] = np.eye(state_dim) * 10\n    \n    # Process noise covariance\n    Q = L @ L.T * dt\n    \n    for i in range(n_obs):\n        if i &gt; 0:\n            # Predict step\n            dt_step = t_obs[i] - t_obs[i-1]\n            # Simple Euler integration for transition\n            x_pred[i] = x_filt[i-1] + F @ x_filt[i-1] * dt_step\n            P_pred[i] = P_filt[i-1] + (F @ P_filt[i-1] + P_filt[i-1] @ F.T + Q) * dt_step\n        \n        # Update step\n        innovation = y_obs[i] - H @ x_pred[i]\n        S = H @ P_pred[i] @ H.T + R\n        K = (P_pred[i] @ H.T / S).reshape(-1, 1)\n        \n        x_filt[i] = x_pred[i] + (K * innovation).flatten()\n        P_filt[i] = P_pred[i] - K.reshape(-1, 1) @ H @ P_pred[i]\n    \n    return x_filt, P_filt\n\n# Generate synthetic data\nnp.random.seed(42)\n\n# GP parameters\nlength_scale = 1.0\nsigma = 1.0\nnoise_std = 0.1\n\n# Generate true function using SDE simulation\nF, L, H = matern_32_sde_matrices(length_scale, sigma)\nt_span = (0, 10)\ndt = 0.01\n\nt_fine, X_true = simulate_matern_sde(F, L, t_span, dt)\nf_true = H @ X_true.T  # Extract function values\n\n# Create sparse observations\nn_obs = 20\nobs_indices = np.sort(np.random.choice(len(t_fine), n_obs, replace=False))\nt_obs = t_fine[obs_indices]\ny_obs = f_true[0, obs_indices] + noise_std * np.random.randn(n_obs)\n\nfig, axes = plt.subplots(3, 2, figsize=(16, 12))\n\n# Plot true function and observations\naxes[0, 0].plot(t_fine, f_true[0], 'b-', alpha=0.7, linewidth=2, label='True function')\naxes[0, 0].scatter(t_obs, y_obs, color='red', s=30, zorder=5, label='Observations')\naxes[0, 0].set_title('Matérn 3/2 Process: True Function and Observations')\naxes[0, 0].set_xlabel('Time t')\naxes[0, 0].set_ylabel('f(t)')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Kalman filter inference\nR = noise_std**2  # Observation noise\nx_filt, P_filt = kalman_filter_gp(y_obs, t_obs, F, L, H, R, dt=0.1)\n\n# Extract posterior mean and variance at observation points\npost_mean = H @ x_filt.T\npost_var = np.array([H @ P @ H.T for P in P_filt])\n\naxes[0, 1].plot(t_fine, f_true[0], 'b-', alpha=0.7, linewidth=2, label='True function')\naxes[0, 1].scatter(t_obs, y_obs, color='red', s=30, zorder=5, label='Observations')\naxes[0, 1].plot(t_obs, post_mean[0], 'g-', linewidth=2, label='GP posterior mean')\naxes[0, 1].fill_between(t_obs, \n                       post_mean[0] - 2*np.sqrt(post_var[:, 0, 0]),\n                       post_mean[0] + 2*np.sqrt(post_var[:, 0, 0]),\n                       alpha=0.3, color='green', label='±2σ confidence')\naxes[0, 1].set_title('GP Inference via Kalman Filter')\naxes[0, 1].set_xlabel('Time t')\naxes[0, 1].set_ylabel('f(t)')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Compare with standard GP regression using sklearn\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import Matern\n\n# Standard GP\nkernel = Matern(length_scale=length_scale, nu=1.5) * sigma**2\ngp = GaussianProcessRegressor(kernel=kernel, alpha=noise_std**2)\ngp.fit(t_obs.reshape(-1, 1), y_obs)\n\n# Predict on fine grid\nt_pred = np.linspace(0, 10, 100)\ny_pred, y_std = gp.predict(t_pred.reshape(-1, 1), return_std=True)\n\naxes[1, 0].plot(t_fine, f_true[0], 'b-', alpha=0.7, linewidth=2, label='True function')\naxes[1, 0].scatter(t_obs, y_obs, color='red', s=30, zorder=5, label='Observations')\naxes[1, 0].plot(t_pred, y_pred, 'purple', linewidth=2, label='Standard GP mean')\naxes[1, 0].fill_between(t_pred, y_pred - 2*y_std, y_pred + 2*y_std,\n                       alpha=0.3, color='purple', label='±2σ confidence')\naxes[1, 0].set_title('Standard GP Regression')\naxes[1, 0].set_xlabel('Time t')\naxes[1, 0].set_ylabel('f(t)')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# State evolution visualization\naxes[1, 1].plot(t_obs, x_filt[:, 0], 'g-', linewidth=2, label='State x₁ (function)')\naxes[1, 1].plot(t_obs, x_filt[:, 1], 'orange', linewidth=2, label='State x₂ (derivative)')\naxes[1, 1].fill_between(t_obs, \n                       x_filt[:, 0] - 2*np.sqrt(P_filt[:, 0, 0]),\n                       x_filt[:, 0] + 2*np.sqrt(P_filt[:, 0, 0]),\n                       alpha=0.3, color='green')\naxes[1, 1].fill_between(t_obs, \n                       x_filt[:, 1] - 2*np.sqrt(P_filt[:, 1, 1]),\n                       x_filt[:, 1] + 2*np.sqrt(P_filt[:, 1, 1]),\n                       alpha=0.3, color='orange')\naxes[1, 1].set_title('State Space Evolution (Kalman Filter)')\naxes[1, 1].set_xlabel('Time t')\naxes[1, 1].set_ylabel('State value')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\n# Phase space plot\naxes[2, 0].plot(X_true[:, 0], X_true[:, 1], 'b-', alpha=0.7, linewidth=1, label='True trajectory')\naxes[2, 0].plot(x_filt[:, 0], x_filt[:, 1], 'ro-', markersize=4, label='Filtered states')\naxes[2, 0].set_title('Phase Space: Function vs Derivative')\naxes[2, 0].set_xlabel('f(t)')\naxes[2, 0].set_ylabel(\"f'(t)\")\naxes[2, 0].legend()\naxes[2, 0].grid(True, alpha=0.3)\n\n# Computational comparison\nimport time\n\n# Time standard GP\nstart_time = time.time()\nfor _ in range(10):\n    gp.fit(t_obs.reshape(-1, 1), y_obs)\n    y_pred, _ = gp.predict(t_pred.reshape(-1, 1), return_std=True)\ngp_time = (time.time() - start_time) / 10\n\n# Time Kalman filter approach  \nstart_time = time.time()\nfor _ in range(10):\n    x_filt, P_filt = kalman_filter_gp(y_obs, t_obs, F, L, H, R)\nkf_time = (time.time() - start_time) / 10\n\nmethods = ['Standard GP', 'Kalman Filter GP']\ntimes = [gp_time, kf_time]\ncolors = ['purple', 'green']\n\nbars = axes[2, 1].bar(methods, times, color=colors, alpha=0.7)\naxes[2, 1].set_title('Computational Efficiency Comparison')\naxes[2, 1].set_ylabel('Time (seconds)')\naxes[2, 1].grid(True, alpha=0.3)\n\n# Add time labels on bars\nfor bar, time_val in zip(bars, times):\n    height = bar.get_height()\n    axes[2, 1].text(bar.get_x() + bar.get_width()/2., height + 0.001,\n                   f'{time_val:.4f}s', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"GP-SDE Connection Benefits:\")\nprint(\"=\" * 40)\nprint(f\"Standard GP time: {gp_time:.4f} seconds\")\nprint(f\"Kalman Filter time: {kf_time:.4f} seconds\") \nprint(f\"Speedup factor: {gp_time/kf_time:.2f}x\")\nprint(\"\\nKey advantages of SDE representation:\")\nprint(\"1. Linear complexity O(n) vs O(n³) for standard GP\")\nprint(\"2. Online/streaming inference capability\")\nprint(\"3. Natural handling of non-stationary processes\")\nprint(\"4. Connection to control theory and signal processing\")\n\n\n\n\n\n\n\n\nFigure 14: Gaussian Processes as SDE solutions: Efficient inference via state-space methods\n\n\n\n\n\nGP-SDE Connection Benefits:\n========================================\nStandard GP time: 0.0043 seconds\nKalman Filter time: 0.0004 seconds\nSpeedup factor: 10.01x\n\nKey advantages of SDE representation:\n1. Linear complexity O(n) vs O(n³) for standard GP\n2. Online/streaming inference capability\n3. Natural handling of non-stationary processes\n4. Connection to control theory and signal processing\n\n\n\n\n7.4 Neural Processes: Bridging GPs and Neural Networks\nNeural Processes (NPs) combine the flexibility of neural networks with the uncertainty quantification of Gaussian Processes, representing a paradigm shift in meta-learning and few-shot prediction.\nArchitecture: Neural Processes consist of: 1. Encoder: Maps context points to representations 2. Aggregator: Combines representations (often permutation-invariant)\n3. Decoder: Generates predictions at target points\nMathematical Formulation: Given context set \\(\\mathcal{C} = \\{(x_i, y_i)\\}_{i=1}^n\\) and target inputs \\(\\mathbf{x}_*\\), NPs model: \\[p(y_* | \\mathbf{x}_*, \\mathcal{C}) = \\int p(y_* | \\mathbf{x}_*, z) p(z | \\mathcal{C}) dz\\]\nwhere \\(z\\) is a latent representation capturing the underlying function.\nThe connection to SDEs emerges through: - Stochastic processes: NPs model distributions over functions - Uncertainty propagation: Similar to SDE solution uncertainty - Continuous-time extensions: Neural Process SDEs for temporal modeling\n\n\n7.5 Diffusion Models: SDEs for Generative AI\nDiffusion models represent one of the most successful applications of SDE theory in modern machine learning, powering state-of-the-art generative models for images, audio, and text.\nMathematical Framework: Diffusion models are built on the theory of denoising diffusion processes, which can be formulated as SDEs. The framework consists of two processes:\n\nForward Process (Noise Addition): A fixed diffusion process that gradually adds Gaussian noise: \\[dX_t = -\\frac{1}{2}\\beta(t) X_t dt + \\sqrt{\\beta(t)} dW_t\\]\nReverse Process (Denoising): A learned SDE that reverses the noise addition: \\[dX_t = \\left[-\\frac{1}{2}\\beta(t) X_t - \\beta(t) \\nabla_{X_t} \\log p_t(X_t)\\right] dt + \\sqrt{\\beta(t)} dW_t\\]\n\nwhere \\(\\beta(t)\\) is the noise schedule and \\(\\nabla_{X_t} \\log p_t(X_t)\\) is the score function.\nScore-Based Generative Models: The key insight is that learning the score function \\(\\nabla_{X_t} \\log p_t(X_t)\\) enables sample generation by solving the reverse SDE.\n\n\nCode\ndef linear_beta_schedule(timesteps, beta_start=0.0001, beta_end=0.02):\n    \"\"\"Linear noise schedule for diffusion process.\"\"\"\n    return np.linspace(beta_start, beta_end, timesteps)\n\ndef cosine_beta_schedule(timesteps, s=0.008):\n    \"\"\"Cosine noise schedule for improved sampling.\"\"\"\n    steps = timesteps + 1\n    x = np.linspace(0, timesteps, steps)\n    alphas_cumprod = np.cos(((x / timesteps) + s) / (1 + s) * np.pi * 0.5) ** 2\n    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n    return np.clip(betas, 0.0001, 0.9999)\n\nclass SimpleDiffusionModel:\n    \"\"\"Simplified diffusion model for demonstration.\"\"\"\n    \n    def __init__(self, timesteps=1000, beta_schedule='linear'):\n        self.timesteps = timesteps\n        \n        if beta_schedule == 'linear':\n            self.betas = linear_beta_schedule(timesteps)\n        else:\n            self.betas = cosine_beta_schedule(timesteps)\n        \n        self.alphas = 1.0 - self.betas\n        self.alphas_cumprod = np.cumprod(self.alphas)\n        self.alphas_cumprod_prev = np.concatenate([np.array([1.0]), self.alphas_cumprod[:-1]])\n        \n        # Precompute useful quantities\n        self.sqrt_alphas_cumprod = np.sqrt(self.alphas_cumprod)\n        self.sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - self.alphas_cumprod)\n        \n    def q_sample(self, x_start, t, noise=None):\n        \"\"\"Forward diffusion process: q(x_t | x_0).\"\"\"\n        if noise is None:\n            noise = np.random.randn(*x_start.shape)\n        \n        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t]\n        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t]\n        \n        return (sqrt_alphas_cumprod_t * x_start + \n                sqrt_one_minus_alphas_cumprod_t * noise)\n    \n    def p_sample_step(self, model_output, x_t, t):\n        \"\"\"Single reverse diffusion step (simplified).\"\"\"\n        # Extract noise prediction\n        predicted_noise = model_output\n        \n        # Compute coefficients\n        alpha_t = self.alphas[t]\n        alpha_cumprod_t = self.alphas_cumprod[t]\n        alpha_cumprod_t_prev = self.alphas_cumprod_prev[t]\n        \n        # Compute mean of reverse process\n        pred_original_sample = (x_t - np.sqrt(1 - alpha_cumprod_t) * predicted_noise) / np.sqrt(alpha_cumprod_t)\n        pred_original_sample = np.clip(pred_original_sample, -1, 1)\n        \n        # Compute coefficients for x_t\n        pred_sample_direction = np.sqrt(1 - alpha_cumprod_t_prev) * predicted_noise\n        pred_prev_sample = np.sqrt(alpha_cumprod_t_prev) * pred_original_sample + pred_sample_direction\n        \n        return pred_prev_sample\n\ndef simple_score_network(x, t, target_shape=(28, 28)):\n    \"\"\"\n    Simplified score network (noise predictor).\n    In practice, this would be a sophisticated neural network (U-Net, etc.)\n    \"\"\"\n    # For demonstration, just add some structured noise based on time\n    noise_level = t / 1000.0\n    spatial_pattern = np.sin(np.arange(target_shape[0])[:, None] * 0.3) * np.cos(np.arange(target_shape[1])[None, :] * 0.3)\n    predicted_noise = noise_level * spatial_pattern + 0.1 * np.random.randn(*target_shape)\n    return predicted_noise\n\n# Generate synthetic 2D data (Swiss roll)\ndef generate_swiss_roll(n_samples=1000, noise=0.1):\n    \"\"\"Generate Swiss roll dataset.\"\"\"\n    t = 1.5 * np.pi * (1 + 2 * np.random.rand(n_samples))\n    x = t * np.cos(t)\n    y = t * np.sin(t)\n    data = np.column_stack([x, y])\n    data += noise * np.random.randn(n_samples, 2)\n    return data\n\n# Set up diffusion model\nnp.random.seed(42)\ndiffusion = SimpleDiffusionModel(timesteps=200, beta_schedule='cosine')\n\n# Generate training data\nn_samples = 500\ndata_2d = generate_swiss_roll(n_samples, noise=0.1)\ndata_2d = (data_2d - data_2d.mean(axis=0)) / data_2d.std(axis=0)  # Normalize\n\nfig, axes = plt.subplots(4, 4, figsize=(18, 16))\n\n# Original data\naxes[0, 0].scatter(data_2d[:, 0], data_2d[:, 1], alpha=0.6, s=20, c='blue')\naxes[0, 0].set_title('Original Data')\naxes[0, 0].set_xlabel('x₁')\naxes[0, 0].set_ylabel('x₂')\naxes[0, 0].grid(True, alpha=0.3)\n\n# Forward diffusion process at different timesteps\ntimesteps_to_show = [0, 50, 100, 150, 199]\ncolors = ['blue', 'green', 'orange', 'red', 'purple']\n\n# Show forward process\nfor i, (t, color) in enumerate(zip(timesteps_to_show[1:], colors[1:])):\n    noisy_data = []\n    for sample in data_2d:\n        noisy_sample = diffusion.q_sample(sample, t)\n        noisy_data.append(noisy_sample)\n    noisy_data = np.array(noisy_data)\n    \n    row = (i + 1) // 4\n    col = (i + 1) % 4\n    axes[row, col].scatter(noisy_data[:, 0], noisy_data[:, 1], alpha=0.6, s=20, c=color)\n    axes[row, col].set_title(f'Forward Process t={t}')\n    axes[row, col].set_xlabel('x₁')\n    axes[row, col].set_ylabel('x₂')\n    axes[row, col].grid(True, alpha=0.3)\n\n# Beta schedule visualization\naxes[1, 0].plot(diffusion.betas, 'b-', linewidth=2, label='β(t)')\naxes[1, 0].set_title('Noise Schedule β(t)')\naxes[1, 0].set_xlabel('Timestep t')\naxes[1, 0].set_ylabel('β(t)')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Alpha cumulative product\naxes[1, 1].plot(diffusion.alphas_cumprod, 'r-', linewidth=2, label='ᾱ(t)')\naxes[1, 1].set_title('Cumulative Product ᾱ(t)')\naxes[1, 1].set_xlabel('Timestep t')\naxes[1, 1].set_ylabel('ᾱ(t)')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\n# Signal-to-noise ratio\nsnr = diffusion.alphas_cumprod / (1 - diffusion.alphas_cumprod)\naxes[1, 2].semilogy(snr, 'g-', linewidth=2, label='SNR')\naxes[1, 2].set_title('Signal-to-Noise Ratio')\naxes[1, 2].set_xlabel('Timestep t')\naxes[1, 2].set_ylabel('SNR (log scale)')\naxes[1, 2].legend()\naxes[1, 2].grid(True, alpha=0.3)\n\n# Demonstrate reverse process (simplified)\n# Start from noise\nfinal_noise = np.random.randn(100, 2)\naxes[1, 3].scatter(final_noise[:, 0], final_noise[:, 1], alpha=0.6, s=20, c='red')\naxes[1, 3].set_title('Starting Noise (t=T)')\naxes[1, 3].set_xlabel('x₁')\naxes[1, 3].set_ylabel('x₂')\naxes[1, 3].grid(True, alpha=0.3)\n\n# Simplified reverse sampling (without actual trained model)\ncurrent_samples = final_noise.copy()\nreverse_timesteps = [199, 150, 100, 50, 0]\n\nfor i, t in enumerate(reverse_timesteps[1:]):\n    # Simulate denoising step (in practice, this would use trained score network)\n    noise_factor = diffusion.sqrt_one_minus_alphas_cumprod[t]\n    signal_factor = diffusion.sqrt_alphas_cumprod[t]\n    \n    # Simple denoising: move towards data manifold\n    target_samples = data_2d[np.random.choice(len(data_2d), len(current_samples))]\n    denoising_direction = target_samples - current_samples\n    current_samples = current_samples + 0.1 * denoising_direction + 0.1 * np.random.randn(*current_samples.shape)\n    \n    row = 2 + i // 4\n    col = i % 4\n    axes[row, col].scatter(current_samples[:, 0], current_samples[:, 1], \n                          alpha=0.6, s=20, c=colors[i+1])\n    axes[row, col].set_title(f'Reverse Process t={t}')\n    axes[row, col].set_xlabel('x₁')\n    axes[row, col].set_ylabel('x₂')\n    axes[row, col].grid(True, alpha=0.3)\n\n# Compare noise schedules\nfig2, axes2 = plt.subplots(2, 2, figsize=(12, 8))\n\n# Linear vs Cosine schedules\nlinear_betas = linear_beta_schedule(200)\ncosine_betas = cosine_beta_schedule(200)\n\naxes2[0, 0].plot(linear_betas, 'b-', linewidth=2, label='Linear')\naxes2[0, 0].plot(cosine_betas, 'r-', linewidth=2, label='Cosine')\naxes2[0, 0].set_title('Noise Schedules Comparison')\naxes2[0, 0].set_xlabel('Timestep')\naxes2[0, 0].set_ylabel('β(t)')\naxes2[0, 0].legend()\naxes2[0, 0].grid(True, alpha=0.3)\n\n# Corresponding alpha cumprod\nlinear_alphas_cumprod = np.cumprod(1.0 - linear_betas)\ncosine_alphas_cumprod = np.cumprod(1.0 - cosine_betas)\n\naxes2[0, 1].plot(linear_alphas_cumprod, 'b-', linewidth=2, label='Linear')\naxes2[0, 1].plot(cosine_alphas_cumprod, 'r-', linewidth=2, label='Cosine')\naxes2[0, 1].set_title('Signal Preservation ᾱ(t)')\naxes2[0, 1].set_xlabel('Timestep')\naxes2[0, 1].set_ylabel('ᾱ(t)')\naxes2[0, 1].legend()\naxes2[0, 1].grid(True, alpha=0.3)\n\n# Demonstrate ancestral sampling concept\nx_start = data_2d[0]  # Single sample\nt_values = np.arange(0, 200, 20)\nforward_trajectory = []\n\nfor t in t_values:\n    x_t = diffusion.q_sample(x_start, t)\n    forward_trajectory.append(x_t)\n\nforward_trajectory = np.array(forward_trajectory)\n\naxes2[1, 0].plot(forward_trajectory[:, 0], forward_trajectory[:, 1], 'bo-', \n                markersize=4, linewidth=1, alpha=0.7)\naxes2[1, 0].scatter([x_start[0]], [x_start[1]], color='red', s=100, \n                   marker='*', zorder=5, label='Original')\naxes2[1, 0].set_title('Forward Diffusion Trajectory')\naxes2[1, 0].set_xlabel('x₁')\naxes2[1, 0].set_ylabel('x₂')\naxes2[1, 0].legend()\naxes2[1, 0].grid(True, alpha=0.3)\n\n# Score function illustration (simplified)\nx_grid = np.linspace(-3, 3, 20)\ny_grid = np.linspace(-3, 3, 20)\nX, Y = np.meshgrid(x_grid, y_grid)\ngrid_points = np.stack([X.flatten(), Y.flatten()], axis=1)\n\n# Approximate score function (points toward data)\nscores = np.zeros_like(grid_points)\nfor i, point in enumerate(grid_points):\n    # Find nearest data points\n    distances = np.linalg.norm(data_2d - point[None, :], axis=1)\n    nearest_idx = np.argmin(distances)\n    nearest_point = data_2d[nearest_idx]\n    \n    # Score points toward data manifold\n    direction = nearest_point - point\n    scores[i] = direction / (np.linalg.norm(direction) + 1e-8)\n\nscores = scores.reshape(20, 20, 2)\n\naxes2[1, 1].quiver(X, Y, scores[:, :, 0], scores[:, :, 1], \n                  alpha=0.7, scale=20, color='blue')\naxes2[1, 1].scatter(data_2d[:, 0], data_2d[:, 1], alpha=0.6, s=20, c='red')\naxes2[1, 1].set_title('Score Function ∇log p(x)')\naxes2[1, 1].set_xlabel('x₁')\naxes2[1, 1].set_ylabel('x₂')\naxes2[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nfig.tight_layout()\nplt.show()\n\n# Print key insights\nprint(\"Diffusion Models: Key Insights\")\nprint(\"=\" * 40)\nprint(\"1. Forward Process: Systematic noise addition following SDE\")\nprint(\"2. Reverse Process: Learned denoising via score function estimation\")\nprint(\"3. Training: Learn to predict noise added at each timestep\")\nprint(\"4. Sampling: Reverse the diffusion process to generate new samples\")\nprint(\"5. Score Function: ∇log p(x) guides the reverse process\")\nprint(\"\\nAdvantages over GANs:\")\nprint(\"- More stable training\")\nprint(\"- Better mode coverage\") \nprint(\"- Theoretical guarantees\")\nprint(\"- Flexible sampling procedures\")\n\n\nDiffusion Models: Key Insights\n========================================\n1. Forward Process: Systematic noise addition following SDE\n2. Reverse Process: Learned denoising via score function estimation\n3. Training: Learn to predict noise added at each timestep\n4. Sampling: Reverse the diffusion process to generate new samples\n5. Score Function: ∇log p(x) guides the reverse process\n\nAdvantages over GANs:\n- More stable training\n- Better mode coverage\n- Theoretical guarantees\n- Flexible sampling procedures\n\n\n\n\n\n\n\n\n\n\n\n(a) Diffusion models: SDE-based generative modeling with forward and reverse processes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 15\n\n\n\n\nKey Theoretical Results:\n\nProbability Flow ODE: Every SDE has a corresponding ODE with the same marginal distributions: \\[\\frac{dX_t}{dt} = f(X_t, t) - \\frac{1}{2}g^2(t) \\nabla_{X_t} \\log p_t(X_t)\\]\nScore Matching: The score function can be learned by minimizing: \\[\\mathbb{E}_{t,X_0,X_t}\\left[\\left\\|\\epsilon - \\epsilon_\\theta(X_t, t)\\right\\|^2\\right]\\]\nSampling: Generation is achieved by numerically solving the reverse SDE or probability flow ODE.\n\nModern Applications: - Image Generation: DALL-E 2, Stable Diffusion, Imagen - Audio Synthesis: WaveGrad, DiffWave - Text Generation: Diffusion-LM - 3D Shape Generation: Point-E, DreamFusion - Video Generation: Imagen Video, Make-A-Video\n\n\n7.6 Stochastic Neural Differential Equations\nStochastic Neural Differential Equations (SNDEs) extend Neural Ordinary Differential Equations (NODEs) by incorporating stochasticity directly into the neural network dynamics. This allows them to model systems with inherent randomness, making them particularly suitable for tasks like time series forecasting with uncertainty, generative modeling, and reinforcement learning in stochastic environments.\nThe general form of a Neural SDE can be written as:\n\\(dh_t = f_\\theta(h_t, t) dt + g_\\theta(h_t, t) dW_t\\)\nwhere: - \\(h_t\\) is the hidden state of the neural network at time \\(t\\). - \\(f_\\theta(h_t, t)\\) is the drift function, typically parameterized by a neural network with parameters \\(\\theta\\). - \\(g_\\theta(h_t, t)\\) is the diffusion function, also parameterized by a neural network with parameters \\(\\theta\\). - \\(dW_t\\) is a Wiener process (Brownian motion), representing the stochastic input.\nThis formulation allows the model to learn both the deterministic evolution and the noise characteristics of the underlying system.\n\n7.6.1 Implementation Example: Simple Neural SDE in PyTorch\nHere’s a basic PyTorch implementation of a Neural SDE, demonstrating how to define the drift and diffusion networks and simulate the process. For simplicity, we’ll use a fixed time step Euler-Maruyama solver.\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the Neural SDE model\nclass NeuralSDE(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(NeuralSDE, self).__init__()\n        self.drift_net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, output_dim)\n        )\n        self.diffusion_net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, output_dim)\n        )\n\n    def forward(self, h, t):\n        # h: current hidden state, t: current time\n        # For simplicity, we'll ignore 't' in this basic example,\n        # but it can be incorporated into the network inputs.\n        drift = self.drift_net(h)\n        diffusion = self.diffusion_net(h)\n        return drift, diffusion\n\n# Euler-Maruyama SDE solver\ndef sde_solver_euler_maruyama(model, h0, t_span, dt, num_paths=1):\n    h_paths = []\n    for _ in range(num_paths):\n        h_path = [h0]\n        h_current = h0\n        for i in range(len(t_span) - 1):\n            t_current = t_span[i]\n            dW = torch.randn_like(h_current) * np.sqrt(dt)\n            \n            drift, diffusion = model(h_current, t_current)\n            h_next = h_current + drift * dt + diffusion * dW\n            h_path.append(h_next)\n            h_current = h_next\n        h_paths.append(torch.stack(h_path))\n    return torch.stack(h_paths)\n\n# Parameters\ninput_dim = 1\nhidden_dim = 32\noutput_dim = 1\nh0 = torch.tensor([0.5], dtype=torch.float32) # Initial state\nT_end = 2.0\nnum_steps = 100\ndt = T_end / num_steps\nt_span = np.linspace(0, T_end, num_steps + 1)\nnum_paths = 5\n\n# Initialize Neural SDE model\nsde_model = NeuralSDE(input_dim, hidden_dim, output_dim)\n\n# Simulate paths\ntorch.manual_seed(42) # for reproducibility\nsimulated_paths = sde_solver_euler_maruyama(sde_model, h0, t_span, dt, num_paths)\n\n# Plotting\nplt.figure(figsize=(10, 6))\nfor i in range(num_paths):\n    plt.plot(t_span, simulated_paths[i].detach().numpy(), alpha=0.7)\nplt.title('Simulated Paths from a Simple Neural SDE')\nplt.xlabel('Time')\nplt.ylabel('Hidden State h(t)')\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(f\"Shape of simulated paths: {simulated_paths.shape}\")\nThis example demonstrates the basic structure. In real-world applications, the drift and diffusion networks would be more complex, and training would involve defining a loss function (e.g., likelihood-based or score-matching) and using optimization algorithms to learn the parameters \\(\\theta\\).\nLatent SDEs: Model latent dynamics with SDEs while observing through deterministic functions: \\(dZ_t = f_\\theta(Z_t, t) dt + g_\\theta(Z_t, t) dW_t\\) \\(X_t = h_\\phi(Z_t) + \\epsilon_t\\)\nNeural SDE Training: Uses the reparameterization trick and efficient SDE solvers for gradient computation.\nApplications: Time series modeling, dynamics learning, uncertainty quantification in deep learning.\nThese modern developments demonstrate how classical SDE theory continues to drive innovation in machine learning, providing both theoretical foundations and practical algorithms for the next generation of AI systems.\nRecent work has explored neural networks that directly parameterize SDE coefficients, enabling:\nLatent SDEs: Model latent dynamics with SDEs while observing through deterministic functions: \\[dZ_t = f_\\theta(Z_t, t) dt + g_\\theta(Z_t, t) dW_t\\] \\[X_t = h_\\phi(Z_t) + \\epsilon_t\\]\nNeural SDE Training: Uses the reparameterization trick and efficient SDE solvers for gradient computation.\nApplications: Time series modeling, dynamics learning, uncertainty quantification in deep learning.\nThese modern developments demonstrate how classical SDE theory continues to drive innovation in machine learning, providing both theoretical foundations and practical algorithms for the next generation of AI systems."
  },
  {
    "objectID": "posts/03-04-2025_intro_to_sde/index.html#sec-conclusion",
    "href": "posts/03-04-2025_intro_to_sde/index.html#sec-conclusion",
    "title": "Stochastic Differential Equations: From Mathematical Foundations to Modern Applications",
    "section": "8 Conclusion and Future Directions",
    "text": "8 Conclusion and Future Directions\nThis comprehensive exploration of stochastic differential equations reveals the profound mathematical elegance and practical power of this theoretical framework. From Itô’s revolutionary development of stochastic calculus in the 1940s to today’s cutting-edge applications in generative AI, SDEs have consistently provided the mathematical foundation for modeling and understanding complex random phenomena.\n\n8.1 Key Contributions and Insights\nMathematical Foundations: We have seen how the careful construction of stochastic integration and Itô’s lemma provides the rigorous mathematical framework necessary for analyzing continuous-time random processes. The fundamental insight that \\((dW_t)^2 = dt\\) transforms our understanding of calculus in stochastic settings.\nComputational Methods: The development of numerical schemes like Euler-Maruyama and Milstein demonstrates how theoretical insights translate into practical computational tools. The trade-offs between accuracy, computational cost, and stability remain central to successful implementation.\nFinancial Applications: The Black-Scholes model, interest rate models, and stochastic volatility frameworks showcase how SDE theory has revolutionized quantitative finance. These applications demonstrate the power of mathematical modeling in creating practical solutions to complex real-world problems.\nMachine Learning Renaissance: The emergence of Neural ODEs, diffusion models, and neural processes illustrates how classical mathematical theory continues to inspire breakthrough innovations in artificial intelligence. The connection between score-based generative models and SDE theory represents a particularly elegant synthesis of probability theory and deep learning.\n\n\n8.2 Future Research Directions\nSeveral exciting avenues for future research emerge from our exploration:\nComputational Advances: Development of more efficient numerical methods for high-dimensional SDEs, particularly for machine learning applications where computational scalability is crucial.\nTheoretical Extensions: Investigation of fractional SDEs, jump-diffusion processes, and SDEs on manifolds to capture more complex real-world phenomena.\nMachine Learning Integration: Deeper integration of SDE theory with modern machine learning, including applications to reinforcement learning, causal inference, and interpretable AI.\nCross-Disciplinary Applications: Extension of SDE methods to new domains such as biology, climate science, and social networks, where stochastic modeling can provide new insights.\n\n\n8.3 Final Reflections\nThe journey from Brownian motion to modern AI demonstrates the enduring value of rigorous mathematical theory. Stochastic differential equations exemplify how abstract mathematical concepts, developed through careful theoretical investigation, ultimately find profound practical applications that transform entire fields.\nAs we stand at the intersection of classical probability theory and modern artificial intelligence, SDEs continue to provide both the theoretical foundation and practical tools necessary for the next generation of scientific and technological breakthroughs. The mathematical elegance of Itô calculus, combined with the computational power of modern algorithms, ensures that stochastic differential equations will remain at the forefront of mathematical innovation for decades to come."
  },
  {
    "objectID": "posts/03-04-2025_intro_to_sde/index.html#sec-references",
    "href": "posts/03-04-2025_intro_to_sde/index.html#sec-references",
    "title": "Stochastic Differential Equations: From Mathematical Foundations to Modern Applications",
    "section": "9 References",
    "text": "9 References\nThe comprehensive nature of this exploration draws upon decades of mathematical and computational research. Key references include foundational texts on stochastic calculus, numerical analysis, financial mathematics, and modern machine learning applications. The bibliography provides entry points for deeper investigation into each topic area covered in this treatise."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jan Schlegel",
    "section": "",
    "text": "Hi there! I’m Jan, a Master’s student in Statistics at ETH Zurich with a passion for machine learning, healthcare applications, and probabilistic modeling. My journey combines rigorous academic training in statistics with hands-on experience in research and data analysis. I thrive on solving complex problems and am particularly interested in the intersection of machine learning and healthcare. With a strong foundation in both theoretical statistics and practical implementation, I’m always eager to tackle new challenges and contribute to meaningful projects. Currently seeking opportunities in machine learning and data science - let’s connect!"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Jan Schlegel",
    "section": "Education",
    "text": "Education\n\n\n🎓 Master of Science (M.Sc. ETH) in Statistics\n\n🏛️ ETH Zurich | 📍 Zurich, Switzerland | 📅 Sep 2023 - Sep 2025\n\n\nGPA: 5.98 / 6.00\n\n\nRelevant Coursework: Deep Learning; Image Analysis and Computer Vision; Machine Learning for Healthcare; Probabilistic AI; Statistical Learning Theory; Introduction to Machine Learning; Causality; Computational Statistics; Fundamentals of Mathematical Statistics; High Dimensional Statistics; Bayesian Statistics; Time Series Analysis\nSemester Paper: Extrapolation and Distributional Robustness for Climate Downscaling\nMentor for New Students\n\n\n\n🎓 Special Student Statistics\n\n🏛️ ETH Zurich | 📍 Zurich, Switzerland | 📅 Feb 2022 - Feb 2023\n\n\nGPA: 6.00 / 6.00\n\n\nVoluntarily enrolled to take advanced courses in statistics\nCoursework: Applied Time Series, Applied Multivariate Statistics\n\n\n\n🎓 Bachelor of Arts in Business and Economics\n\n🏛️ University of Zurich (UZH) | 📍 Zurich, Switzerland | 📅 Sep 2019 - Feb 2023\n\n\nGPA: 5.87 / 6.00\n\n\nFinished the degree with 215 ECTS (instead of 180 ECTS)\nMajor in Banking and Finance (150 ECTS)\nMinor in Applied Probability and Statistics (30 ECTS)\nRelevant Coursework: Likelihood Inference; Statistical Modelling; Numerical Methods in Informatics; Introduction to Machine Learning; Introduction to Statistics; Introductory Econometrics Thesis: Portfolio Value at Risk Forecasting with GARCH-Type Models (Grade: 6.00/6.00)"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Jan Schlegel",
    "section": "Experience",
    "text": "Experience\n\n\nResearch Assistant (Civilian Service)\n\n🏛️ University of Zurich (EBPI) | 📍 Zurich, Switzerland | 📅 Feb 2022 - Aug 2023\n\n\nConducted sophisticated data analysis and statistical modeling using R for epidemiological research\nCollaborated with interdisciplinary team of epidemiologists, statisticians, and public health experts\nContributed to research paper published in Nature Communications (2023): “Persistent humoral immune response in youth throughout the COVID-19 pandemic: prospective school-based cohort study”\n\n\n\nAccountant (Military Service)\n\n🏛️ Swiss Armed Forces | 📍 Kloten, Switzerland | 📅 Jul 2021 - Nov 2021\n\n\nSuccessfully balanced military service obligations while maintaining full-time academic studies\nDemonstrated exceptional time management and organizational skills\n\n\n\nTeaching Assistant\n\n🏛️ University of Zurich | 📍 Zurich, Switzerland | 📅 Sep 2020 - Jul 2021\n\n\nConducted bi-weekly interactive exercise classes in Analysis and Linear Algebra for first-year students\nEffectively communicated complex mathematical concepts to diverse student groups\nDeveloped strong presentation and educational communication skills"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nStochastic Differential Equations: From Mathematical Foundations to Modern Applications\n\n\n\n\n\n\nMathematics\n\n\nFinance\n\n\nMachine Learning\n\n\nPython\n\n\nStochastic Processes\n\n\n\nThis comprehensive treatment of stochastic differential equation theory covers fundamental concepts from Brownian motion and Itô calculus to advanced applications in mathematical finance, neural processes, and diffusion models. We provide rigorous mathematical foundations, numerical solution methods, and extensive Python implementations with publication-ready visualizations, bridging classical stochastic analysis with cutting-edge machine learning applications.\n\n\n\n\n\nApr 3, 2025\n\n\nJan Schlegel\n\n\n\n\n\n\n\n\n\n\n\n\nComparing Univariate and Multivariate Models for Value at Risk Forecasting\n\n\n\n\n\n\nSimulation\n\n\nR\n\n\n\nThis post explores the effectiveness of univariate and multivariate GARCH-based models in forecasting Value at Risk (VaR) for a long equity portfolio. While multivariate models generally perform better in backtests, univariate models often fall short. However, neither model type consistently outperforms the other in predictive accuracy, highlighting the trade-offs between simplicity and complexity in risk forecasting.\n\n\n\n\n\nFeb 7, 2025\n\n\nJan Schlegel\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html",
    "href": "posts/07-02-2025_portfolio-var/index.html",
    "title": "Portfolio Value at Risk Forecasting with GARCH-Type Models",
    "section": "",
    "text": "This thesis examines the value at risk (VaR) forecasting ability of various univariate and multivariate models for a long equity portfolio. All of the considered models involve a generalized autoregressive conditional heteroskedasticity (GARCH)-type structure. The resulting forecasts are checked for desirable properties using violation-based backtests and compared in terms of predictive ability. We find that the VaR forecasts of almost all univariate models are inadequate, while the multivariate models have few problems passing these backtests. However, we do not find evidence that the multivariate models systematically outperform their univariate counterparts with regards to predictive accuracy, or vice versa."
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#sec-univariate",
    "href": "posts/07-02-2025_portfolio-var/index.html#sec-univariate",
    "title": "Portfolio Value at Risk Forecasting with GARCH-Type Models",
    "section": "3.1 Univariate Models",
    "text": "3.1 Univariate Models\nFor our univariate models we assume the following portfolio return dynamics:\n\\[\nr_{\\text{PF},t}=\\mu+\\epsilon_t,\n\\tag{2}\\]\nwhere for all but the MixN(k)-GARCH we let\n\\[\n\\epsilon_t = \\sigma_t z_t, \\quad z_t\\stackrel{iid}{\\sim}F(0,1),\n\\tag{3}\\]\nwhere \\(F(0,1)\\) is some standardized distribution (i.e. zero-location and unit-scale), \\(\\mu\\) is the unconditional location and \\(\\sigma_t\\) is the scale parameter. In this framework the conditional variance \\(\\sigma_t^2=\\mathbb{V}[r_{\\text{PF},t}|\\mathcal{F}_{t-1}]\\), where \\(\\mathcal{F}_{t-1}=\\{r_{\\text{PF},1},...,r_{\\text{PF},t-1}\\}\\) denotes the information available at time \\(t-1\\), is assumed to be non-constant.\nA simple way to model this conditional variance is by means of a generalized autoregressive conditional heteroskedasticity (GARCH) process. This GARCH model introduced in Bollerslev (1986) is a generalisation of the ARCH model by Robert F. Engle (1982). The most prominent version, the GARCH(1,1), can be formulated via\n\\[\n\\sigma_t^2=\\omega+\\alpha\\epsilon_{t-1}^2+\\beta\\sigma_{t-1}^2,\n\\tag{4}\\]\nwhere \\(\\omega&gt;0,\\alpha\\geq0\\), and \\(\\beta\\geq0\\). In other words, in a GARCH(1,1) model the conditional volatility behaves similarly to how an ARMA(1,1) process would for the conditional mean. For covariance stationarity of the process the parameters have to fulfill \\(\\alpha+\\beta&lt;1\\). In our empirical application, we will only consider the GARCH(1,1) model with normal innovation terms i.e. \\(z_t\\stackrel{iid}{\\sim}\\mathcal{N}(0,1)\\) which will serve as our univariate benchmark. However, it is possible to define higher order GARCH(p,q) processes, but we will only regard the case where \\(p=q=1\\). Thus, henceforth whenever a GARCH-type model is mentioned without explicitly stating \\(p\\) and \\(q\\), we will refer to the \\(p=q=1\\) case.\nA relevant special case of the GARCH model is the exponentially weighted moving average (EWMA). It sets \\(\\omega=0\\) and the weights \\(\\alpha\\) are exponentially decaying and sum up to one:\n\\[\n\\sigma^2_t=\\lambda\\sigma^2_{t-1}+(1-\\lambda)\\epsilon_t^2, \\quad\\lambda\\in(0,1).\n\\tag{5}\\]\nThis model formulation puts more weight on the most recent observations which can be beneficial in some cases. But note that this process is not covariance stationary and the variance is thus not mean-reverting since \\(\\lambda+(1-\\lambda)=1\\). This can pose a problem when large returns are observed as they will influence conditional volatility estimates for a long time (score_EWMA?). For our model comparison, we will be using daily data and therefore we will set the decaying factor \\(\\lambda = 0.94\\) in accordance with (Riskmetrics?).\nA well known stylized fact of stock returns is that negative news tends to increase volatility more than positive news of equal magnitude. To account for this so-called ‘leverage effect’ Glosten, Jagannathan, and Runkle (1993) created an asymmetric extension of the standard GARCH model which they called GJR-GARCH(1,1):\n\\[\n\\sigma_t^2=\\omega+(\\alpha+\\gamma I_{t-1})\\epsilon_{t-1}^2+\\beta\\sigma_{t-1}^2,\n\\tag{6}\\]\nwhere \\(\\omega&gt;0\\), \\(\\alpha+\\gamma\\geq0\\), \\(\\beta\\geq0\\) and\n\\[\nI_{t-1}=\\mathbb{1}_{\\{\\epsilon_{t-1}&lt;0\\}}=\\begin{cases}\n0 & \\text{if } \\epsilon_{t-1}\\geq0\\\\\n1 & \\text{if } \\epsilon_{t-1}&lt;0\n\\end{cases}\n\\tag{7}\\]\nThe process is thereby covariance stationary if \\(\\alpha+\\beta+\\frac{1}{2}\\gamma&lt;1\\). Obviously, if \\(\\gamma=0\\) this model reduces to the GARCH(1,1) from equation Equation 4 which treats news symmetrically, i.e. bad news (\\(\\epsilon_{t-1}&lt;0\\)) have the same impact as good news (\\(\\epsilon_{t-1}\\geq0\\)). In practice however, \\(\\gamma\\) is usually found to be positive (GARCH_glossary?). This leads to an asymmetric relationship since negative news is weighted with \\(\\alpha+\\gamma\\) and positive news only with \\(\\alpha\\). We decided to include this model in our comparison due to the great performance of the GJR with Student t innovations in Santos, Nogales, and Ruiz (2013). Additionally, we tried out a GJR with the skewed-t distribution by Hansen (1994) for the error terms.\nAnother way to capture the leverage effect is presented in Robert F. Engle and Ng (1993) which they termed the NGARCH(1,1) model:\n\\[\n\\sigma_t^2=\\omega + \\alpha\\sigma_{t-1}^2(\\epsilon_{t-1}-\\theta)^2 + \\beta\\sigma_{t-1}^2,\n\\tag{8}\\]\nwhere \\(\\omega&gt;0\\), \\(\\alpha\\geq 0\\) and \\(\\beta\\geq 0\\). For covariance stationarity \\(\\alpha(1+\\theta^2)+\\beta&lt;1\\) is required. It is apparent that for \\(\\theta&gt;0\\) negative innovations (\\(\\epsilon_{t-1}&lt;0\\)) have a larger impact on the conditional variance than positive errors of the same magnitude which causes this model to be asymmetric too (P. Christoffersen and Langlois 2013). We will incorporate this model with a skewed-t distribution for the innovations as this is the same specification that Fortin, Simonato, and Dionne (2022) and P. Christoffersen and Langlois (2013) used in their factor copula models.\nThe last class of univariate models that we included in our empirical comparison is the k-component mixed normal GARCH(1,1) (MixN(k)-GARCH) as introduced in Haas, Mittnik, and Paolella (2004). We will only consider the case which was referred to as diagonal by Haas, Mittnik, and Paolella (2004) as they identified it to be the superior choice in terms of performance and interpretability. In this framework, the conditional distribution of the error term \\(\\epsilon_t\\) is assumed to be mixed normal (see Section 7.3.1) with zero mean,\n\\[\n\\epsilon_t|\\mathcal{F}_{t-1}\\sim\\text{Mix}_k\\text{N}(p_1,...,p_k, \\mu_1,...,\\mu_k, \\sigma_{1,t}^2,...,\\sigma_{k,t}^2),\\quad \\sum_{i=1}^k p_i\\mu_i = 0,\n\\tag{9}\\]\nwhere \\(p_i\\in(0,1)\\,\\forall i\\), \\(\\sum_{i=1}^kp_i=1\\) and \\(\\mathcal{F}_{t-1}=\\{r_{\\text{PF},1},...,r_{\\text{PF},t-1}\\}\\) is the information set at time \\(t-1\\). The associated conditional variances are given by GARCH processes:\n\\[\n\\sigma_{i,t}=\\omega_i+\\alpha_i\\epsilon_{i,t-1}^2+\\beta_i\\sigma_{i,t-1}^2, \\quad i=1,...,k.\n\\tag{10}\\]\nThe \\(k\\geq 2\\) components that are used for the conditional variance each represent a different market condition (Slim, Koubaa, and BenSaïda 2017). Hence, often a low number of components such as 2 or 3 is sufficient as discovered in Haas, Mittnik, and Paolella (2004). In case of different means for the different components, a normal mixture density incorporates skewness and fat tails (or thin tails) (MixN_Carol?). This property enables the MixN(k)-GARCH model to produce sound VaR estimates as showcased in Haas, Mittnik, and Paolella (2004) and Marc S. Paolella, Kuester, and Mittnik (2006)."
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#sec-factor-copula",
    "href": "posts/07-02-2025_portfolio-var/index.html#sec-factor-copula",
    "title": "Portfolio Value at Risk Forecasting with GARCH-Type Models",
    "section": "3.2 A Multivariate Factor Copula-DCC-GARCH Model",
    "text": "3.2 A Multivariate Factor Copula-DCC-GARCH Model\nThe models described in the previous section all forecasted future returns of a portfolio based on past portfolio returns. Alternatively, we can take a multivariate approach where we first model the constituents of the portfolio and only in a second step draw conclusions about the portfolio. One such model was proposed in Fortin, Simonato, and Dionne (2022) following the discoveries made about asymmetric tail dependency of weekly factor returns in P. Christoffersen and Langlois (2013). Consequently, Fortin, Simonato, and Dionne (2022) make use of equity factors that should capture the main risks of stock returns to forecast the VaR of a portfolio of single stocks. More precisely, they utilize the Carhart four-factor model suggested in Carhart (1997) which adds a momentum factor to the linear three-factor model of Fama and French (1993). Within this model, the return of a single stock \\(k\\) in excess of the weekly risk free rate \\(r_{f,t}\\) is given by\n\\[\n\\begin{aligned}\nr_{k,t}-r_{f,t}&=\\alpha_{k,t}+\\beta_{k, \\text{RMRF}}\\text{RMRF}_t+\\beta_{k,\\text{SMB}}\\text{SMB}_t+\\beta_{k,\\text{HML}}\\text{HML}_t+\\beta_{k, \\text{MOM}}\\text{MOM}_t+\\varepsilon_{k,t}\\\\\n&=\\alpha_{k,t}+\\bm{\\beta}_k'\\bm{r}_{\\text{F},t}+\\varepsilon_{k,t},\\qquad t=1,2,...,T,\n\\end{aligned}\n\\tag{11}\\]\nwhere both the intercept \\(\\alpha_{k,t}\\) and the vector of factor loadings \\(\\bm{\\beta}_k\\) are assumed to be constant over time. The vector of equity factors at time \\(t\\), \\(\\bm{r}_{\\text{F},t}=(\\text{RMRF}_t, \\text{SMB}_t, \\text{HML}_t,\\text{MOM}_t)'\\), consists of the market factor (RMRF), the size factor (SMB), the value factor (HML) and the momentum factor (MOM) (cf. Carhart (1997)). This leads to a reduction in dimensionality of the problem since only four factors have to be modelled instead of all constituents of the stock portfolio.\nFortin, Simonato, and Dionne (2022) decided to not only model the conditional variances but also the dependence between the factors. One possible way to do this is by using the Dynamic Conditional Correlation (DCC) structure^[In contrast to Fortin, Simonato, and Dionne (2022), we will be using the DCC model of R. Engle (2002) instead of the cDCC model of Aielli (2013) due to its more established R implementation. The differences between these two models are relatively small according to P. Christoffersen and Langlois (2013). introduced in R. Engle (2002) which is a dynamic extension of the Constant Conditional Correlation (CCC) model by Bollerslev (1990). Let \\(\\bm{Y_t}=(y_{1,t}, y_{2,t},...,y_{n,t})'\\) be a vector consisting of the returns of n assets or of n factors at time \\(t=1,...,T\\) and \\(\\bm{\\mu}=\\mathbb{E}[\\bm{Y_t}|\\mathcal{F}_{t-1}]\\) the corresponding mean vector which is assumed to be constant. The information set at time \\(t-1\\) once again is denoted by \\(\\mathcal{F}_{t-1}\\) and consists of the past returns \\(\\{\\bm{Y_1},...,\\bm{Y_{t-1}}\\}\\).\nWe can then define the conditional covariance matrix of \\(\\bm{r_t}\\) as \\(\\bm{\\Sigma_t}\\eqdef\\mathbb{E}[(\\bm{Y_t}-\\bm{\\mu})(\\bm{Y_t}-\\bm{\\mu})'\\|\\mathcal{F}_{t-1}]\\). The DCC model decomposes the dynamics of the conditional covariance matrix into standard deviations and correlation and can be written as:\n\\[\n\\bm{Y}_{t}|\\mathcal{F}_{t-1}\\sim\\mathcal{N}_n(\\bm{\\mu}, \\bm{\\Sigma_t}), \\quad\\bm{\\Sigma_t}=\\bm{D_t}\\bm{\\Gamma_t}\\bm{D_t}\n\\tag{12}\\]\nwhere \\(\\bm{D_t}=\\text{diag}(\\sigma_{1,t},\\sigma_{2,t},...,\\sigma_{n,t})\\) i.e. a \\(n\\times n\\) diagonal matrix with the diagonal consisting of the square roots of the conditional variances of the assets or the factor returns. These conditional variances can be modelled using one of the univariate models from section Section 3.1. Fortin, Simonato, and Dionne (2022) chose an AR(3)-NGARCH(1,1) model with the skewed-t distribution of Hansen (1994) for the innovations of all factor returns. We however tried out two different univariate models in our empirical application: a NGARCH(1,1) with skewed-t and a GARCH(1,1) with normal innovations, both without an ARMA term. Henceforward, these multivariate models will be named by first specifying the conditional correlation structure followed by the GARCH-type process used for the conditional variances e.g. DCC-GARCH or CCC-GJR.\nThe \\(n\\times n\\) conditional correlation matrix \\(\\bm{\\Gamma_t}\\) from equation Equation 12 is symmetric positive-definite and given by\n\\[\n\\bm{\\Gamma_t}\\eqdef\\mathbb{E}[\\bm{z_t}\\bm{z_t}'|\\mathcal{F}_{t-1}]=\\text{diag}(\\bm{Q_t}^{-1/2})\\,\\bm{Q_t}\\,\\text{diag}(\\bm{Q_t}^{-1/2})\n\\tag{13}\\]\nwhere \\(\\bm{z_t}\\eqdef\\bm{D_t}^{-1}(\\bm{Y_t}-\\bm{\\mu})\\) are the standardized residuals. \\(\\bm{Q_t}\\) also is a \\(n\\times n\\) symmetric positive-definite matrix that fulfills\n\\[\n\\bm{Q_t} = (1-\\alpha-\\beta)\\bar{\\bm{Q}}+\\alpha\\bm{z_{t-1}}\\bm{z_{t-1}}'+\\beta\\bm{Q_{t-1}},\n\\quad \\alpha,\\beta\\geq 0,\n\\tag{14}\\]\nwhere \\(\\bar{\\bm{Q}}\\eqdef\\frac{1}{T}\\sum_{t=1}^T\\bm{z_t}\\bm{z_t}'\\) is the \\(n\\times n\\) unconditional covariance matrix. Setting \\(\\alpha=\\beta=0\\) in equation Equation 14 yields the CCC model of Bollerslev (1990) where \\(\\bm{\\Gamma_t}=\\bm{\\Gamma}=\\bar{\\bm{Q}}\\) for all \\(t\\).\nPossibly the biggest drawback of the DCC model as specified above is the underlying assumption of multivariate normality. In particular, P. Christoffersen and Langlois (2013) discovered that ignoring the multivariate non-normality inherent in weekly factor returns leads to severe underestimation of the Expected Shortfall of an equally weighted portfolio of factors. To encompass this non-normality and the aforementioned asymmetric tail dependence of weekly equity factors, Fortin, Simonato, and Dionne (2022) stick to P. Christoffersen and Langlois (2013) and make use of so-called copulas to fit the joint conditional distribution of the factor returns. Copulas are functions that allow us to model the marginals (in our case the standardized residuals of the DCC-(N)GARCH model) individually and independently of the multivariate distribution making them incredibly flexible. The notion of Copulas is based on Sklar’s theorem which when applied to the standardized DCC-(N)GARCH residuals of our \\(n\\) factor returns states that\n\\[\n\\bm{F_t}(\\bm{z_t})=\\bm{C_t}(F_{1,t}(z_{1,t}),...,F_{n,t}(z_{n,t})),\n\\tag{15}\\]\nwhere \\(\\bm{F_t}(\\bm{z_t})\\) is the joint conditional distribution of the standardized residuals at time \\(t\\), \\(F_{i,t}(\\cdot)\\) is the conditional marginal distribution of the standardized residual of factor i at time \\(t\\) and \\(\\bm{C_t}:[0,1]^n\\rightarrow[0,1]\\) is the conditional copula that links these marginal distributions. As highlighted in Heinen and Valdesogo (2012), we can only apply Sklar’s theorem to conditional distributions if we condition all marginals and the copula on the same information. This can be done by assuming that each marginal only depends on its own past and that the copula depends on the history of all four factors (Heinen and Valdesogo 2012). As explained in e.g. Heinen and Valdesogo (2012) a copula can also be expressed as a multivariate distribution with \\(\\mathcal{U}(0,1)\\) margins\n\\[\n\\bm{C_t}(u_1,...,u_n)=\\bm{F_t}(F_{1,t}^{-1}(u_{1,t}),...,F_{n,t}^{-1}(u_{n,t})),\n\\tag{16}\\]\nwhere \\(u_{i,t}=F_{i,t}(\\epsilon_{i,t})\\) is the probability integral transform (PIT) (see Section 7.1.1) of the standardized residuals of factor i at time \\(t\\). This formulation shows that if we can sample from the copula, we can sample from the corresponding multivariate distribution \\(\\bm{F_t}(\\cdot)\\). Thus, for Monte Carlo simulations one can first generate a vector of probabilities \\(\\bm{u_t}\\eqdef(u_{1,t},...,u_{n,t})'\\) with conditional distribution \\(\\bm{C_t}(\\cdot)\\) and then apply the quantile transform (see Section 7.1.2) to these \\(u_{i,t}\\) to return a sample vector \\((F_{1,t}^{-1}(u_{1,t}),...,F_{n,t}^{-1}(u_{n,t}))'\\) from \\(\\bm{F_t}(\\cdot)\\).\nIn summary, we follow P. Christoffersen (2011, chap. 9) and apply the following scheme to simulate future factor returns:\n\nFit a DCC model with a GARCH or NGARCH model for the conditional variances and extract the standardized residuals \\(\\bm{z_t}=(z_{1,t},...,z_{n,t})'\\).\nCalculate probabilities \\(u_{i,t}=F_{i,t}(z_{i,t})\\), where \\(F_{i,t}(\\cdot)\\) is the conditional distribution that was estimated in the (N)GARCH model for the \\(i\\)’th factor.\nFit a copula to these probabilities. In our empirical application we consider the normal copula (see Section 7.2.1), the Student t copula (see Section 7.2.2) and the skewed-t copula. For more information about the Student t copula and the skewed-t copula consult (t_copula?) and (skewt_cop_R?).\nSimulate a vector of probabilities \\((\\tilde{u}_{1,t}, ..., \\tilde{u}_{n,t})'\\) from the conditional copula.\nCreate simulated standardized residuals from the simulated copula probabilities using quantile transforms: \\(\\widetilde{\\bm{z_{t}}}\\eqdef(F_{1,t}^{-1}(\\tilde{u}_{1,t}),...,F_{n,t}^{-1}(\\tilde{u}_{n,t}))'\\).\nTo create factor returns from the simulated standardized residuals we use the forecasted dynamics obtained from the DCC model:\n\n\\[\n\\widetilde{\\bm{r_{\\text{F}, t}}}=\\bm{\\mu}+\\bm{\\Sigma_{t}}^{1/2}\\,\\widetilde{\\bm{z_{t}}},\n\\tag{17}\\]\nwhere \\(\\bm{\\Sigma_{t}}^{1/2}\\) denotes the matrix square root obtained through a Cholesky decomposition of the one-period-ahead forecast of the (DCC) covariance matrix of the factor returns and \\(\\bm{\\mu}\\) the unconditional mean vector of the factor returns.\nApplying equation Equation 11 to these simulated one-period-ahead factor returns yields forecasts of the returns of the single stocks (see Section 4.2 for more information). Finally, it is straightforward to calculate the simulated one-day-ahead portfolio returns and the desired risk measures since this approach is simulation-based."
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#sec-comfort",
    "href": "posts/07-02-2025_portfolio-var/index.html#sec-comfort",
    "title": "Portfolio Value at Risk Forecasting with GARCH-Type Models",
    "section": "3.3 The COMFORT Model",
    "text": "3.3 The COMFORT Model\nIn the previous section we presented the approach of using copulas to introduce non-normality (and heterogeneous tails when using e.g. the skewed-t copula) into a CCC or DCC structure. But one could also directly choose a non-Gaussian distribution for the CCC or DCC model. One possible way to implement this is by first fitting a normal CCC or DCC model to the stock returns and then in a second step fitting a multivariate, non-normal distribution to the filtered residuals obtained in the first step (Paolella_TS_book?). But this so-called quasi maximum likelihood approach is inferior to joint maximum likelihood estimation of all parameters (Paolella_TS_book?). Consequently, Marc S. Paolella and Polak (2015) developed an efficient expectation-maximization (EM) type algorithm that allows for full maximum likelihood estimation of all parameters of the multivariate generalized hyperbolic (MGHyp) distribution. This uni-modal distribution is commonly used in finance literature and offers great flexibility (Marc S. Paolella and Polak 2022). In particular, many popular distribution choices such as the multivariate normal, the multivariate Student t, the multivariate variance gamma or the normal inverse Gaussian (and many more) can be expressed as a limiting or special case of the MGHyp (Marc S. Paolella and Polak 2022). For a more detailed breakdown of this distribution we refer to McNeil, Frey, and Embrechts (2015, chap. 6), Marc S. Paolella and Polak (2015) and Marc S. Paolella and Polak (2022). Applying the aforementioned multi-stage EM algorithm to a MGHyp distribution, for whose covariance matrix a CCC or DCC structure was used, results in the so-called Common Market Factor Non-Gaussian Returns (COMFORT) model as introduced in Marc S. Paolella and Polak (2015).\nLet \\(\\bm{Y_t}=(y_{1,t}, y_{2,t},...,y_{n,t})'\\) be an n-dimensional vector of returns of the constituents of a portfolio at time \\(t=1,...,T\\). The COMFORT model then has the form \\(\\bm{Y_t}\\sim\\text{MGHyp}(\\bm{\\mu},\\bm{\\gamma},\\bm{\\Sigma_t},\\lambda_t,\\chi_t, \\psi_t)\\) and can be expressed as a continuous normal mixture in the following way:\n\\[\n\\bm{Y_t}=\\bm{\\mu}+\\bm{\\gamma} G_t + \\bm{\\varepsilon_t}, \\qquad\\bm{\\varepsilon_t}=\\bm{\\Sigma_t}^{1/2}\\sqrt{G_t}\\bm{Z_t},\n\\tag{18}\\]\nwhere the mean vector \\(\\bm{\\mu}\\) and the asymmetry vector \\(\\bm{\\gamma}\\) are in \\(\\mathbb{R}^n\\) and \\(\\bm{Z_t}\\stackrel{iid}{\\sim}\\mathcal{N}_n(\\bm{0},\\bm{I_n})\\) (Marc S. Paolella and Polak 2022). Further, the symmetric positive-definite covariance matrix \\(\\bm{\\Sigma_t}=\\bm{D_t}\\bm{\\Gamma_t}\\bm{D_t}\\) is modelled using a CCC or DCC structure (see Section 3.2) where the matrix square root is taken by means of a Cholesky decomposition. Lastly, the mixing random variables \\(G_t|\\mathcal{F}_{t-1}\\sim \\text{GIG}(\\lambda_t,\\chi_t,\\psi_t)\\), where \\(\\mathcal{F}_{t-1} = \\{\\bm{Y_1},...,\\bm{Y_{t-1}}\\}\\) is the information set at time \\(t-1\\), follow a generalized inverse Gaussian (GIG) random variable and are independent of \\(\\bm{Z_t}\\) (Marc S. Paolella and Polak 2022). Consult (GIG?) for a detailed account on the GIG distribution or Marc S. Paolella and Polak (2022) for more information about the GIG in the COMFORT model. This sequence \\(\\{G_t\\}\\) can be interpreted as a common market factor because conditional on this common market factor, which incorporates jumps and news arrival, the returns are multivariate normal distributed (Paolella_TS_book?).\nIn our empirical application in Section 5 we will only consider the CCC augmentation in which \\(\\bm{\\Gamma_t}=\\bm{\\Gamma}\\) is time invariant. For the dynamics of the diagonal elements of \\(\\bm{D_t}\\) adjusted GARCH-type processes, which incorporate the common market factor, are used. More precisely, we will utilize the GARCH(1,1) and the GJR-GARCH(1,1) from Section 3.1 but with error term \\(\\epsilon_{i,t}=y_{i,t}-\\mu_i-\\gamma_i G_t\\) instead of \\(\\epsilon_{i,t}=y_{i,t}-\\mu_i\\) for the \\(i\\)’th portfolio constituent. Furthermore, we will only study the so-called multivariate variance-gamma (MVG) distribution which can be expressed as a MGHyp distribution with \\(\\lambda_t&gt;0\\), \\(\\chi_t=0\\) and \\(\\psi_t=2\\) for all \\(t\\) (see Marc S. Paolella and Polak (2022)). This restriction of the shape parameters \\(\\chi_t\\) and \\(\\psi_t\\) is recommended by Marc S. Paolella and Polak (2015) to circumvent potential numerical problems caused by otherwise relatively flat likelihoods.\nAn important property of the MGHyp distribution is that it is closed under linear operations as shown in McNeil, Frey, and Embrechts (2015, chap. 6). Thus, the returns of a portfolio consisting of our \\(n\\) constituents with constant portfolio weights \\(\\textbf{w}=(\\text{w}_1,\\text{w}_2,...,\\text{w}_n)'\\in\\mathbb{R}^n\\setminus\\bm{0}\\) are given by \\(r_{\\text{PF},t}=\\textbf{w}'\\bm{Y_t}\\) and are univariate GHyp distributed:\n\\[\nr_{\\text{PF},t}|\\mathcal{F}_{t-1}\\sim\\text{GHyp}(\\textbf{w}'\\bm{\\mu},\\textbf{w}'\\bm{\\gamma},\\textbf{w}'\\bm{\\Sigma_t}\\textbf{w},\\lambda_t,\\chi_t, \\psi_t).\n\\tag{19}\\]\nThis property is especially useful in portfolio optimization and risk management and will be used in Section 4.2 to estimate the VaR. We decided to include this model class because of the adequate VaR forecasts it generated in Marc S. Paolella and Polak (2022). Particularly, the empirical analysis in Marc S. Paolella and Polak (2022) showed that the VaR estimates of this model class were superior to the ones of a CCC-GARCH structure that assumed multivariate normality."
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#sec-data",
    "href": "posts/07-02-2025_portfolio-var/index.html#sec-data",
    "title": "Portfolio Value at Risk Forecasting with GARCH-Type Models",
    "section": "4.1 Data",
    "text": "4.1 Data\nThis thesis assesses models based on their ability to forecast the VaR of a long portfolio. This portfolio is equally weighted and consists of the ten large cap single stocks also used in Fortin, Simonato, and Dionne (2022). However, we consider 2767 daily returns (instead of weekly returns) which were observed from January 2, 2001 to December 30, 2011. This data is freely available on Yahoo Finance. For reasons of numerical stability we will be working with daily percentage log returns i.e. \n\\[r_t=100\\cdot\\log\\left(\\dfrac{P_t}{P_{t-1}}\\right)\\]\nwhere \\(P_t\\) is the price at time t. Additionally, we require daily returns of the factors from the Carhart four-factor model for the same time frame. These returns are from Kenneth R. French’s data library. Note that these factor returns are in percent but nominal and first have to be converted to percentage log returns. In the following, whenever returns are mentioned we will refer to daily percentage log returns. Furthermore, the VaR forecasts will be the one-step-ahead percentage log return VaR.\n\n\n\nTable 1: Summary statistics of daily factor, stock and portfolio percentage log returns\n\n\n\n\n\n\n\n\nIn Table Table 1 the summary statistics for the daily factor, stocks and portfolio returns are presented. One can see that whilst the median is larger than the mean in most instances, both are close to zero for all factors and stocks in question. In addition, the mean absolute deviation (MAD) is considerably smaller than the standard deviation indicating the presence of outliers. Three of the four factors, six of the ten stocks and the portfolio are left skewed and thus have a longer left tail. All factors, stocks and the portfolio show leptokurtic behavior i.e. their kurtosis is larger than three. This signals that the return distributions have fatter tails than a Gaussian distribution would have. Besides, we can reject the assumption of univariate normality for all of the return distributions using the extraordinarily high Jarque-Bera statistics. Additionally, we checked for multivariate normality by constructing Q-Q plots (see ) of the robust squared Mahalanobis distances of the stock returns (factor returns) and the corresponding \\(\\chi^2_d\\) distribution with \\(d=10\\) (\\(d=4\\)) degrees of freedom. The relationship in these Q-Q plots is not linear at all indicating large multivariate outliers and multivariate non-normality.\nMoreover, the portfolio returns exhibit large volatility around their mean as shown in panel A of Figure 1. Panel A further shows blatant volatility clustering indicating that the conditional variance is non-constant. In addition, it is apparent from Table Table 1 that the portfolio returns are not normally distributed. Panel B graphically displays this observation and shows that the distribution of the portfolio percentage log returns is slightly left skewed and has long tails with some extreme values on either side. Interestingly, both the smallest and the largest portfolio returns were observed in October of 2008.\n\n\n\n\n\nPlot and histogram of the percentage log-returns of the equally weighted long equity portfolio\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\nACF Plots of the Fama-French-Carhart Factors\n\n\n\n\nFigure 2\n\n\n\nDespite using daily and not weekly factor returns, the autocorrelation function (ACF) plots in Figure 2 display a similar picture as the ones in P. Christoffersen and Langlois (2013). It is evident that the factors exhibit stronger autocorrelation in absolute returns than in the returns themselves. In particular, panels A to D illustrate that whilst the ACF values for the returns mostly stay within the 95%-confidence bands, the absolute factor returns show significant autocorrelation for all lags considered. The same phenomenon can also be observed for the single stock returns and the portfolio returns (not displayed here) and can be seen as further justification for fitting a volatility model to the return series."
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#sec-var-forecasts",
    "href": "posts/07-02-2025_portfolio-var/index.html#sec-var-forecasts",
    "title": "Portfolio Value at Risk Forecasting with GARCH-Type Models",
    "section": "4.2 Value at Risk Forecasts",
    "text": "4.2 Value at Risk Forecasts\nFor all models in question we assume that the conditional mean is constant over time and thus will proceed without specifying an ARMA(p,q) process for the conditional mean. Santos, Nogales, and Ruiz (2013) took the same approach for the reason that the dynamic dependence of conditional means in daily portfolio returns is very weak if existent at all. Forecasting is done using a rolling window approach where the previous 1000 observations are used to predict the one step-ahead-VaR. The model parameters will be refit after every rolling window iteration as in Marc S. Paolella, Kuester, and Mittnik (2006). The only exception is the skewed-t copula-DCC-GARCH model where we decided to reestimate the skewed-t copula parameters only every 20 rolling windows due to the high computational burden. Additionally, there were some numerical issues for the skewed-t copula with skewed-t NGARCH marginals which we could not resolve and thus only the version with normal GARCH marginals will be included.\nFor all univariate GARCH models mentioned in Section 3.1 but the MixN(k)-GARCH we can rely on the following analytical formula to estimate the portfolio VaR:\n\\[\n\\widehat{\\text{VaR}_t^p} = -\\mu_{\\text{PF}}-\\sigma_{\\text{PF}, t} Q_p(z_t|\\mathcal{F}_{t-1})\n\\tag{20}\\]\nwhere \\(\\sigma_{\\text{PF}, t}\\) is the portfolio conditional standard deviation at time t and \\(Q_p(z_t)\\) is the p-quantile of the conditional distribution of the standardized portfolio returns, \\(z_t=(r_{\\text{PF},t}-\\mu_{\\text{PF}})/\\sigma_{\\text{PF}, t}\\). Note that formula Equation 20 includes the unconditional mean instead of the conditional one due to our mean specification. In the MixN(k)-GARCH setting the VaR forecasts are generated by the ‘MSGARCH’ package of (MSGARCH?) by applying the definition of the VaR from equation Equation 1 to the predictive mixed normal distribution of the portfolio returns.\n\n\n\nTable 2: Parameter Estimates for the Carhart Four-Factor Model\n\n\n\n\n\n\n\n\nFor the factor copula-DCC-(N)GARCH models in Section 3.2, we proceed in the same way as Fortin, Simonato, and Dionne (2022). Hence, we start off by calculating the ordinary least squares (OLS) estimates of equation Equation 11. These estimates are displayed in Table Table 2. Striking are the high Jarque-Bera statistics for the OLS residuals indicating univariate non-normality. Further analysis of the OLS residuals shows that the residuals are slightly left skewed and highly leptokurtic. Moreover, we can see in, which looks very similar to Panel B of, that the OLS residuals clearly are not multivariate normal either. Additionally, residuals of firms from similar sectors tend to be stronger correlated than the residuals of those from different sectors.\nNext, we follow the procedure described in Section 3.2 to simulate a vector \\(\\tilde{\\bm{r}}_{\\text{F},t}\\). We then simulate from the distribution of the OLS residuals \\(\\bm{F_\\varepsilon}(\\cdot)\\) by randomly drawing a bootstrap sample vector \\(\\bm{\\tilde{\\varepsilon}}_t=[\\tilde{\\varepsilon}_{1,t}, \\tilde{\\varepsilon}_{2,t},..., \\tilde{\\varepsilon}_{10,t}]\\), \\(t=1,2,...,T\\). It is important to note that, as pointed out in P. Christoffersen (2009), each draw of \\(\\bm{F_\\varepsilon}(\\cdot)\\) has to be a vector of error terms from the same day so that aforementioned dependencies between the OLS residuals of different stock returns are maintained. Further, we have to pay attention to data leakage and can therefore only use OLS residuals which have already been observed i.e. are part of the rolling window that is used for time \\(t=1,2,...,T\\). Using equation Equation 11 then yields the simulated return of single stock k:\n\\[\n\\tilde{r}_{k,t}=r_{f,t}+\\alpha_{k,t}+\\bm{\\beta}_{k}'\\tilde{\\bm{r}}_{\\text{F},t}+\\bm{\\tilde{\\varepsilon}_{k,t}}.\n\\tag{21}\\]\nThis procedure is repeated \\(N=200'000\\) times. For each simulation we calculate the simulated return of the equal weighted portfolio. This yields a simulated portfolio return sample \\(\\{r_{\\text{PF}}^n\\}_{n=1}^N\\). Finally, the VaR estimate is obtained as the negative \\(p\\)-quantile of the simulated portfolio returns:\n\\[\n\\widehat{\\text{VaR}_t^p}=-Q_p(\\{r_{\\text{PF}}^n\\}_{n=1}^N).\n\\tag{22}\\]\nLastly, in the COMFORT framework we make use of the fact that when assuming a MGHyp distribution for \\(\\bm{Y_t}\\) (i.e. the returns of the constituents) the corresponding portfolio returns \\(r_{\\text{PF},t}=\\textbf{w}'\\bm{Y}\\) are univariate GHyp distributed (see Section 3.3). Thus, according to Marc S. Paolella and Polak (2022) the VaR forecast can be calculated using the \\(p\\)-quantile function \\(Q_p(\\cdot)\\) of the corresponding conditional univariate GHyp distribution (in our case the univariate Variance Gamma distribution) of the portfolio returns:\n\\[\n\\widehat{\\text{VaR}_t^p}=-Q_p(r_{\\text{PF},t}|\\mathcal{F}_{t-1})=-\\inf_x\\{x\\in\\mathbb{R}:\\mathbb{P}(r_{\\text{PF},t}\\leq x|\\mathcal{F}_{t-1})\\geq p\\},\n\\tag{23}\\]\nwhere \\(\\mathcal{F}_{t-1}=\\{\\bm{Y_1},...,\\bm{Y_{t-1}}\\}\\) is the information set at time \\(t-1\\)."
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#sec-backtesting",
    "href": "posts/07-02-2025_portfolio-var/index.html#sec-backtesting",
    "title": "Portfolio Value at Risk Forecasting with GARCH-Type Models",
    "section": "4.3 Backtesting",
    "text": "4.3 Backtesting\nBacktesting is used to check whether the forecasts of the models evince some desirable properties. In doing so, we will follow the framework of P. F. Christoffersen (1998) which consists of three likelihood-ratio tests. Let \\(I_t\\) be the indicator variable for a \\(\\text{VaR}_t^p\\) forecast made at time t-1, that is,\n\\[\nI_t=\\mathbb{1}_{\\{r_{\\text{PF},t}&lt;-\\text{VaR}_t^p\\}}=\\begin{cases}\n1 & \\text{if } r_{\\text{PF},t}&lt;-\\text{VaR}_t^p\\\\\n0 & \\text{otherwise}\\\\\n\\end{cases}\n\\tag{24}\\]\nAccording to P. F. Christoffersen (1998), a sequence of value at risk forecasts is then said to be efficient with respect to \\(\\mathcal{F}_{t-1}\\), the information set at time \\(t-1\\), if\n\\[\n\\mathbb{E}[I_t|\\mathcal{F}_{t-1}]=\\mathbb{E}[I_t|I_{t-1},I_{t-2},...,I_1]=p,\\quad t=1,2,...,T.\n\\tag{25}\\]\nP. F. Christoffersen (1998) shows that testing whether a sequence of value at risk forecasts is efficient is equivalent to testing that \\(\\{I_t\\}_{t=1}^T \\overset{\\mathrm{iid}}{\\sim}\\text{Bernoulli}(p)\\). In case this property is fulfilled, the VaR forecast has correct conditional coverage. Although one could test for correct conditional coverage directly we will follow Marc S. Paolella, Kuester, and Mittnik (2006) and additionally provide intermediate test statistics which help to better understand deficiencies of the models.\nFirst, we test for the correct number of exceedances under independence. The corresponding hypothesis of correct conditional coverage can be written as\n\\[H_0:\\mathbb{E}[I_t]=p \\quad\\text{versus}\\quad H_A:\\mathbb{E}[I_t]\\neq p,\\]\nwhich is the result of applying the law of total expectation on equation Equation 25. Let \\(n_1\\) be the number of violations i.e. ones in the indicator sequence \\(\\{I_t\\}_{t=1}^T\\) and \\(n_0\\) the number of zeros in the indicator sequence. The likelihood-ratio test statistic then is\n\\[\nLR_{uc}=-2\\log\\bigg(\\dfrac{L(p;I_1,I_2,...,I_T)}{L(\\hat{p};I_1,I_2,...,I_T)}\\bigg)\\overset{\\mathrm{asy}}{\\sim}\\chi_1^2,\n\\tag{26}\\]\nwhere \\(\\hat{p}=\\hat{p}_{\\text{ML}} = \\frac{n_1}{n_0+n_1}\\) is the maximum likelihood estimate of \\(p\\) and \\(L(\\cdot)\\) is the corresponding likelihood of a \\(\\mathcal{B}in(n_0+n_1,p)\\) distribution (P. F. Christoffersen 1998)."
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#proofs",
    "href": "posts/07-02-2025_portfolio-var/index.html#proofs",
    "title": "Portfolio Value at Risk Forecasting with GARCH-Type Models",
    "section": "7.1 Proofs",
    "text": "7.1 Proofs\n\n7.1.1 Probability Integral Transform\n\n\n7.1.2 Quantile Transform"
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#copulas",
    "href": "posts/07-02-2025_portfolio-var/index.html#copulas",
    "title": "Portfolio Value at Risk Forecasting with GARCH-Type Models",
    "section": "7.2 Copulas",
    "text": "7.2 Copulas\n\n7.2.1 Gaussian Copula\n\n\n7.2.2 Student t Copula"
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#distributions",
    "href": "posts/07-02-2025_portfolio-var/index.html#distributions",
    "title": "Portfolio Value at Risk Forecasting with GARCH-Type Models",
    "section": "7.3 Distributions",
    "text": "7.3 Distributions"
  }
]