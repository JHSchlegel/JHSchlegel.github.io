[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nComparing Univariate and Multivariate Models for Value at Risk Forecasting\n\n\n\n\n\n\nSimulation\n\n\nR\n\n\n\nThis post explores the effectiveness of univariate and multivariate GARCH-based models in forecasting Value at Risk (VaR) for a long equity portfolio. While multivariate models generally perform better in backtests, univariate models often fall short. However, neither model type consistently outperforms the other in predictive accuracy, highlighting the trade-offs between simplicity and complexity in risk forecasting.\n\n\n\n\n\nFeb 7, 2025\n\n\nJan Schlegel\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jan Schlegel",
    "section": "",
    "text": "Hi there! I‚Äôm Jan, a Master‚Äôs student in Statistics at ETH Zurich with a passion for machine learning, healthcare applications, and probabilistic modeling. My journey combines rigorous academic training in statistics with hands-on experience in research and data analysis. I thrive on solving complex problems and am particularly interested in the intersection of machine learning and healthcare. With a strong foundation in both theoretical statistics and practical implementation, I‚Äôm always eager to tackle new challenges and contribute to meaningful projects. Currently seeking opportunities in machine learning and data science - let‚Äôs connect!"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Jan Schlegel",
    "section": "Education",
    "text": "Education\n\n\nüéì Master of Science (M.Sc. ETH) in Statistics\n\nüèõÔ∏è ETH Zurich | üìç Zurich, Switzerland | üìÖ Sep 2023 - Sep 2025\n\n\nGPA: 5.98 / 6.00\n\n\nRelevant Coursework: Deep Learning; Image Analysis and Computer Vision; Machine Learning for Healthcare; Probabilistic AI; Statistical Learning Theory; Introduction to Machine Learning; Causality; Computational Statistics; Fundamentals of Mathematical Statistics; High Dimensional Statistics; Bayesian Statistics; Time Series Analysis\nSemester Paper: Extrapolation and Distributional Robustness for Climate Downscaling\nMentor for New Students\n\n\n\nüéì Special Student Statistics\n\nüèõÔ∏è ETH Zurich | üìç Zurich, Switzerland | üìÖ Feb 2022 - Feb 2023\n\n\nGPA: 6.00 / 6.00\n\n\nVoluntarily enrolled to take advanced courses in statistics\nCoursework: Applied Time Series, Applied Multivariate Statistics\n\n\n\nüéì Bachelor of Arts in Business and Economics\n\nüèõÔ∏è University of Zurich (UZH) | üìç Zurich, Switzerland | üìÖ Sep 2019 - Feb 2023\n\n\nGPA: 5.87 / 6.00\n\n\nFinished the degree with 215 ECTS (instead of 180 ECTS)\nMajor in Banking and Finance (150 ECTS)\nMinor in Applied Probability and Statistics (30 ECTS)\nRelevant Coursework: Likelihood Inference; Statistical Modelling; Numerical Methods in Informatics; Introduction to Machine Learning; Introduction to Statistics; Introductory Econometrics Thesis: Portfolio Value at Risk Forecasting with GARCH-Type Models (Grade: 6.00/6.00)"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Jan Schlegel",
    "section": "Experience",
    "text": "Experience\n\n\nResearch Assistant (Civilian Service)\n\nüèõÔ∏è University of Zurich (EBPI) | üìç Zurich, Switzerland | üìÖ Feb 2022 - Aug 2023\n\n\nConducted sophisticated data analysis and statistical modeling using R for epidemiological research\nCollaborated with interdisciplinary team of epidemiologists, statisticians, and public health experts\nContributed to research paper published in Nature Communications (2023): ‚ÄúPersistent humoral immune response in youth throughout the COVID-19 pandemic: prospective school-based cohort study‚Äù\n\n\n\nAccountant (Military Service)\n\nüèõÔ∏è Swiss Armed Forces | üìç Kloten, Switzerland | üìÖ Jul 2021 - Nov 2021\n\n\nSuccessfully balanced military service obligations while maintaining full-time academic studies\nDemonstrated exceptional time management and organizational skills\n\n\n\nTeaching Assistant\n\nüèõÔ∏è University of Zurich | üìç Zurich, Switzerland | üìÖ Sep 2020 - Jul 2021\n\n\nConducted bi-weekly interactive exercise classes in Analysis and Linear Algebra for first-year students\nEffectively communicated complex mathematical concepts to diverse student groups\nDeveloped strong presentation and educational communication skills"
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html",
    "href": "posts/07-02-2025_portfolio-var/index.html",
    "title": "Comparing Univariate and Multivariate Models for Value at Risk Forecasting",
    "section": "",
    "text": "Value at Risk (VaR) is a crucial metric in modern financial risk management"
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#sec-introduction",
    "href": "posts/07-02-2025_portfolio-var/index.html#sec-introduction",
    "title": "Comparing Univariate and Multivariate Models for Value at Risk Forecasting",
    "section": "1 Introduction",
    "text": "1 Introduction\nThe past four decades have been shaped by extreme events in financial markets, such as the Black Monday crash, the dot-com bubble, and the 2008 global financial crisis. These supposedly rare events highlighted that reducing systemic risk is crucial for financial stability (Embrechts et al. 2014). This led to the introduction and tightening of the Basel Accords, which use risk measures to determine appropriate risk capital requirements for financial institutions.\nValue at Risk (VaR) remains the most popular measure for downside market risk, despite a shift towards the severity-based expected shortfall (ES) (Embrechts et al. 2014). For a long equity portfolio, the \\(p\\)% VaR for period \\(t\\) forecasted at time \\(t-1\\) is defined as the negative \\(p\\)-quantile of the conditional portfolio return distribution:\n\\[\n\\text{VaR}_t^p=-Q_p(r_{\\text{PF},t}|\\mathcal{F}_{t-1})=-\\inf_x\\{x\\in\\mathbb{R}:\\mathbb{P}(r_{\\text{PF},t}\\leq x|\\mathcal{F}_{t-1})\\geq p\\},\\quad p\\in(0,1).\n\\tag{1}\\]\nHere, \\(Q_p(\\cdot)\\) denotes the quantile function and \\(\\mathcal{F}_{t-1}\\) represents all information available at time \\(t-1\\). The parameter \\(p\\) indicates that with target probability \\(p\\), the portfolio losses will exceed the VaR (Marc S. Paolella, Kuester, and Mittnik 2006).\nDue to the practical relevance of VaR, it is essential to determine estimation methods that neither severely underestimate nor overestimate future losses. Many models use the generalized autoregressive conditional heteroskedasticity (GARCH) framework (Bollerslev 1986) or extensions to account for volatility clustering and the ‚Äúleverage effect‚Äù in financial time series.\nA fundamental question in VaR modeling is whether more complex multivariate models outperform simpler univariate alternatives. Santos, Nogales, and Ruiz (2013) found that multivariate models significantly outperform univariate counterparts for large portfolios, while Kole et al. (2017) found that multivariate models have greater predictive ability, though differences are often not significant. They also found that data frequency is more important than model choice.\nThis study compares factor copula-DCC-NGARCH models introduced by Fortin, Simonato, and Dionne (2022) with established models like the diagonal MixN(k)-GARCH (Haas, Mittnik, and Paolella 2004) and the COMFORT model class (Marc S. Paolella and Polak 2015). We show that multivariate models display desirable VaR properties in terms of correct unconditional coverage and independence of violations, though we don‚Äôt find sufficient evidence to claim that multivariate approaches outperform univariate procedures in terms of forecast ability, or vice versa."
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#sec-methodology",
    "href": "posts/07-02-2025_portfolio-var/index.html#sec-methodology",
    "title": "Comparing Univariate and Multivariate Models for Value at Risk Forecasting",
    "section": "2 Methodology",
    "text": "2 Methodology\n\n2.1 Univariate Models\nFor univariate models, we assume the following portfolio return dynamics:\n\\[\nr_{\\text{PF},t} = \\mu + \\epsilon_t\n\\]\nwhere\n\\[\n\\epsilon_t = \\sigma_t z_t, \\quad z_t \\stackrel{iid}{\\sim} F(0,1)\n\\]\nHere, \\(F(0,1)\\) is a standardized distribution, \\(\\mu\\) is the unconditional location, and \\(\\sigma_t\\) is the scale parameter. The conditional variance \\(\\sigma_t^2 = \\mathbb{V}[r_{\\text{PF},t}|\\mathcal{F}_{t-1}]\\) is modeled as non-constant.\n\n2.1.1 GARCH and Extensions\nThe standard GARCH(1,1) model (Bollerslev 1986) is formulated as:\n\\[\n\\sigma_t^2 = \\omega + \\alpha\\epsilon_{t-1}^2 + \\beta\\sigma_{t-1}^2\n\\tag{2}\\]\nwhere \\(\\omega &gt; 0, \\alpha \\geq 0\\), and \\(\\beta \\geq 0\\). For covariance stationarity, the parameters must satisfy \\(\\alpha + \\beta &lt; 1\\).\nA special case is the exponentially weighted moving average (EWMA):\n\\[\n\\sigma^2_t = \\lambda\\sigma^2_{t-1} + (1-\\lambda)\\epsilon_t^2, \\quad \\lambda \\in (0,1)\n\\tag{3}\\]\nThis formulation puts more weight on recent observations, but is not covariance stationary since \\(\\lambda + (1-\\lambda) = 1\\).\nTo account for the ‚Äúleverage effect‚Äù (negative news increasing volatility more than positive news of equal magnitude), we include the GJR-GARCH(1,1) model (Glosten, Jagannathan, and Runkle 1993):\n\\[\n\\sigma_t^2 = \\omega + (\\alpha + \\gamma I_{t-1})\\epsilon_{t-1}^2 + \\beta\\sigma_{t-1}^2\n\\tag{4}\\]\nwhere \\(I_{t-1} = \\mathbb{I}_{\\{\\epsilon_{t-1} &lt; 0\\}}\\) is an indicator function.\nAnother asymmetric model is the NGARCH(1,1) (R. F. Engle and Ng 1993):\n\\[\n\\sigma_t^2 = \\omega + \\alpha\\sigma_{t-1}^2(\\epsilon_{t-1} - \\theta)^2 + \\beta\\sigma_{t-1}^2\n\\]\nFor \\(\\theta &gt; 0\\), negative innovations have a larger impact on conditional variance than positive errors of the same magnitude.\n\n\n2.1.2 Mixed Normal GARCH\nWe also include the k-component mixed normal GARCH(1,1) (MixN(k)-GARCH) (Haas, Mittnik, and Paolella 2004), where the conditional distribution of the error term \\(\\epsilon_t\\) is assumed to be mixed normal with zero mean:\n\\[\n\\epsilon_t|\\mathcal{F}_{t-1} \\sim \\text{Mix}_k\\text{N}(p_1,...,p_k, \\mu_1,...,\\mu_k, \\sigma_{1,t}^2,...,\\sigma_{k,t}^2), \\quad \\sum_{i=1}^k p_i\\mu_i = 0\n\\]\nThe associated conditional variances follow GARCH processes:\n\\[\n\\sigma_{i,t}^2 = \\omega_i + \\alpha_i\\epsilon_{i,t-1}^2 + \\beta_i\\sigma_{i,t-1}^2, \\quad i=1,...,k\n\\]\n\n\n\n2.2 Multivariate Models\n\n2.2.1 Factor Copula-DCC-GARCH Model\nThe factor copula model proposed by Fortin, Simonato, and Dionne (2022) uses equity factors to capture the main risks of stock returns. It utilizes the Carhart four-factor model (Carhart 1997), which adds a momentum factor to the Fama-French three-factor model (Fama and French 1993):\n\\[\nr_{k,t} - r_{f,t} = \\alpha_{k,t} + \\beta_{k, \\text{RMRF}}\\text{RMRF}_t + \\beta_{k,\\text{SMB}}\\text{SMB}_t + \\beta_{k,\\text{HML}}\\text{HML}_t + \\beta_{k, \\text{MOM}}\\text{MOM}_t + \\varepsilon_{k,t}\n\\tag{5}\\]\nor in vector form:\n\\[\nr_{k,t} - r_{f,t} = \\alpha_{k,t} + \\mathbf{\\beta}_k'\\mathbf{r}_{\\text{F},t} + \\varepsilon_{k,t}\n\\tag{6}\\]\nThis reduces dimensionality by modeling only four factors instead of all portfolio constituents.\nFor the factor dynamics, we use the Dynamic Conditional Correlation (DCC) structure (R. Engle 2002), which decomposes the conditional covariance matrix into standard deviations and correlations:\n\\[\n\\mathbf{Y}_{t}|\\mathcal{F}_{t-1} \\sim \\mathcal{N}_n(\\mathbf{\\mu}, \\mathbf{\\Sigma_t}), \\quad \\mathbf{\\Sigma_t} = \\mathbf{D_t}\\mathbf{\\Gamma_t}\\mathbf{D_t}\n\\tag{7}\\]\nwhere \\(\\mathbf{D_t} = \\text{diag}(\\sigma_{1,t},\\sigma_{2,t},...,\\sigma_{n,t})\\) contains the conditional standard deviations.\nTo account for non-normality, we use copulas to model the joint conditional distribution of factor returns. Copulas allow modeling marginals independently of the multivariate distribution. By Sklar‚Äôs theorem:\n\\[\n\\mathbf{F_t}(\\mathbf{z_t}) = \\mathbf{C_t}(F_{1,t}(z_{1,t}),...,F_{n,t}(z_{n,t}))\n\\tag{8}\\]\nwhere \\(\\mathbf{F_t}(\\mathbf{z_t})\\) is the joint conditional distribution, \\(F_{i,t}(\\cdot)\\) are the conditional marginals, and \\(\\mathbf{C_t}:[0,1]^n \\rightarrow [0,1]\\) is the conditional copula.\n\n\n2.2.2 COMFORT Model\nThe Common Market Factor Non-Gaussian Returns (COMFORT) model (Marc S. Paolella and Polak 2015) uses a multivariate generalized hyperbolic (MGHyp) distribution with a CCC or DCC structure for the covariance matrix. This model can be expressed as a continuous normal mixture:\n\\[\n\\mathbf{Y_t} = \\mathbf{\\mu} + \\mathbf{\\gamma} G_t + \\mathbf{\\varepsilon_t}, \\quad \\mathbf{\\varepsilon_t} = \\mathbf{\\Sigma_t}^{1/2}\\sqrt{G_t}\\mathbf{Z_t}\n\\tag{9}\\]\nwhere \\(\\mathbf{Z_t} \\stackrel{iid}{\\sim} \\mathcal{N}_n(\\mathbf{0},\\mathbf{I_n})\\) and the mixing random variables \\(G_t|\\mathcal{F}_{t-1} \\sim \\text{GIG}(\\lambda_t,\\chi_t,\\psi_t)\\) follow a generalized inverse Gaussian distribution.\nAn important property of the MGHyp distribution is that it is closed under linear operations. Therefore, portfolio returns \\(r_{\\text{PF},t} = \\mathbf{w}'\\mathbf{Y_t}\\) are univariate GHyp distributed:\n\\[\nr_{\\text{PF},t}|\\mathcal{F}_{t-1} \\sim \\text{GHyp}(\\mathbf{w}'\\mathbf{\\mu},\\mathbf{w}'\\mathbf{\\gamma},\\mathbf{w}'\\mathbf{\\Sigma_t}\\mathbf{w},\\lambda_t,\\chi_t, \\psi_t)\n\\tag{10}\\]\n\n\n\n2.3 Data\nWe use an equally weighted portfolio of ten large-cap stocks (identical to those used by Fortin, Simonato, and Dionne (2022)): Boeing, Caterpillar, Chevron, Coca-Cola, Exxon, GE, IBM, Merck, P&G, and UTC. However, we analyze 2,767 daily returns from January 2, 2001, to December 30, 2011, rather than weekly returns.\nThe return data shows that most factors and stocks have means close to zero, with the median larger than the mean in most cases. The mean absolute deviation (MAD) is considerably smaller than the standard deviation, indicating the presence of outliers. Most returns are left-skewed with leptokurtic behavior, and all return distributions reject the assumption of normality based on Jarque-Bera statistics.\n\n\n\n\n\n\n\n\nFigure¬†1: ACF Plots of the Fama-French-Carhart Factors\n\n\n\n\n\nFigure Figure¬†1 shows autocorrelation function (ACF) plots for the factors. It is evident that the factors exhibit stronger autocorrelation in absolute returns than in the returns themselves, justifying the use of volatility models.\n\n\n\n\n\n\n\n\nFigure¬†2: Chi-Square Q-Q Plots of the Stock and Factor Returns\n\n\n\n\n\nFigure Figure¬†2 shows Q-Q plots of the robust squared Mahalanobis distances against \\(\\chi^2\\) distributions. The non-linear relationship indicates large multivariate outliers and multivariate non-normality.\n\n\n\n\n\n\n\n\nFigure¬†3: Portfolio Returns\n\n\n\n\n\nFigure Figure¬†3 demonstrates blatant volatility clustering (Panel A) and that the portfolio returns are not normally distributed (Panel B).\n\n\n2.4 Value at Risk Forecasts\nFor all models, we assume a constant conditional mean over time. Forecasting uses a rolling window approach with the previous 1,000 observations to predict the one-step-ahead VaR.\nFor univariate GARCH models (except MixN(k)-GARCH), we use the analytical formula:\n\\[\n\\widehat{\\text{VaR}_t^p} = -\\mu_{\\text{PF}} - \\sigma_{\\text{PF}, t} Q_p(z_t|\\mathcal{F}_{t-1})\n\\tag{11}\\]\nwhere \\(\\sigma_{\\text{PF}, t}\\) is the conditional standard deviation and \\(Q_p(z_t)\\) is the p-quantile of the standardized returns.\nFor the factor copula-DCC-(N)GARCH models, we simulate factor returns, apply the Carhart model to generate single stock returns, and calculate the portfolio return. The VaR estimate is then the negative p-quantile of the simulated portfolio returns.\nFor the COMFORT model, we use the p-quantile function of the corresponding conditional univariate GHyp distribution:\n\\[\n\\widehat{\\text{VaR}_t^p} = -Q_p(r_{\\text{PF},t}|\\mathcal{F}_{t-1})\n\\tag{12}\\]\n\n\n2.5 Backtesting\nBacktesting checks whether the forecasts exhibit desirable properties. Following Christoffersen (1998), we use three likelihood-ratio tests.\nLet \\(I_t\\) be the indicator variable for a \\(\\text{VaR}_t^p\\) forecast:\n\\[\nI_t = \\mathbb{I}_{\\{r_{\\text{PF},t} &lt; -\\text{VaR}_t^p\\}} =\n\\begin{cases}\n1 & \\text{if } r_{\\text{PF},t} &lt; -\\text{VaR}_t^p \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nA sequence of VaR forecasts is efficient with respect to \\(\\mathcal{F}_{t-1}\\) if:\n\\[\n\\mathbb{E}[I_t|\\mathcal{F}_{t-1}] = \\mathbb{E}[I_t|I_{t-1},I_{t-2},...,I_1] = p, \\quad t=1,2,...,T\n\\tag{13}\\]\nThis is equivalent to testing that \\(\\{I_t\\}_{t=1}^T \\overset{\\text{iid}}{\\sim} \\text{Bernoulli}(p)\\).\n\n2.5.1 Unconditional Coverage Test\nThis tests whether the expected value of \\(I_t\\) equals \\(p\\):\n\\[\nH_0: \\mathbb{E}[I_t] = p \\quad \\text{versus} \\quad H_A: \\mathbb{E}[I_t] \\neq p\n\\]\nThe likelihood-ratio test statistic is:\n\\[\nLR_{uc} = -2\\log\\left(\\frac{L(p;I_1,I_2,...,I_T)}{L(\\hat{p};I_1,I_2,...,I_T)}\\right) \\overset{\\text{asy}}{\\sim} \\chi_1^2\n\\tag{14}\\]\nwhere \\(\\hat{p} = \\frac{n_1}{n_0+n_1}\\) is the maximum likelihood estimate of \\(p\\).\n\n\n2.5.2 Independence Test\nThis tests whether the indicator sequence is independently distributed, against a first-order Markov chain alternative:\n\\[\nLR_{ind} = -2\\log\\left(\\frac{L(\\hat{\\Pi}_2;I_2,...,I_T|I_1)}{L(\\hat{\\Pi}_1;I_2,...,I_T|I_1)}\\right) \\overset{\\text{asy}}{\\sim} \\chi_1^2\n\\tag{15}\\]\nwhere \\(\\hat{\\Pi}_1\\) and \\(\\hat{\\Pi}_2\\) are the estimated transition probability matrices under the alternative and null hypotheses.\n\n\n2.5.3 Conditional Coverage Test\nThis combines the unconditional coverage and independence tests:\n\\[\nLR_{cc} = -2\\log\\left(\\frac{L(p;I_2,...,I_T|I_1)}{L(\\hat{\\Pi}_1;I_2,...,I_T|I_1)}\\right) \\overset{\\text{asy}}{\\sim} \\chi_2^2\n\\tag{16}\\]\nIt can also be calculated as:\n\\[\nLR_{cc} = LR_{uc} + LR_{ind}\n\\]\n\n\n\n2.6 Comparison of Predictive Ability\nTo rank VaR estimates, we use the ‚Äútick‚Äù loss function:\n\\[\nL_V(\\theta_t, r_{\\text{PF},t}) = (r_{\\text{PF},t} + \\theta_t)(p - \\mathbb{I}_{\\{r_{\\text{PF},t} &lt; -\\theta_t\\}})\n\\]\nThis loss function has the property that:\n\\[\nQ_p(r_{\\text{PF},t}|\\mathcal{F}_{t-1}) = -\\text{VaR}_t^p = \\arg\\min_{\\theta_t} \\mathbb{E}[L_V(\\theta_t, r_{\\text{PF},t})]\n\\tag{17}\\]\nFor statistical inference, we use the conditional predictive ability (CPA) test (Giacomini and White 2006). The null hypothesis of equal conditional predictive ability is:\n\\[\nH_0: \\mathbb{E}[\\Delta L_{i,j,t}|\\mathcal{F}_{t-1}] = 0 \\quad \\text{almost surely } \\forall t\n\\]\nwhere \\(\\Delta L_{i,j,t} = L_{V_{i,t}} - L_{V_{j,t}}\\) is the loss differential between models i and j.\nThe test statistic is:\n\\[\nGW_{i,j} = T\\bar{Z}'\\hat{\\Omega}\\bar{Z} \\overset{d}{\\rightarrow} \\chi^2_q \\quad \\text{as } T \\rightarrow \\infty\n\\]\nwhere \\(Z_t = h_{t-1}\\Delta L_{i,j,t}\\), \\(\\bar{Z} = \\frac{1}{T}\\sum_{t=2}^T Z_t\\), and \\(\\hat{\\Omega} = \\frac{1}{T}\\sum_{t=2}^T Z_t Z_t'\\)."
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#sec-results",
    "href": "posts/07-02-2025_portfolio-var/index.html#sec-results",
    "title": "Comparing Univariate and Multivariate Models for Value at Risk Forecasting",
    "section": "3 Results",
    "text": "3 Results\n\n3.1 Value at Risk Backtests\n\n\n\n\n\n\n\n\nFigure¬†4: Chi-Square Q-Q Plot of the Carhart OLS Residuals\n\n\n\n\n\nFigure Figure¬†4 shows the Q-Q plot of OLS residuals, clearly indicating their non-normality.\nTable 1 shows that while all univariate models passed the likelihood ratio test of independence, only a few showed adequate conditional or unconditional coverage. In particular, only the skewed-t GJR-GARCH passed the test of conditional coverage for both VaR levels. In contrast, most multivariate models passed all likelihood ratio tests, with the exception of the multivariate normal (MN)-DCC-GARCH.\nIt‚Äôs notable that using skewed-t NGARCH marginals in the factor copula framework leads to a very low percentage of violations compared to other models. Surprisingly, even using a normal copula with normal GARCH marginals for factor returns produces sound VaR estimates, possibly because much of the multivariate non-normality of stock returns might be captured in the bootstrapped OLS residuals.\nModels assuming normality of returns (GARCH, EWMA, and MN-DCC-GARCH) consistently show too many violations, with none displaying adequate conditional or unconditional coverage. For these models, the percentage of violations is higher than the corresponding VaR level. For the COMFORT and factor copula models, however, we observe fewer violations than expected.\nThe most adequate coverage is achieved by the factor copula-DCC models with GARCH marginals for the 1% VaR level and by the COMFORT models for the 5% level. The factor skewed-t copula with GARCH marginals belongs to the three models with the most appropriate coverage for both VaR percentiles.\nNearly all exceedances for the COMFORT models occurred during or after the 2008 financial crisis, despite passing the independence test. At the 1% VaR level, Gaussian models without leverage effects have approximately three times as many exceedances as expected. At the 5% VaR level, this discrepancy is smaller.\n\n\n3.2 Conditional Predictive Ability Tests\nIn terms of average tick loss, univariate models perform well despite their suboptimal backtesting results. The skewed-t GJR model had the lowest and the MN-DCC-GARCH the highest average loss for both VaR levels. Multivariate models achieve better ranks at the 1% level than at the 5% level, with factor copula-based models showing lower average loss than COMFORT models at both levels.\nWithin the factor copula-DCC models, GARCH marginals achieved lower mean losses than skewed-t NGARCH marginals, reinforcing the hypothesis that bootstrapped OLS residuals account for much of the non-normality in stock returns.\nThe CPA test results show that the MN-DCC-GARCH is significantly outperformed by every other model. Most rejections occur in univariate vs.¬†univariate or multivariate vs.¬†multivariate comparisons. For multivariate models, factor copula-DCC models using skewed-t NGARCH marginals have significantly higher predictive ability than their counterparts with normal GARCH marginals. The normal copula is superior for skewed-t NGARCH marginals, but for normal GARCH marginals, the t and skewed-t copula versions significantly outperform the normal copula.\nAt the 5% VaR level, in addition to MN-DCC-GARCH, the MixN(3)-GARCH is also significantly outperformed by all other models. The Student t GJR-GARCH, skewed-t GJR-GARCH, and MixN(2)-GARCH all display significantly higher predictive ability than the COMFORT models.\nInterestingly, the skewed-t GJR-GARCH, which significantly outperformed every other univariate model, did not achieve significantly better VaR forecasts than the factor copula-DCC-(N)GARCH models."
  },
  {
    "objectID": "posts/07-02-2025_portfolio-var/index.html#sec-conclusion",
    "href": "posts/07-02-2025_portfolio-var/index.html#sec-conclusion",
    "title": "Comparing Univariate and Multivariate Models for Value at Risk Forecasting",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nOur study assessed various univariate and multivariate models for VaR forecasting. Most univariate models produced inadequate VaR estimates with too many violations, while most multivariate models displayed adequate coverage with independently occurring VaR exceedances.\nThe CPA tests revealed that the MN-DCC-GARCH model is significantly outperformed by all other models, which is expected given the evident multivariate non-normality of stock returns. However, we found no general, significant outperformance of multivariate models over univariate ones, or vice versa, at either VaR level.\nOne important finding is that using daily returns (higher frequency data) makes the factor copula-DCC-NGARCH models feasible for VaR forecasting, consistent with Kole et al. (2017) who found that data frequency is more important than model choice for VaR forecasts.\nWe also found that replacing skewed-t NGARCH marginals with normal GARCH marginals for factor returns increased predictive accuracy and yielded better unconditional coverage. This may be because OLS residuals from the Carhart model capture most of the multivariate non-normality in stock returns.\nFor future research, it would be interesting to examine how the factor copula-DCC-GARCH model performs with larger portfolios, which would highlight the advantage of its dimensionality reduction. The only computationally expensive parts‚Äîfitting the DCC-GARCH structure and the copula‚Äîdepend only on the number of factors, not the portfolio size."
  },
  {
    "objectID": "temp.html",
    "href": "temp.html",
    "title": "Jan Schlegel",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\nimport matplotlib as mpl\n\n# Set the style to be clean and professional\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nmpl.rcParams[\"font.family\"] = \"Arial\"\nmpl.rcParams[\"axes.grid\"] = True\nmpl.rcParams[\"grid.alpha\"] = 0.3\nmpl.rcParams[\"grid.linestyle\"] = \"--\"\n\n# Parameters\nnp.random.seed(42)  # For reproducibility\nnum_paths = 1000\ntime_steps = 252  # Trading days in a year\nT = 1.0  # 1 year\ndt = T / time_steps\nmu = 0.05  # Drift (annual return)\nsigma = 0.2  # Volatility (annual)\nS0 = 100  # Initial price\nvar_percentile = 5  # 5% Value at Risk\n\n\n# Simulate Brownian Motion paths\ndef simulate_gbm(S0, mu, sigma, T, N, num_paths):\n    \"\"\"\n    Simulate geometric Brownian motion paths\n    S0: initial value\n    mu: drift\n    sigma: volatility\n    T: time horizon\n    N: number of time steps\n    num_paths: number of paths to simulate\n    \"\"\"\n    dt = T / N\n    paths = np.zeros((num_paths, N + 1))\n    paths[:, 0] = S0\n\n    for t in range(1, N + 1):\n        z = np.random.standard_normal(num_paths)\n        paths[:, t] = paths[:, t - 1] * np.exp(\n            (mu - 0.5 * sigma**2) * dt + sigma * np.sqrt(dt) * z\n        )\n\n    return paths\n\n\n# Time points\ntime_points = np.linspace(0, T, time_steps + 1)\n\n# Generate paths\npaths = simulate_gbm(S0, mu, sigma, T, time_steps, num_paths)\n\n# Calculate 5% VaR at each time point\nvar_index = int((var_percentile / 100) * num_paths)\nsorted_paths = np.sort(paths, axis=0)\nvar_path = sorted_paths[var_index, :]\n\n# Calculate the final VaR loss percentage\nfinal_mean = paths[:, -1].mean()\nfinal_var_loss = ((final_mean - var_path[-1]) / final_mean) * 100\n\n# Create figure with appropriate size and DPI for high quality\nplt.figure(figsize=(10, 6), dpi=300)\n\n# Paths to display (for efficiency only plot a subset with very low alpha)\ndisplay_paths = 300\npaths_to_display = np.random.choice(num_paths, display_paths, replace=False)\n\n# Plot paths\nfor i in paths_to_display:\n    # Check if this path is below the VaR at the end\n    for j in range(time_steps):\n        is_below_var = paths[i, j] &lt;= var_path[j]\n        if is_below_var:\n            plt.plot(\n                time_points[j], paths[i], color=\"#ef4444\", alpha=0.15, linewidth=0.5\n            )\n        else:\n            plt.plot(\n                time_points[j], paths[i], color=\"#3b82f6\", alpha=0.08, linewidth=0.5\n            )\n\n# Plot the VaR path\nplt.plot(\n    time_points,\n    var_path,\n    color=\"#dc2626\",\n    linewidth=3,\n    label=f\"{var_percentile}% Value at Risk\",\n)\n\n# Mark the final VaR point\nplt.scatter([T], [var_path[-1]], color=\"#dc2626\", s=50, zorder=5)\n\n# Styling\nplt.title(\n    \"Value at Risk (VaR) - Simulated Brownian Motion Paths\",\n    fontsize=14,\n    fontweight=\"bold\",\n)\nplt.xlabel(\"Time (years)\", fontsize=12, fontweight=\"bold\")\nplt.ylabel(\"Asset Price\", fontsize=12, fontweight=\"bold\")\n\n# Custom legend\nlegend_elements = [\n    Line2D(\n        [0], [0], color=\"#3b82f6\", lw=1, alpha=0.5, label=\"Simulated paths (above VaR)\"\n    ),\n    Line2D(\n        [0], [0], color=\"#ef4444\", lw=1, alpha=0.5, label=\"Simulated paths (below VaR)\"\n    ),\n    Line2D([0], [0], color=\"#dc2626\", lw=3, label=\"5% Value at Risk\"),\n]\nplt.legend(handles=legend_elements, loc=\"upper right\", fontsize=10)\n\n# Set axis limits for better visualization\nplt.xlim(0, T)\nmin_val = min(np.min(sorted_paths[0]), np.min(var_path)) * 0.95\nmax_val = max(np.max(paths), np.max(var_path)) * 1.05\nplt.ylim(min_val, max_val)\n\n# Tight layout for better spacing\nplt.tight_layout()\n\n# Save as PNG\nplt.savefig(\"value_at_risk.png\", dpi=300, bbox_inches=\"tight\")\nprint(\"Saved as value_at_risk.png\")\n\n# Save as JPEG\nplt.savefig(\"value_at_risk.jpg\", dpi=300, bbox_inches=\"tight\", quality=95)\nprint(\"Saved as value_at_risk.jpg\")\n\n# Display the plot (optional)\nplt.show()\n\nSaved as value_at_risk.png\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[1], line 109\n    106 print(\"Saved as value_at_risk.png\")\n    108 # Save as JPEG\n--&gt; 109 plt.savefig('value_at_risk.jpg', dpi=300, bbox_inches='tight', quality=95)\n    110 print(\"Saved as value_at_risk.jpg\")\n    112 # Display the plot (optional)\n\nFile ~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\matplotlib\\pyplot.py:1243, in savefig(*args, **kwargs)\n   1240 fig = gcf()\n   1241 # savefig default implementation has no return, so mypy is unhappy\n   1242 # presumably this is here because subclasses can return?\n-&gt; 1243 res = fig.savefig(*args, **kwargs)  # type: ignore[func-returns-value]\n   1244 fig.canvas.draw_idle()  # Need this if 'transparent=True', to reset colors.\n   1245 return res\n\nFile ~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\matplotlib\\figure.py:3490, in Figure.savefig(self, fname, transparent, **kwargs)\n   3488     for ax in self.axes:\n   3489         _recursively_make_axes_transparent(stack, ax)\n-&gt; 3490 self.canvas.print_figure(fname, **kwargs)\n\nFile ~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\matplotlib\\backend_bases.py:2184, in FigureCanvasBase.print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\n   2180 try:\n   2181     # _get_renderer may change the figure dpi (as vector formats\n   2182     # force the figure dpi to 72), so we need to set it again here.\n   2183     with cbook._setattr_cm(self.figure, dpi=dpi):\n-&gt; 2184         result = print_method(\n   2185             filename,\n   2186             facecolor=facecolor,\n   2187             edgecolor=edgecolor,\n   2188             orientation=orientation,\n   2189             bbox_inches_restore=_bbox_inches_restore,\n   2190             **kwargs)\n   2191 finally:\n   2192     if bbox_inches and restore_bbox:\n\nFile ~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\matplotlib\\backend_bases.py:2040, in FigureCanvasBase._switch_canvas_and_return_print_method.&lt;locals&gt;.&lt;lambda&gt;(*args, **kwargs)\n   2036     optional_kws = {  # Passed by print_figure for other renderers.\n   2037         \"dpi\", \"facecolor\", \"edgecolor\", \"orientation\",\n   2038         \"bbox_inches_restore\"}\n   2039     skip = optional_kws - {*inspect.signature(meth).parameters}\n-&gt; 2040     print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(\n   2041         *args, **{k: v for k, v in kwargs.items() if k not in skip}))\n   2042 else:  # Let third-parties do as they see fit.\n   2043     print_method = meth\n\nTypeError: FigureCanvasAgg.print_jpg() got an unexpected keyword argument 'quality'"
  }
]