---
title: "Portfolio Value at Risk Forecasting with GARCH-Type Models"
author: "Jan Heinrich Schlegel"
date: "2023-02-16"
format:
  html:
    toc: true
    number-sections: true
    html-math-method: katex
bibliography: references.bib
---

# Abstract

This thesis examines the value at risk (VaR) forecasting ability of various univariate and multivariate models for a long equity portfolio. All of the considered models involve a generalized autoregressive conditional heteroskedasticity (GARCH)-type structure. The resulting forecasts are checked for desirable properties using violation-based backtests and compared in terms of predictive ability. We find that the VaR forecasts of almost all univariate models are inadequate, while the multivariate models have few problems passing these backtests. However, we do not find evidence that the multivariate models systematically outperform their univariate counterparts with regards to predictive accuracy, or vice versa.

# Introduction {#sec-intro}

The last four decades were shaped by many extreme events on the financial markets. Especially noteworthy incidents were the sudden market crash on the Black Monday, the dot-com bubble and the global financial crisis of 2008. All of these supposedly rare events pointed out that reducing systematic risk is paramount to ensure the stability of the financial system. Thus, the demand for more stringent regulation strongly increased which led to the introduction and consequent tightening of the Basel Accords. This framework makes use of so-called risk measures to determine the appropriate amount of risk capital that a institution has to hold. Even though a shift towards the severity-based expected shortfall (ES) can be observed, the value at risk (VaR) to this day remains the most popular measure for downside market risk (see @embrechts_2014 for the differences between the VaR and the ES). When considering a long equity portfolio, the $p$\% VaR for period $t$ forecasted at time $t-1$ is defined as the negative $p$-quantile of the conditional portfolio return distribution, i.e.

$$
\text{VaR}_t^p=-Q_p(r_{\text{PF},t}|\mathcal{F}_{t-1})=-\inf_x\{x\in\mathbb{R}:\mathbb{P}(r_{\text{PF},t}\leq x|\mathcal{F}_{t-1})\geq p\},\quad p\in(0,1).
$$ {#eq-VaR_def}

Hereby $Q_p(\cdot)$ denotes the quantile function and $\mathcal{F}_{t-1}$ is a filtration that represents all the information available at time $t-1$. The parameter $p$ is the level of the VaR and indicates that with target probability $p$ the losses of the portfolio will exceed the VaR [@Paolella2006]. We will follow @Santos2013 and consider the 1\% and the 5\% VaR level in our empirical analysis.

Due to the practical relevance of this risk measure it is essential to determine how the VaR should be estimated such that it neither severely underestimates nor overestimates future losses. To this end, a plethora of models have been proposed to generate VaR forecasts, many of which involve the generalized autoregressive conditional heteroskedasticity (GARCH) model by @bollerslev_garch or an extension thereof. These GARCH structures should account for volatility clustering and the so-called "leverage effect" which both are typically inherent in financial time series (see e.g. @quant_risk_man [chap. 3]). Moreover, one can either choose a univariate approach, where only the conditional portfolio variance is modelled, or a multivariate approach, where the joint dynamics of the portfolio constituents are modelled.

The question whether the more complex multivariate models are to be preferred over the simpler univariate alternatives has been extensively discussed in literature. @Santos2013, who considered three large portfolios, found that the multivariate models significantly outperform their univariate counterparts. Similarly, @kole_forecasting found that multivariate models have greater predictive ability but these differences are in most cases not high enough to be deemed significant. Further, they find that the choice of return frequency is more important than the choice of the model itself. In addition to the data frequency and the volatility or correlation model, the assumed distribution of the error terms often majorly impacts VaR forecasts. The importance of the assumed distribution of the innovations is highlighted in @Paolella2006, @levyGARCH, @diks_density or @COMFORT_2022 among others.

A middle ground to fitting a GARCH-type model to all the constituents and a univariate model is to only provide equity factors, which should capture the main risks of the portfolio, with a GARCH structure. @Fortin2022 introduced such a framework but failed to find significant advantages over more parsimonious univariate models. Moreover, none of the models they considered adequately forecasted the one-week-ahead VaR of a portfolio in terms of conditional coverage. We replicate their models and consider an equally weighted portfolio consisting of the same ten large cap single stocks they used. However, instead of the weekly return data @Fortin2022 used, we based our analysis on daily returns as this is, as identified in @kole_forecasting, the data frequency which yields the most adequate VaR forecasts (out of the time intervals they considered).

We extend the existing literature by comparing these factor copula-DCC-NGARCH models introduced in @Fortin2022 to more established models such as the diagonal MixN(k)-GARCH presented in @MN2004 or the COMFORT model class by @COMFORT_2015. We show that, contrary to most univariate models, the forecasts generated by the multivariate models display desirable VaR properties in the form of correct unconditional coverage and independence of the violations. Further, we do not find enough evidence to justify stating that the multivariate approaches outperform the univariate procedures in terms of forecast ability, or vice versa. Additionally, we present a slightly modified version of the model class presented in @Fortin2022 and compare it to the original models. Finally, we come up with a plausible explanation for the differences in forecasting ability between this original and the modified model class.

The remainder of this thesis is structured as follows. Chapter @sec-theoretical examines the univariate and multivariate models. In Chapter @sec-methodology we present the data, shine a light on how the VaR estimates were created for each model and explain how the forecasts are backtested and compared. In Chapter @sec-results we discuss our empirical results. In Chapter @sec-conclusion we conclude.

# Theoretical Framework {#sec-theoretical}

This chapter will introduce the models that were used to forecast the value at risk (VaR) of a portfolio in our empirical application. First, some of the most popular univariate GARCH-type structures will be presented. Next, the multivariate models in form of the factor copula model class from @Fortin2022 and the COMFORT model by @COMFORT_2015 are discussed. Note that there are far more GARCH-type models than the few that will be mentioned in this chapter (see e.g. @GARCH_glossary for an extensive compendium). This chapter often makes use of the notion of a information set, denoted $\mathcal{F}_{t-1}$, which in simple terms is just the information available at the end of time $t-1$ on which the forecast will be based i.e. in our case an observed sequence of past returns.

## Univariate Models {#sec-univariate}

For our univariate models we assume the following portfolio return dynamics:

$$
r_{\text{PF},t}=\mu+\epsilon_t,
$$ {#eq-return-dynamics1}

where for all but the MixN(k)-GARCH we let

$$
\epsilon_t = \sigma_t z_t, \quad z_t\stackrel{iid}{\sim}F(0,1),
$$ {#eq-return-dynamics2}

where $F(0,1)$ is some standardized distribution (i.e. zero-location and unit-scale), $\mu$ is the unconditional location and $\sigma_t$ is the scale parameter. In this framework the conditional variance $\sigma_t^2=\mathbb{V}[r_{\text{PF},t}|\mathcal{F}_{t-1}]$, where $\mathcal{F}_{t-1}=\{r_{\text{PF},1},...,r_{\text{PF},t-1}\}$ denotes the information available at time $t-1$, is assumed to be non-constant.

A simple way to model this conditional variance is by means of a generalized autoregressive conditional heteroskedasticity (GARCH) process. This GARCH model introduced in @bollerslev_garch is a generalisation of the ARCH model by @engle-arch. The most prominent version, the GARCH(1,1), can be formulated via

$$
\sigma_t^2=\omega+\alpha\epsilon_{t-1}^2+\beta\sigma_{t-1}^2,
$$ {#eq-GARCH}

where $\omega>0,\alpha\geq0$, and $\beta\geq0$. In other words, in a GARCH(1,1) model the conditional volatility behaves similarly to how an ARMA(1,1) process would for the conditional mean. For covariance stationarity of the process the parameters have to fulfill $\alpha+\beta<1$. In our empirical application, we will only consider the GARCH(1,1) model with normal innovation terms i.e. $z_t\stackrel{iid}{\sim}\mathcal{N}(0,1)$ which will serve as our univariate benchmark. However, it is possible to define higher order GARCH(p,q) processes, but we will only regard the case where $p=q=1$. Thus, henceforth whenever a GARCH-type model is mentioned without explicitly stating $p$ and $q$, we will refer to the $p=q=1$ case.

A relevant special case of the GARCH model is the exponentially weighted moving average (EWMA). It sets $\omega=0$ and the weights $\alpha$ are exponentially decaying and sum up to one:

$$
\sigma^2_t=\lambda\sigma^2_{t-1}+(1-\lambda)\epsilon_t^2, \quad\lambda\in(0,1).
$$ {#eq-ewma}

This model formulation puts more weight on the most recent observations which can be beneficial in some cases. But note that this process is not covariance stationary and the variance is thus not mean-reverting since $\lambda+(1-\lambda)=1$. This can pose a problem when large returns are observed as they will influence conditional volatility estimates for a long time [@score_EWMA]. For our model comparison, we will be using daily data and therefore we will set the decaying factor $\lambda = 0.94$ in accordance with @Riskmetrics.

A well known stylized fact of stock returns is that negative news tends to increase volatility more than positive news of equal magnitude. To account for this so-called 'leverage effect' @GJR created an asymmetric extension of the standard GARCH model which they called GJR-GARCH(1,1):

$$
\sigma_t^2=\omega+(\alpha+\gamma I_{t-1})\epsilon_{t-1}^2+\beta\sigma_{t-1}^2,
$$ {#eq-GJR}

where $\omega>0$, $\alpha+\gamma\geq0$, $\beta\geq0$ and

$$
I_{t-1}=\mathbb{1}_{\{\epsilon_{t-1}<0\}}=\begin{cases}
0 & \text{if } \epsilon_{t-1}\geq0\\
1 & \text{if } \epsilon_{t-1}<0
\end{cases}
$$ {#eq-indicator}

The process is thereby covariance stationary if $\alpha+\beta+\frac{1}{2}\gamma<1$. Obviously, if $\gamma=0$ this model reduces to the GARCH(1,1) from equation @eq-GARCH which treats news symmetrically, i.e. bad news ($\epsilon_{t-1}<0$) have the same impact as good news ($\epsilon_{t-1}\geq0$). In practice however, $\gamma$ is usually found to be positive [@GARCH_glossary]. This leads to an asymmetric relationship since negative news is weighted with $\alpha+\gamma$ and positive news only with $\alpha$. We decided to include this model in our comparison due to the great performance of the GJR with Student t innovations in @Santos2013. Additionally, we tried out a GJR with the skewed-t distribution by @skewt_Hansen for the error terms.

Another way to capture the leverage effect is presented in @NGARCH which they termed the NGARCH(1,1) model:

$$
\sigma_t^2=\omega + \alpha\sigma_{t-1}^2(\epsilon_{t-1}-\theta)^2 + \beta\sigma_{t-1}^2,
$$ {#eq-NGARCH}

where $\omega>0$, $\alpha\geq 0$ and $\beta\geq 0$. For covariance stationarity $\alpha(1+\theta^2)+\beta<1$ is required. It is apparent that for $\theta>0$ negative innovations ($\epsilon_{t-1}<0$) have a larger impact on the conditional variance than positive errors of the same magnitude which causes this model to be asymmetric too [@christoffersen-FFCCopula]. We will incorporate this model with a skewed-t distribution for the innovations as this is the same specification that @Fortin2022 and @christoffersen-FFCCopula used in their factor copula models.


The last class of univariate models that we included in our empirical comparison is the k-component mixed normal GARCH(1,1) (MixN(k)-GARCH) as introduced in @MN2004. We will only consider the case which was referred to as diagonal by @MN2004 as they identified it to be the superior choice in terms of performance and interpretability. In this framework, the conditional distribution of the error term $\epsilon_t$ is assumed to be mixed normal (see @sec-appendix-mixn) with zero mean,

$$
\epsilon_t|\mathcal{F}_{t-1}\sim\text{Mix}_k\text{N}(p_1,...,p_k, \mu_1,...,\mu_k, \sigma_{1,t}^2,...,\sigma_{k,t}^2),\quad \sum_{i=1}^k p_i\mu_i = 0,
$$ {#eq-MixN_GARCH_dist}

where $p_i\in(0,1)\,\forall i$, $\sum_{i=1}^kp_i=1$ and $\mathcal{F}_{t-1}=\{r_{\text{PF},1},...,r_{\text{PF},t-1}\}$ is the information set at time $t-1$. The associated conditional variances are given by GARCH processes:

$$
\sigma_{i,t}=\omega_i+\alpha_i\epsilon_{i,t-1}^2+\beta_i\sigma_{i,t-1}^2, \quad i=1,...,k.
$$ {#eq-MixN_GARCH}

The $k\geq 2$ components that are used for the conditional variance each represent a different market condition [@levyGARCH]. Hence, often a low number of components such as 2 or 3 is sufficient as discovered in @MN2004. In case of different means for the different components, a normal mixture density incorporates skewness and fat tails (or thin tails) [@MixN_Carol]. This property enables the MixN(k)-GARCH model to produce sound VaR estimates as showcased in @MN2004 and @Paolella2006.

## A Multivariate Factor Copula-DCC-GARCH Model {#sec-factor-copula}

The models described in the previous section all forecasted future returns of a portfolio based on past portfolio returns. Alternatively, we can take a multivariate approach where we first model the constituents of the portfolio and only in a second step draw conclusions about the portfolio. One such model was proposed in @Fortin2022 following the discoveries made about asymmetric tail dependency of weekly factor returns in @christoffersen-FFCCopula. Consequently, @Fortin2022 make use of equity factors that should capture the main risks of stock returns to forecast the VaR of a portfolio of single stocks. More precisely, they utilize the Carhart four-factor model suggested in @Carhart1997 which adds a momentum factor to the linear three-factor model of @FamaFrench. Within this model, the return of a single stock $k$ in excess of the weekly risk free rate $r_{f,t}$ is given by

$$
\begin{aligned}
r_{k,t}-r_{f,t}&=\alpha_{k,t}+\beta_{k, \text{RMRF}}\text{RMRF}_t+\beta_{k,\text{SMB}}\text{SMB}_t+\beta_{k,\text{HML}}\text{HML}_t+\beta_{k, \text{MOM}}\text{MOM}_t+\varepsilon_{k,t}\\
&=\alpha_{k,t}+\bm{\beta}_k'\bm{r}_{\text{F},t}+\varepsilon_{k,t},\qquad t=1,2,...,T,
\end{aligned}
$$ {#eq-Carhart}

where both the intercept $\alpha_{k,t}$ and the vector of factor loadings $\bm{\beta}_k$ are assumed to be constant over time. The vector of equity factors at time $t$, $\bm{r}_{\text{F},t}=(\text{RMRF}_t, \text{SMB}_t, \text{HML}_t,\text{MOM}_t)'$, consists of the market factor (RMRF), the size factor (SMB), the value factor (HML) and the momentum factor (MOM) (cf. @Carhart1997). This leads to a reduction in dimensionality of the problem since only four factors have to be modelled instead of all constituents of the stock portfolio.

@Fortin2022 decided to not only model the conditional variances but also the dependence between the factors. One possible way to do this is by using the Dynamic Conditional Correlation (DCC) structure^[In contrast to @Fortin2022, we will be using the DCC model of @DCC instead of the cDCC model of @cDCC due to its more established R implementation. The differences between these two models are relatively small according to @christoffersen-FFCCopula. introduced in @DCC which is a dynamic extension of the Constant Conditional Correlation (CCC) model by @CCC. Let $\bm{Y_t}=(y_{1,t}, y_{2,t},...,y_{n,t})'$ be a vector consisting of the returns of n assets or of n factors at time $t=1,...,T$ and $\bm{\mu}=\mathbb{E}[\bm{Y_t}|\mathcal{F}_{t-1}]$ the corresponding mean vector which is assumed to be constant. The information set at time $t-1$ once again is denoted by $\mathcal{F}_{t-1}$ and consists of the past returns $\{\bm{Y_1},...,\bm{Y_{t-1}}\}$.

We can then define the conditional covariance matrix of $\bm{r_t}$ as $\bm{\Sigma_t}\eqdef\mathbb{E}[(\bm{Y_t}-\bm{\mu})(\bm{Y_t}-\bm{\mu})'\|\mathcal{F}_{t-1}]$. The DCC model decomposes the dynamics of the conditional covariance matrix into standard deviations and correlation and can be written as:

$$
\bm{Y}_{t}|\mathcal{F}_{t-1}\sim\mathcal{N}_n(\bm{\mu}, \bm{\Sigma_t}), \quad\bm{\Sigma_t}=\bm{D_t}\bm{\Gamma_t}\bm{D_t}
$$ {#eq-sigma_decomp}

where $\bm{D_t}=\text{diag}(\sigma_{1,t},\sigma_{2,t},...,\sigma_{n,t})$ i.e. a $n\times n$ diagonal matrix with the diagonal consisting of the square roots of the conditional variances of the assets or the factor returns. These conditional variances can be modelled using one of the univariate models from section @sec-univariate. @Fortin2022 chose an AR(3)-NGARCH(1,1) model with the skewed-t distribution of @skewt_Hansen for the innovations of all factor returns. We however tried out two different univariate models in our empirical application: a NGARCH(1,1) with skewed-t and a GARCH(1,1) with normal innovations, both without an ARMA term. Henceforward, these multivariate models will be named by first specifying the conditional correlation structure followed by the GARCH-type process used for the conditional variances e.g. DCC-GARCH or CCC-GJR.

The $n\times n$ conditional correlation matrix $\bm{\Gamma_t}$ from equation @eq-sigma_decomp is symmetric positive-definite and given by

$$
\bm{\Gamma_t}\eqdef\mathbb{E}[\bm{z_t}\bm{z_t}'|\mathcal{F}_{t-1}]=\text{diag}(\bm{Q_t}^{-1/2})\,\bm{Q_t}\,\text{diag}(\bm{Q_t}^{-1/2})
$$ {#eq-dcc_gamma}

where $\bm{z_t}\eqdef\bm{D_t}^{-1}(\bm{Y_t}-\bm{\mu})$ are the standardized residuals. $\bm{Q_t}$ also is a $n\times n$ symmetric positive-definite matrix that fulfills

$$
\bm{Q_t} = (1-\alpha-\beta)\bar{\bm{Q}}+\alpha\bm{z_{t-1}}\bm{z_{t-1}}'+\beta\bm{Q_{t-1}},
\quad \alpha,\beta\geq 0,
$$ {#eq-dcc_Q}

where $\bar{\bm{Q}}\eqdef\frac{1}{T}\sum_{t=1}^T\bm{z_t}\bm{z_t}'$ is the $n\times n$ unconditional covariance matrix. Setting $\alpha=\beta=0$ in equation @eq-dcc_Q yields the CCC model of @CCC where $\bm{\Gamma_t}=\bm{\Gamma}=\bar{\bm{Q}}$ for all $t$.

Possibly the biggest drawback of the DCC model as specified above is the underlying assumption of multivariate normality. In particular, @christoffersen-FFCCopula discovered that ignoring the multivariate non-normality inherent in weekly factor returns leads to severe underestimation of the Expected Shortfall of an equally weighted portfolio of factors. To encompass this non-normality and the aforementioned asymmetric tail dependence of weekly equity factors, @Fortin2022 stick to @christoffersen-FFCCopula and make use of so-called copulas to fit the joint conditional distribution of the factor returns. Copulas are functions that allow us to model the marginals (in our case the standardized residuals of the DCC-(N)GARCH model) individually and independently of the multivariate distribution making them incredibly flexible. The notion of Copulas is based on Sklar's theorem which when applied to the standardized DCC-(N)GARCH residuals of our $n$ factor returns states that 

$$
\bm{F_t}(\bm{z_t})=\bm{C_t}(F_{1,t}(z_{1,t}),...,F_{n,t}(z_{n,t})),
$$ {#eq-Sklar}

where $\bm{F_t}(\bm{z_t})$ is the joint conditional distribution of the standardized residuals at time $t$, $F_{i,t}(\cdot)$ is the conditional marginal distribution of the standardized residual of factor i at time $t$ and $\bm{C_t}:[0,1]^n\rightarrow[0,1]$ is the conditional copula that links these marginal distributions. As highlighted in @Heinen_copula, we can only apply Sklar's theorem to conditional distributions if we condition all marginals and the copula on the same information. This can be done by assuming that each marginal only depends on its own past and that the copula depends on the history of all four factors [@Heinen_copula]. As explained in e.g. @Heinen_copula a copula can also be expressed as a multivariate distribution with $\mathcal{U}(0,1)$ margins

$$
\bm{C_t}(u_1,...,u_n)=\bm{F_t}(F_{1,t}^{-1}(u_{1,t}),...,F_{n,t}^{-1}(u_{n,t})),
$$ {#eq-Sklar_2}

where $u_{i,t}=F_{i,t}(\epsilon_{i,t})$ is the probability integral transform (PIT) (see @sec-appendix-pit) of the standardized residuals of factor i at time $t$. This formulation shows that if we can sample from the copula, we can sample from the corresponding multivariate distribution $\bm{F_t}(\cdot)$. Thus, for Monte Carlo simulations one can first generate a vector of probabilities $\bm{u_t}\eqdef(u_{1,t},...,u_{n,t})'$ with conditional distribution $\bm{C_t}(\cdot)$ and then apply the quantile transform (see @sec-appendix-quantile-trafo) to these $u_{i,t}$ to return a sample vector $(F_{1,t}^{-1}(u_{1,t}),...,F_{n,t}^{-1}(u_{n,t}))'$ from $\bm{F_t}(\cdot)$.

In summary, we follow @Elements_of_FRM [chap. 9] and apply the following scheme to simulate future factor returns:

1. Fit a DCC model with a GARCH or NGARCH model for the conditional variances and extract the standardized residuals $\bm{z_t}=(z_{1,t},...,z_{n,t})'$.
2. Calculate probabilities $u_{i,t}=F_{i,t}(z_{i,t})$, where $F_{i,t}(\cdot)$ is the conditional distribution that was estimated in the (N)GARCH model for the $i$'th factor.
3. Fit a copula to these probabilities. In our empirical application we consider the normal copula (see @sec-appendix-cop-gauss), the Student t copula (see @sec-appendix-cop-t) and the skewed-t copula. For more information about the Student t copula and the skewed-t copula consult @t_copula and @skewt_cop_R.
4. Simulate a vector of probabilities $(\tilde{u}_{1,t}, ..., \tilde{u}_{n,t})'$ from the conditional copula.
5. Create simulated standardized residuals from the simulated copula probabilities using quantile transforms: $\widetilde{\bm{z_{t}}}\eqdef(F_{1,t}^{-1}(\tilde{u}_{1,t}),...,F_{n,t}^{-1}(\tilde{u}_{n,t}))'$.
6. To create factor returns from the simulated standardized residuals we use the forecasted dynamics obtained from the DCC model:

$$
\widetilde{\bm{r_{\text{F}, t}}}=\bm{\mu}+\bm{\Sigma_{t}}^{1/2}\,\widetilde{\bm{z_{t}}},
$$ {#eq-back_rets}

where $\bm{\Sigma_{t}}^{1/2}$ denotes the matrix square root obtained through a Cholesky decomposition of the one-period-ahead forecast of the (DCC) covariance matrix of the factor returns and $\bm{\mu}$ the unconditional mean vector of the factor returns.

Applying equation @eq-Carhart to these simulated one-period-ahead factor returns yields forecasts of the returns of the single stocks (see @sec-var-forecasts for more information). Finally, it is straightforward to calculate the simulated one-day-ahead portfolio returns and the desired risk measures since this approach is simulation-based.

## The COMFORT Model {#sec-comfort}

In the previous section we presented the approach of using copulas to introduce non-normality (and heterogeneous tails when using e.g. the skewed-t copula) into a CCC or DCC structure. But one could also directly choose a non-Gaussian distribution for the CCC or DCC model. One possible way to implement this is by first fitting a normal CCC or DCC model to the stock returns and then in a second step fitting a multivariate, non-normal distribution to the filtered residuals obtained in the first step [@Paolella_TS_book, chap. 11]. But this so-called quasi maximum likelihood approach is inferior to joint maximum likelihood estimation of all parameters [@Paolella_TS_book, chap. 11]. Consequently, @COMFORT_2015 developed an efficient expectation-maximization (EM) type algorithm that allows for full maximum likelihood estimation of all parameters of the multivariate generalized hyperbolic (MGHyp) distribution. This uni-modal distribution is commonly used in finance literature and offers great flexibility [@COMFORT_2022]. In particular, many popular distribution choices such as the multivariate normal, the multivariate Student t, the multivariate variance gamma or the normal inverse Gaussian (and many more) can be expressed as a limiting or special case of the MGHyp [@COMFORT_2022]. For a more detailed breakdown of this distribution we refer to @quant_risk_man [chap. 6], @COMFORT_2015 and @COMFORT_2022. Applying the aforementioned multi-stage EM algorithm to a MGHyp distribution, for whose covariance matrix a CCC or DCC structure was used, results in the so-called Common Market Factor Non-Gaussian Returns (COMFORT) model as introduced in @COMFORT_2015.

Let $\bm{Y_t}=(y_{1,t}, y_{2,t},...,y_{n,t})'$ be an n-dimensional vector of returns of the constituents of a portfolio at time $t=1,...,T$. The COMFORT model then has the form $\bm{Y_t}\sim\text{MGHyp}(\bm{\mu},\bm{\gamma},\bm{\Sigma_t},\lambda_t,\chi_t, \psi_t)$ and can be expressed as a continuous normal mixture in the following way:

$$
\bm{Y_t}=\bm{\mu}+\bm{\gamma} G_t + \bm{\varepsilon_t}, \qquad\bm{\varepsilon_t}=\bm{\Sigma_t}^{1/2}\sqrt{G_t}\bm{Z_t},
$$ {#eq-COMFORT}

where the mean vector $\bm{\mu}$ and the asymmetry vector $\bm{\gamma}$ are in $\mathbb{R}^n$ and $\bm{Z_t}\stackrel{iid}{\sim}\mathcal{N}_n(\bm{0},\bm{I_n})$ [@COMFORT_2022]. Further, the symmetric positive-definite covariance matrix $\bm{\Sigma_t}=\bm{D_t}\bm{\Gamma_t}\bm{D_t}$ is modelled using a CCC or DCC structure (see @sec-factor-copula) where the matrix square root is taken by means of a Cholesky decomposition. Lastly, the mixing random variables $G_t|\mathcal{F}_{t-1}\sim \text{GIG}(\lambda_t,\chi_t,\psi_t)$, where $\mathcal{F}_{t-1} = \{\bm{Y_1},...,\bm{Y_{t-1}}\}$ is the information set at time $t-1$, follow a generalized inverse Gaussian (GIG) random variable and are independent of $\bm{Z_t}$ [@COMFORT_2022]. Consult @GIG for a detailed account on the GIG distribution or @COMFORT_2022 for more information about the GIG in the COMFORT model. This sequence $\{G_t\}$ can be interpreted as a common market factor because conditional on this common market factor, which incorporates jumps and news arrival, the returns are multivariate normal distributed [@Paolella_TS_book, chap. 11].

In our empirical application in @sec-results we will only consider the CCC augmentation in which $\bm{\Gamma_t}=\bm{\Gamma}$ is time invariant. For the dynamics of the diagonal elements of $\bm{D_t}$ adjusted GARCH-type processes, which incorporate the common market factor, are used. More precisely, we will utilize the GARCH(1,1) and the GJR-GARCH(1,1) from @sec-univariate but with error term $\epsilon_{i,t}=y_{i,t}-\mu_i-\gamma_i G_t$ instead of $\epsilon_{i,t}=y_{i,t}-\mu_i$ for the $i$'th portfolio constituent. Furthermore, we will only study the so-called multivariate variance-gamma (MVG) distribution which can be expressed as a MGHyp distribution with $\lambda_t>0$, $\chi_t=0$ and $\psi_t=2$ for all $t$ (see @COMFORT_2022). This restriction of the shape parameters $\chi_t$ and $\psi_t$ is recommended by @COMFORT_2015 to circumvent potential numerical problems caused by otherwise relatively flat likelihoods.

An important property of the MGHyp distribution is that it is closed under linear operations as shown in @quant_risk_man [chap. 6]. Thus, the returns of a portfolio consisting of our $n$ constituents with constant portfolio weights $\textbf{w}=(\text{w}_1,\text{w}_2,...,\text{w}_n)'\in\mathbb{R}^n\setminus\bm{0}$ are given by $r_{\text{PF},t}=\textbf{w}'\bm{Y_t}$ and are univariate GHyp distributed:

$$
r_{\text{PF},t}|\mathcal{F}_{t-1}\sim\text{GHyp}(\textbf{w}'\bm{\mu},\textbf{w}'\bm{\gamma},\textbf{w}'\bm{\Sigma_t}\textbf{w},\lambda_t,\chi_t, \psi_t).
$$ {#eq-GHyp_PF}

This property is especially useful in portfolio optimization and risk management and will be used in @sec-var-forecasts to estimate the VaR. We decided to include this model class because of the adequate VaR forecasts it generated in @COMFORT_2022. Particularly, the empirical analysis in @COMFORT_2022 showed that the VaR estimates of this model class were superior to the ones of a CCC-GARCH structure that assumed multivariate normality.

# Methodology {#sec-methodology}

This chapter will present the data we considered for our empirical application. Further, we will describe how the VaR forecasts were generated and shine a light on some issues we faced. Finally, the violation based likelihood ratio tests of @CCTest as well as a loss function based test in form of the conditional predictive ability (CPA) test by @CPATests will be introduced.

## Data {#sec-data}

This thesis assesses models based on their ability to forecast the VaR of a long portfolio. This portfolio is equally weighted and consists of the ten large cap single stocks also used in @Fortin2022. However, we consider 2767 daily returns (instead of weekly returns) which were observed from January 2, 2001 to December 30, 2011. This data is freely available on Yahoo Finance. For reasons of numerical stability we will be working with daily percentage log returns i.e. 

$$r_t=100\cdot\log\left(\dfrac{P_t}{P_{t-1}}\right)$$

where $P_t$ is the price at time t. Additionally, we require daily returns of the factors from the Carhart four-factor model for the same time frame. These returns are from Kenneth R. French's data library. Note that these factor returns are in percent but nominal and first have to be converted to percentage log returns. In the following, whenever returns are mentioned we will refer to daily percentage log returns. Furthermore, the VaR forecasts will be the one-step-ahead percentage log return VaR.

```{r}
#| label: tbl-summary-stats
#| tbl-cap: Summary statistics of daily factor, stock and portfolio percentage log returns
#| echo: false
```

In Table @tbl-summary-stats the summary statistics for the daily factor, stocks and portfolio returns are presented. One can see that whilst the median is larger than the mean in most instances, both are close to zero for all factors and stocks in question. In addition, the mean absolute deviation (MAD) is considerably smaller than the standard deviation indicating the presence of outliers. Three of the four factors, six of the ten stocks and the portfolio are left skewed and thus have a longer left tail. All factors, stocks and the portfolio show leptokurtic behavior i.e. their kurtosis is larger than three. This signals that the return distributions have fatter tails than a Gaussian distribution would have. Besides, we can reject the assumption of univariate normality for all of the return distributions using the extraordinarily high Jarque-Bera statistics. Additionally, we checked for multivariate normality by constructing Q-Q plots (see ) of the robust squared Mahalanobis distances of the stock returns (factor returns) and the corresponding $\chi^2_d$ distribution with $d=10$ ($d=4$) degrees of freedom. The relationship in these Q-Q plots is not linear at all indicating large multivariate outliers and multivariate non-normality.

```{r}
#| label: fig-md-chisq-plots
#| fig-cap: χ² Q-Q plots of the robust squared Mahalanobis distance of the factor returns and the stock returns. The degrees of freedom correspond to the number of factors in Panel A and to the number of stocks in Panel B.
#| echo: false
#| fig-width: 6.4
#| fig-height: 4

# Code for creating the Q-Q plots would go here
```

Moreover, the portfolio returns exhibit large volatility around their mean as shown in panel A of @fig-pf-plots. Panel A further shows blatant volatility clustering indicating that the conditional variance is non-constant. In addition, it is apparent from Table @tbl-summary-stats that the portfolio returns are not normally distributed. Panel B graphically displays this observation and shows that the distribution of the portfolio percentage log returns is slightly left skewed and has long tails with some extreme values on either side. Interestingly, both the smallest and the largest portfolio returns were observed in October of 2008.

```{r}
#| label: fig-pf-plots
#| fig-cap: Plot and histogram of the percentage log-returns of the equally weighted long equity portfolio
#| fig-subcap: 
#|   - Time series of returns
#|   - Distribution of returns
#| layout-ncol: 2
#| echo: false

# Code for portfolio return plots would go here
```

```{r}
#| label: fig-acf-plots-factors
#| fig-cap: ACF Plots of the Fama-French-Carhart Factors
#| fig-subcap: 
#|   - Market factor
#|   - Size factor
#|   - Value factor
#|   - Momentum factor
#| layout-ncol: 2
#| echo: false

# Code for ACF plots would go here
```

Despite using daily and not weekly factor returns, the autocorrelation function (ACF) plots in @fig-acf-plots-factors display a similar picture as the ones in @christoffersen-FFCCopula. It is evident that the factors exhibit stronger autocorrelation in absolute returns than in the returns themselves. In particular, panels A to D illustrate that whilst the ACF values for the returns mostly stay within the 95%-confidence bands, the absolute factor returns show significant autocorrelation for all lags considered. The same phenomenon can also be observed for the single stock returns and the portfolio returns (not displayed here) and can be seen as further justification for fitting a volatility model to the return series.

## Value at Risk Forecasts {#sec-var-forecasts}

For all models in question we assume that the conditional mean is constant over time and thus will proceed without specifying an ARMA(p,q) process for the conditional mean. @Santos2013 took the same approach for the reason that the dynamic dependence of conditional means in daily portfolio returns is very weak if existent at all. Forecasting is done using a rolling window approach where the previous 1000 observations are used to predict the one step-ahead-VaR. The model parameters will be refit after every rolling window iteration as in @Paolella2006. The only exception is the skewed-t copula-DCC-GARCH model where we decided to reestimate the skewed-t copula parameters only every 20 rolling windows due to the high computational burden. Additionally, there were some numerical issues for the skewed-t copula with skewed-t NGARCH marginals which we could not resolve and thus only the version with normal GARCH marginals will be included.

For all univariate GARCH models mentioned in @sec-univariate but the MixN(k)-GARCH we can rely on the following analytical formula to estimate the portfolio VaR:

$$
\widehat{\text{VaR}_t^p} = -\mu_{\text{PF}}-\sigma_{\text{PF}, t} Q_p(z_t|\mathcal{F}_{t-1})
$$ {#eq-VaR_uni}

where $\sigma_{\text{PF}, t}$ is the portfolio conditional standard deviation at time t and $Q_p(z_t)$ is the p-quantile of the conditional distribution of the standardized portfolio returns, $z_t=(r_{\text{PF},t}-\mu_{\text{PF}})/\sigma_{\text{PF}, t}$. Note that formula @eq-VaR_uni includes the unconditional mean instead of the conditional one due to our mean specification. In the MixN(k)-GARCH setting the VaR forecasts are generated by the 'MSGARCH' package of @MSGARCH by applying the definition of the VaR from equation @eq-VaR_def to the predictive mixed normal distribution of the portfolio returns.

```{r}
#| label: tbl-ols
#| tbl-cap: Parameter Estimates for the Carhart Four-Factor Model
#| echo: false

# Code for OLS estimates table would go here
```

For the factor copula-DCC-(N)GARCH models in @sec-factor-copula, we proceed in the same way as @Fortin2022. Hence, we start off by calculating the ordinary least squares (OLS) estimates of equation @eq-Carhart. These estimates are displayed in Table @tbl-ols. Striking are the high Jarque-Bera statistics for the OLS residuals indicating univariate non-normality. Further analysis of the OLS residuals shows that the residuals are slightly left skewed and highly leptokurtic. Moreover, we can see in, which looks very similar to Panel B of, that the OLS residuals clearly are not multivariate normal either. Additionally, residuals of firms from similar sectors tend to be stronger correlated than the residuals of those from different sectors.

Next, we follow the procedure described in @sec-factor-copula to simulate a vector $\tilde{\bm{r}}_{\text{F},t}$. We then simulate from the distribution of the OLS residuals $\bm{F_\varepsilon}(\cdot)$ by randomly drawing a bootstrap sample vector $\bm{\tilde{\varepsilon}}_t=[\tilde{\varepsilon}_{1,t}, \tilde{\varepsilon}_{2,t},..., \tilde{\varepsilon}_{10,t}]$, $t=1,2,...,T$. It is important to note that, as pointed out in @value-at-risk-2009, each draw of $\bm{F_\varepsilon}(\cdot)$ has to be a vector of error terms from the same day so that aforementioned dependencies between the OLS residuals of different stock returns are maintained. Further, we have to pay attention to data leakage and can therefore only use OLS residuals which have already been observed i.e. are part of the rolling window that is used for time $t=1,2,...,T$. Using equation @eq-Carhart then yields the simulated return of single stock k:

$$
\tilde{r}_{k,t}=r_{f,t}+\alpha_{k,t}+\bm{\beta}_{k}'\tilde{\bm{r}}_{\text{F},t}+\bm{\tilde{\varepsilon}_{k,t}}.
$$ {#eq-simulated-return}

This procedure is repeated $N=200'000$ times. For each simulation we calculate the simulated return of the equal weighted portfolio. This yields a simulated portfolio return sample $\{r_{\text{PF}}^n\}_{n=1}^N$. Finally, the VaR estimate is obtained as the negative $p$-quantile of the simulated portfolio returns:

$$
\widehat{\text{VaR}_t^p}=-Q_p(\{r_{\text{PF}}^n\}_{n=1}^N).
$$ {#eq-var-estimate}

```{r}
#| label: fig-md-chisq-plot-ols
#| fig-cap: χ² Q-Q plot of the robust squared Mahalanobis distance of the OLS residuals of the Carhart four-factor model
#| echo: false

# Code for Q-Q plot would go here
```

Lastly, in the COMFORT framework we make use of the fact that when assuming a MGHyp distribution for $\bm{Y_t}$ (i.e. the returns of the constituents) the corresponding portfolio returns $r_{\text{PF},t}=\textbf{w}'\bm{Y}$ are univariate GHyp distributed (see @sec-comfort). Thus, according to @COMFORT_2022 the VaR forecast can be calculated using the $p$-quantile function $Q_p(\cdot)$ of the corresponding conditional univariate GHyp distribution (in our case the univariate Variance Gamma distribution) of the portfolio returns:

$$
\widehat{\text{VaR}_t^p}=-Q_p(r_{\text{PF},t}|\mathcal{F}_{t-1})=-\inf_x\{x\in\mathbb{R}:\mathbb{P}(r_{\text{PF},t}\leq x|\mathcal{F}_{t-1})\geq p\},
$$ {#eq-COMFORT_VaR}

where $\mathcal{F}_{t-1}=\{\bm{Y_1},...,\bm{Y_{t-1}}\}$ is the information set at time $t-1$.

## Backtesting {#sec-backtesting}

Backtesting is used to check whether the forecasts of the models evince some desirable properties. In doing so, we will follow the framework of @CCTest which consists of three likelihood-ratio tests. Let $I_t$ be the indicator variable for a $\text{VaR}_t^p$ forecast made at time t-1, that is,

$$
I_t=\mathbb{1}_{\{r_{\text{PF},t}<-\text{VaR}_t^p\}}=\begin{cases}
1 & \text{if } r_{\text{PF},t}<-\text{VaR}_t^p\\
0 & \text{otherwise}\\
\end{cases}
$$ {#eq-indicator-var}

According to @CCTest, a sequence of value at risk forecasts is then said to be efficient with respect to $\mathcal{F}_{t-1}$, the information set at time $t-1$, if 

$$
\mathbb{E}[I_t|\mathcal{F}_{t-1}]=\mathbb{E}[I_t|I_{t-1},I_{t-2},...,I_1]=p,\quad t=1,2,...,T.
$$ {#eq-cc}

@CCTest shows that testing whether a sequence of value at risk forecasts is efficient is equivalent to testing that $\{I_t\}_{t=1}^T \overset{\mathrm{iid}}{\sim}\text{Bernoulli}(p)$. In case this property is fulfilled, the VaR forecast has correct conditional coverage. Although one could test for correct conditional coverage directly we will follow @Paolella2006 and additionally provide intermediate test statistics which help to better understand deficiencies of the models.

First, we test for the correct number of exceedances under independence. The corresponding hypothesis of correct conditional coverage can be written as 

$$H_0:\mathbb{E}[I_t]=p \quad\text{versus}\quad H_A:\mathbb{E}[I_t]\neq p,$$

which is the result of applying the law of total expectation on equation @eq-cc. Let $n_1$ be the number of violations i.e. ones in the indicator sequence $\{I_t\}_{t=1}^T$ and $n_0$ the number of zeros in the indicator sequence. The likelihood-ratio test statistic then is

$$
LR_{uc}=-2\log\bigg(\dfrac{L(p;I_1,I_2,...,I_T)}{L(\hat{p};I_1,I_2,...,I_T)}\bigg)\overset{\mathrm{asy}}{\sim}\chi_1^2,
$$ {#eq-uc}

where $\hat{p}=\hat{p}_{\text{ML}} = \frac{n_1}{n_0+n_1}$ is the maximum likelihood estimate of $p$ and $L(\cdot)$ is the corresponding likelihood of a $\mathcal{B}in(n_0+n_1,p)$ distribution [@CCTest].


# Results {#sec-results}


# Conclusion {#sec-conclusion}

# Appendix
## Proofs
### Probability Integral Transform {#sec-appendix-pit}

### Quantile Transform {#sec-appendix-quantile-trafo}

## Copulas

### Gaussian Copula {#sec-appendix-cop-gauss}

### Student t Copula {#sec-appendix-cop-t}

## Distributions

### Finite Normal Mixtures {#sec-appendix-mixn}